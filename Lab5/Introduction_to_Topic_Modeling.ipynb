{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXgTV9JqvSkf"
      },
      "source": [
        "## Introduction\n",
        "##### How to get started with topic modeling using LDA in Python\n",
        "** **\n",
        "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
        "\n",
        "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
        "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
        "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
        "\n",
        "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
        "\n",
        "### Theoretical Overview\n",
        "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
        "\n",
        "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
        "\n",
        "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
        "\n",
        "- `psi`, the distribution of words for each topic K\n",
        "- `phi`, the distribution of topics for each document i\n",
        "\n",
        "#### Parameters of LDA\n",
        "\n",
        "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
        "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
        "\n",
        "**To read more: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDLfIv5WvSkh"
      },
      "source": [
        "** **\n",
        "### LDA Implementation\n",
        "\n",
        "1. [Loading data](#load_data)\n",
        "2. [Data cleaning](#clean_data)\n",
        "3. [Exploratory analysis](#eda)\n",
        "4. [Prepare data for LDA analysis](#data_preparation)\n",
        "5. [LDA model training](#train_model)\n",
        "6. [Analyzing LDA model results](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iOTL9wKvSki"
      },
      "source": [
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LlXGpevSki"
      },
      "source": [
        "** **\n",
        "#### Step 1: Loading Data <a class=\"anchor\\\" id=\"load_data\"></a>\n",
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = \"./NIPS Papers.zip\"\n",
        "\n",
        "# Check if the zip file exists\n",
        "if not os.path.exists(zip_file_path):\n",
        "    raise FileNotFoundError(f\"The file {zip_file_path} does not exist. Please upload it to the correct location.\")\n",
        "\n",
        "# Extract the zip file into a temporary directory\n",
        "temp_dir = \"temp\"\n",
        "os.makedirs(temp_dir, exist_ok=True)  # Create the temp directory if it doesn't exist\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(temp_dir)\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_file_path = os.path.join(temp_dir, \"NIPS Papers\", \"papers.csv\")\n",
        "\n",
        "# Check if the extracted CSV file exists\n",
        "if not os.path.exists(csv_file_path):\n",
        "    raise FileNotFoundError(f\"The file {csv_file_path} does not exist. Please check the structure of the extracted files.\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(papers.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsdCu77xxV2R",
        "outputId": "215aa796-598e-4053-b2d5-fc21d7abec7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id  year                                              title event_type  \\\n",
            "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
            "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
            "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
            "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
            "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
            "\n",
            "                                            pdf_name          abstract  \\\n",
            "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
            "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
            "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
            "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
            "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
            "\n",
            "                                          paper_text  \n",
            "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
            "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
            "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
            "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
            "4  Neural Network Ensembles, Cross\\nValidation, a...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7JqOywBvSki"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"./data/NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXrKmgt6vSki"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning <a class=\"anchor\\\" id=\"clean_data\"></a>\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "21KhZ9SmvSki",
        "outputId": "4b567f70-3f5c-4fe9-fa4f-4ba8bba7707a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "831   1999  Building Predictive Models from Fractal Repres...   \n",
              "1136  2001                        Minimax Probability Machine   \n",
              "4086  2012             On the Sample Complexity of Robust PCA   \n",
              "4601  2013          Lexical and Hierarchical Topic Regression   \n",
              "5397  2015  Efficient Non-greedy Optimization of Decision ...   \n",
              "\n",
              "                                               abstract  \\\n",
              "831                                    Abstract Missing   \n",
              "1136                                   Abstract Missing   \n",
              "4086  We estimate the sample complexity of a recent ...   \n",
              "4601  Inspired by a two-level theory that unifies ag...   \n",
              "5397  Decision trees and randomized forests are wide...   \n",
              "\n",
              "                                             paper_text  \n",
              "831   Building Predictive Models from Fractal\\nRepre...  \n",
              "1136  Minimax Probability Machine\\n\\nGert R.G. Lanck...  \n",
              "4086  On the Sample Complexity of Robust PCA\\n\\nMatt...  \n",
              "4601  Lexical and Hierarchical Topic Regression\\n\\nV...  \n",
              "5397  Efficient Non-greedy Optimization of Decision ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b847d1b-31e6-40f9-b142-441fe0728c8a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>831</th>\n",
              "      <td>1999</td>\n",
              "      <td>Building Predictive Models from Fractal Repres...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Building Predictive Models from Fractal\\nRepre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1136</th>\n",
              "      <td>2001</td>\n",
              "      <td>Minimax Probability Machine</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Minimax Probability Machine\\n\\nGert R.G. Lanck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4086</th>\n",
              "      <td>2012</td>\n",
              "      <td>On the Sample Complexity of Robust PCA</td>\n",
              "      <td>We estimate the sample complexity of a recent ...</td>\n",
              "      <td>On the Sample Complexity of Robust PCA\\n\\nMatt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4601</th>\n",
              "      <td>2013</td>\n",
              "      <td>Lexical and Hierarchical Topic Regression</td>\n",
              "      <td>Inspired by a two-level theory that unifies ag...</td>\n",
              "      <td>Lexical and Hierarchical Topic Regression\\n\\nV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5397</th>\n",
              "      <td>2015</td>\n",
              "      <td>Efficient Non-greedy Optimization of Decision ...</td>\n",
              "      <td>Decision trees and randomized forests are wide...</td>\n",
              "      <td>Efficient Non-greedy Optimization of Decision ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b847d1b-31e6-40f9-b142-441fe0728c8a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b847d1b-31e6-40f9-b142-441fe0728c8a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b847d1b-31e6-40f9-b142-441fe0728c8a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a989d53-ee38-4ac0-bc21-0c8f2e433c77\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a989d53-ee38-4ac0-bc21-0c8f2e433c77')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a989d53-ee38-4ac0-bc21-0c8f2e433c77 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1988,\n        \"max\": 2016,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          2003,\n          2009,\n          2007\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Multiple Kernel Learning and the SMO Algorithm\",\n          \"Feedforward Learning of Mixture Models\",\n          \"Gradient Flow Independent Component Analysis in Micropower VLSI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"Time delay is pervasive in neural information processing. To achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system. In the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner. The state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents. The parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network. We also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude.\",\n          \"Many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform. In particular, nonnegative matrix factorization (NMF) of the spectrogram -- the (power) magnitude of the short-time Fourier transform (STFT) -- has been considered in many audio applications. In this setting, NMF with the Itakura-Saito divergence was shown to underly a generative Gaussian composite model (GCM) of the STFT, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications. Still, the GCM is not yet a generative model of the raw signal itself, but only of its STFT. The work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure. In particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional NMF setting. We describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.\",\n          \"A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring latent binary features (topics) for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents defined by the legislation. State-of-the-art results are manifested for prediction of votes on a new piece of legislation, based only on the observed text legislation. The coupling of the text and legislation is also demonstrated to yield insight into the properties of the matrix decomposition for roll-call data.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Multiple Kernel Learning and the SMO Algorithm\\n\\nS. V. N. Vishwanathan, Zhaonan Sun, Nawanol Theera-Ampornpunt\\nPurdue University\\nvishy@stat.purdue.edu, sunz@stat.purdue.edu, ntheeraa@cs.purdue.edu\\nManik Varma\\nMicrosoft Research India\\nmanik@microsoft.com\\n\\nAbstract\\nOur objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential\\nMinimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to\\nimplement and adapt, and efficiently scales to large problems. As a result, it has\\ngained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal\\nin MKL for the very same reasons. Unfortunately, the standard MKL dual is not\\ndifferentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm\\nsquared, or with certain Bregman divergences, can indeed be trained using SMO.\\nThe resulting algorithm retains both simplicity and efficiency and is significantly\\nfaster than state-of-the-art specialised p-norm MKL solvers. We show that we can\\ntrain on a hundred thousand kernels in approximately seven minutes and on fifty\\nthousand points in less than half an hour on a single core.\\n\\n1\\n\\nIntroduction\\n\\nResearch on Multiple Kernel Learning (MKL) needs to follow a two pronged approach. It is important to explore formulations which lead to improvements in prediction accuracy. Recent trends\\nindicate that performance gains can be achieved by non-linear kernel combinations [7,18,21], learning over large kernel spaces [2] and by using general, or non-sparse, regularisation [6, 7, 12, 18].\\nSimultaneously, efficient optimisation techniques need to be developed to scale MKL out of the lab\\nand into the real world. Such algorithms can help in investigating new application areas and different\\nfacets of the MKL problem including dealing with a very large number of kernels and data points.\\nOptimisation using decompositional algorithms such as Sequential Minimal Optimization\\n(SMO) [15] has been a long standing goal in MKL [3] as the algorithms are simple, easy to implement and efficiently scale to large problems. The hope is that they might do for MKL what SMO\\ndid for SVMs ? allow people to play with MKL on their laptops, modify and adapt it for diverse real\\nworld applications and explore large scale settings in terms of number of kernels and data points.\\nUnfortunately, the standard MKL formulation, which learns a linear combination of base kernels\\nsubject to l1 regularisation, leads to a dual which is not differentiable. SMO can not be applied as a\\nresult and [3] had to resort to expensive Moreau-Yosida regularisation to smooth the dual. State-ofthe-art algorithms today overcome this limitation by solving an intermediate saddle point problem\\nrather than the dual itself [12, 16].\\nOur focus, in this paper, is on training p-norm MKL, with p > 1, using the SMO algorithm. More\\ngenerally, we prove that linear MKL regularised by certain Bregman divergences, can also be trained\\n1\\n\\n\\fusing SMO. We shift the emphasis firmly back towards solving the dual in such cases. The lp MKL dual is shown to be differentiable and thereby amenable to co-ordinate ascent. Placing the\\np-norm squared regulariser in the objective lets us efficiently solve the core reduced two variable\\noptimisation problem analytically in some cases and algorithmically in others. Using results from [4,\\n9], we can compute the lp -MKL Hessian, which brings into play second order variable selection\\nmethods which tremendously speed up the rate of convergence [8]. The standard decompositional\\nmethod proof of convergence [14] to the global optimum holds with minor modifications.\\nThe resulting optimisation algorithm, which we call SMO-MKL, is straight forward to implement\\nand efficient. We demonstrate that SMO-MKL can be significantly faster than the state-of-the-art\\nspecialised p-norm solvers [12]. We empirically show that the SMO-MKL algorithm is robust with\\nthe desirable property that it is not greatly affected within large operating ranges of p. This implies\\nthat our algorithm is well suited for learning both sparse, and non-sparse, kernel combinations.\\nFurthermore, SMO-MKL scales well to large problems. We show that we can efficiently combine\\na hundred thousand kernels in approximately seven minutes or train on fifty thousand points in less\\nthan half an hour using a single core on standard hardware where other solvers fail to produce results.\\nThe SMO-MKL code can be downloaded from [20].\\n\\n2\\n\\nRelated Work\\n\\nRecent trends indicate that there are three promising directions of research for obtaining performance\\nimprovements using MKL. The first involves learning non-linear kernel combinations. A framework\\nfor learning general non-linear kernel combinations subject to general regularisation was presented\\nin [18]. It was demonstrated that, for feature selection, the non-linear GMKL formulation could\\nperform significantly better not only as compared to linear MKL but also state-of-the-art wrapper\\nmethods and filter methods with averaging. Very significant performance gains in terms of pure\\nclassification accuracy were reported in [21] by learning a different kernel combination per data\\npoint or cluster. Again, the results were better not only as compared to linear MKL but also baselines\\nsuch as averaging. Similar trends were observed for regression while learning polynomial kernel\\ncombinations [7]. Other promising directions which have resulted in performance gains are sticking\\nto standard MKL but combining an exponentially large number of kernels [2] and linear MKL with\\np-norm regularisers [6, 12]. Thus MKL based methods are beginning to define the state-of-the-art\\nfor very competitive applications, such as object recognition on the Caltech 101 database [21] and\\nobject detection on the PASCAL VOC 2009 challenge [19].\\nIn terms of optimisation, initial work on MKL leveraged general purpose SDP and QCQP\\nsolvers [13]. The SMO+M.-Y. regularisation method of [3] was one of the first techniques that\\ncould efficiently tackle medium scale problems. This was superseded by the SILP technique of [17]\\nwhich could, very impressively, train on a million point problem with twenty kernels using parallelism. Unfortunately, the method did not scale well with the number of kernels. In response, many\\ntwo-stage wrapper techniques came up [2, 10, 12, 16, 18] which could be significantly faster when\\nthe number of training points was reasonable but the number of kernels large. SMO could indirectly\\nbe used in some of these cases to solve the inner SVM optimisation. The primary disadvantage of\\nthese techniques was that they solved the inner SVM to optimality. In fact, the solution needed to\\nbe of high enough precision so that the kernel weight gradient computation was accurate and the\\nalgorithm converged. In addition, Armijo rule based step size selection was also very expensive and\\ncould involve tens of inner SVM evaluations in a single line search. This was particularly expensive\\nsince the kernel cache would be invalidated from one SVM evaluation to the next. The one big\\nadvantage of such two-stage methods for l1 -MKL was that they could quickly identify, and discard,\\nthe kernels with zero weights and thus scaled well with the number of kernels. Most recently, [12]\\nhave come up with specialised p-norm solvers which make substantial gains by not solving the inner\\nSVM to optimality and working with a small active set to better utilise the kernel cache.\\n\\n3\\n\\nThe lp -MKL Formulation\\n\\nThe objective in MKL is to jointly learn kernel and SVM parameters from training data {(xi , yi )}.\\nGiven a set of base kernels {Kk } and corresponding\\nP feature maps {?k }, linear MKL aims to learn\\na linear combination of the base kernels as K = k dk Kk . If the kernel weights are restricted to\\n2\\n\\n\\fbe non-negative, then the MKL task ?\\ncorresponds to learning a standard SVM in the feature space\\nformed by concatenating the vectors dk ?k . The primal can therefore be formulated as\\nXp\\nX\\nX\\n? X p p2\\n1\\ns. t. yi (\\nmin\\ndk wtk ?k (xi )+b) ? 1??i (1)\\nwtk wk +C\\n?i + (\\ndk )\\n2\\nw,b,??0,d?0\\n2\\ni\\nk\\n\\nk\\n\\nk\\n\\nThe regularisation on the kernel weights is necessary to prevent them from shooting off to infinity.\\nWhich regulariser one uses depends on the task at hand. In this Section, we limit ourselves to the\\np-norm squared regulariser with p > 1. If it is felt that certain kernels are noisy and should be\\ndiscarded then a sparse solution can be obtained by letting p tend to unity from above. Alternatively,\\nif the application demands dense solutions, then larger values\\n? of p should be selected. Note that the\\nprimal above can be made convex by substituting wk for dk wk to get\\nX\\nX\\nX\\n? X p p2\\n1\\ns. t. yi (\\nwtk ?k (xi ) + b) ? 1 ? ?i (2)\\nwtk wk /dk + C\\n?i + (\\ndk )\\nmin\\n2\\nw,b,??0,d?0\\n2\\ni\\nk\\n\\nk\\n\\nk\\n\\nWe first derive an intermediate saddle point optimisation problem obtained by minimising only w,\\nb and ?. The Lagrangian is\\nX\\nX\\nX\\n? X p p2 X\\nL = 12\\n?i [yi (\\nwtk ?k (xi ) + b) ? 1 + ?i ] (3)\\nwtk wk /dk +\\n(C ? ?i )?i + (\\ndk ) ?\\n2\\ni\\ni\\nk\\n\\nk\\n\\nk\\n\\nDifferentiating with respect to w, b and ? to get the optimality conditions and substituting back\\nresults in the following intermediate saddle point problem\\nX\\n? X p p2\\n(4)\\nd k ? t Hk ? + (\\nmin max 1t ? ? 12\\ndk )\\nd?0 ??A\\n2\\nk\\n\\nk\\n\\nwhere A = {?|0 ? ? ? C1, 1t Y ? = 0}, Hk = Y Kk Y and Y is a diagonal matrix with the labels\\non the diagonal. Note that most MKL methods end up optimising either this, or a very similar, saddle\\npoint problem. To now eliminate d we again form the Lagrangian\\nX\\n? X p p2 X\\nL = 1t ? ? 21\\nd k ? t Hk ? + (\\ndk ) ?\\n? k dk\\n(5)\\n2\\nk\\nk\\nk\\nX p 2\\n?L\\n(6)\\n= 0 ? ?(\\ndk ) p ?1 dp?1\\n= ?k + 21 ?t Hk ?\\nk\\n?dk\\nk\\nX p 2\\nX\\n(7)\\n? ?(\\ndk ) p =\\ndk (?k + 21 ?t Hk ?)\\nk\\n\\nk\\n\\n? L = 1t ? ?\\n\\n2\\n? X p p2\\n1 X\\n(\\n( (?k + 12 ?t Hk ?)q ) q\\ndk ) = 1t ? ?\\n2\\n2?\\n\\nk\\n\\n(8)\\n\\nk\\n\\nwhere p1 + 1q = 1. Since Hk is positive semi-definite, ?t Hk ? ? 0 and since ?k ? 0 it is clear that\\nthe optimal value of ?k is zero. Our lp -MKL dual therefore becomes\\n2\\n1 X t\\n( (? Hk ?)q ) q\\n(9)\\nD ? max 1t ? ?\\n??A\\n8?\\nk\\n\\nand the kernel weights can be recovered from the dual variables as\\n! q1 ? p1\\nq\\n1 X t\\n(?t Hk ?) p\\n(? Hk ?)q\\ndk =\\n2?\\n\\n(10)\\n\\nk\\n\\nNote that our dual objective, unlike the objective in [3], is differentiable with respect to ?. The\\nSMO algorithm can therefore be brought to bear where two variables are selected and optimised\\nusing gradient or Newton methods and the process repeated until convergence.\\nAlso note that it has sometimes been observed that l2 regularisation can provide better results than\\nl1 [6, 7, 12, 18]. For this special case, when p = q = 2, the reduced two variable problem can\\nbe solved analytically. This was one of the primary motivations for choosing the p-norm squared\\nregulariser and placing it in the primal objective (the other was to be consistent with other p-norm\\nformulations [9, 11]). Had we included the regulariser as a primal constraint then the dual would\\nhave the q-norm rather than the q-norm squared. Our dual would then be near identical to Eq. (9)\\nin [12]. However, it would then no longer have been possible to solve the two variable reduced\\nproblem analytically for the 2-norm special case.\\n3\\n\\n\\f4\\n\\nSMO-MKL Optimisation\\n\\nWe now develop the SMO-MKL algorithm for optimising the lp MKL dual. The algorithm has three\\nmain components: (a) reduced variable optimisation; (b) working set selection and (c) stopping\\ncriterion and kernel caching. We build the SMO-MKL algorithm around the LibSVM code base [5].\\n4.1\\n\\nThe Reduced Variable Optimisation\\n\\nThe SMO algorithm works by repeatedly choosing two variables (assumed to be ?1 and ?2 without\\nloss of generality in this Subsection) and optimising them while holding all other variables constant.\\nIf ?1 ? ?1 + ? and ?2 ? ?2 + s?, the dual simplifies to\\n?? = argmax (1 + s)? ?\\nL???U\\n\\n2\\n1 X\\n( (ak ?2 + 2bk ? + ck )q ) q\\n8?\\n\\n(11)\\n\\nk\\n\\nwhere s = ?y1 y2 , L = (s == +1) ? max(??1 , ??2 ) : max(??1 , ?2 ? C), U =\\n(s == +1) ? min(C ? ?1 , C ? ?2 ) : min(C ? ?1 , ?2 ), ak = H11k + H22k + 2sH12k ,\\nbk = ?t (H:1k + sH:2k ) and ck = ?t Hk ?. Unlike as in SMO, ?? can not be found analytically for arbitrary p. Nevertheless, since this is a simple one dimensional concave optimisation\\nproblem, we can efficiently find the global optimum using a variety of methods. We tried bisection\\nsearch and Brent?s algorithm but the Newton-Raphson method worked best ? partly because the one\\ndimensional Hessian was already available from the working set selection step.\\n4.2\\n\\nWorking Set Selection\\n\\nThe choice of which two variables to select for optimisation can have a big impact on training time.\\nVery simple strategies, such as random sampling, can have very little cost per iteration but need many\\niterations to converge. First and second order working set selection techniques are more expensive\\nper iteration but converge in far fewer iterations.\\nWe implement the greedy second order working set selection strategy of [8]. We do not give the\\nvariable selection equations due to lack of space but refer the interested reader to the WSS2 method\\nof [8] and our source code [20]. The critical thing is that the selection of the first (second) variable\\ninvolves computing the gradient (Hessian) of the dual. These are readily derived to be\\nX\\n?? D = 1 ?\\ndk Hk ? = 1 ? H?\\n(12)\\nk\\n\\n?2? D = ?H ?\\n\\n1X\\n??k f ?1 (?)(Hk ?)(Hk ?)t\\n?\\n\\n(13)\\n\\nk\\n\\nwhere ??k f ?1 (?) = (2 ? q)? 2?2q\\n?k2q?2 + (q ? 1)? q2?q ?kq?2 and ?k\\nq\\n\\n=\\n\\n1 t\\n? Hk ?\\n2?\\n\\n(14)\\n\\nwhere D has been overloaded to now refer to the dual objective. Rather than compute the gradient\\n?? D repeatedly, we speed up variable selection by caching, separately for each kernel, Hk ?. The\\ncache needs to be updated every time we change ? in the reduced variable optimisation. However,\\nsince only two variables are changed, Hk ? can be updated by summing along just two columns of\\nthe kernel matrix. This involves only O(M ) work in all, where M is the number of kernels, since\\nthe column sums can be pre-computed for each kernel. The Hessian is too expensive to cache and is\\nrecomputed on demand.\\n4.3\\n\\nStopping Criterion and Kernel Caching\\n\\nWe terminate the SMO-MKL algorithm when the duality gap falls below a pre-specified threshold.\\nKernel caching strategies can have a big impact on performance since kernel computations can\\ndominate everything else in some cases. While a few different kernel caching techniques have been\\nexplored for SVMs, we stick to the standard one used in LibSVM [5]. A Least Recently Used\\n(LRU) cache is implemented as a circular queue. Each element in the queue is a pointer to a recently\\naccessed (common) row of each of the individual kernel matrices.\\n4\\n\\n\\f5\\n\\nSpecial Cases and Extensions\\n\\nWe briefly discuss a few special cases and extensions which impact our SMO-MKL optimisation.\\n5.1\\n\\n2-Norm MKL\\n\\nAs we noted earlier, 2-norm MKL has sometimes been found to outperform MKL trained with l1\\nregularisation [6, 7, 12, 18]. For this special case, when p = q = 2, our dual and reduced variable\\noptimisation problems simplify to polynomials of degree four\\n1 X t\\nD2 ? max 1t ? ?\\n(? Hk ?)2\\n(15)\\n??A\\n8?\\nk\\n1 X\\n?? = argmax (1 + s)? ?\\n(ak ?2 + 2bk ? + ck )2\\n(16)\\n8?\\nL???U\\nk\\n\\n?\\n\\nJust as in standard SMO, ? can now be found analytically by using the expressions for the roots of\\na cubic. This makes our SMO-MKL algorithm particularly efficient for p = 2 and our code defaults\\nto the analytic solver for this special case.\\n5.2\\n\\nThe Bregman Divergence as a Regulariser\\n\\nThe Bregman divergence generalises the squared p-norm. It is not a metric as it is not symmetric and\\ndoes not obey the triangle inequality. In this Subsection, we demonstrate that our MKL formulation\\ncan also incorporate the Bregman divergence as a regulariser.\\nLet F be any differentiable, strictly convex function and f = ?F represent its gradient. The\\nBregman divergence generated by F is given by rF (d) = F (d) ? F (d0 ) ? (d ? d0 )t f (d0 ). Note\\nthat ?rF (d) = f (d) ? f (d0 ). Incorporating the Bregman divergence as a regulariser in our primal\\nobjective leads to the following intermediate saddle point problem and Lagrangian\\nX\\nIB ? min max 1t ? ? 12\\ndk ?t Hk ? + ?rF (d)\\n(17)\\nd?0 ??A\\n\\nLB = 1t ? ?\\n\\nX\\n\\nk\\n\\ndk (?k + 12 ?t Hk ?) + ?rF (d)\\n\\n?d LB = 0 ? f (d) ? f (d0 ) = g(?, ?)/?\\n?d=f\\n\\n?1\\n\\n(18)\\n\\nk\\n\\n(19)\\n\\n(f (d0 ) + g(?, ?)/?) = f\\n\\n?1\\n\\n(?(?, ?))\\n\\n(20)\\n\\n1 t\\n2 ? Hk ?\\n\\nwhere g is a vector with entries gk (?, ?) = ?k +\\nand ?(?, ?) = f (d0 ) + g(?, ?)/?.\\nSubstituting back in the Lagrangian and discarding terms dependent on just d0 results in the dual\\nDR ?\\n\\nmax\\n\\n??A,??0\\n\\n1t ? + ?(F (f ?1 (?)) ? ? t f ?1 (?))\\n\\n(21)\\n\\nIn many cases the optimal value of ? will turn out to be zero and the optimisation can efficiently be\\ncarried out over ? using our SMO-MKL algorithm.\\nGeneralised KL Divergence To take a concrete example, different from the p-norm squared used\\nthus far, we investigate the use of the generalised KL divergence as a regulariser. Choosing F (d) =\\nP\\nk dk (log(dk ) ? 1) leads to the generalised KL divergence between d and d0\\nX\\nX\\nX\\nrKL (d) =\\ndk log(dk /d0k ) ?\\ndk +\\nd0k\\n(22)\\nk\\n\\nk\\n\\nk\\n\\nPlugging in rKL in IB and following the steps above leads to the following dual problem\\nX\\nt\\n1\\nmax 1t ? ? ?\\nd0k e 2? ? Hk ?\\n??A\\n\\n(23)\\n\\nk\\n\\nwhich can be optimised straight forwardly using our SMO-MKL algorithm once we plug in the\\ngradient and hessian information. However, discussing this further would take us too far out of the\\nscope of this paper. We therefore stay focused on lp -MKL for the remainder of this paper.\\n5\\n\\n\\f5.3\\n\\nRegression and Other Loss Functions\\n\\nWhile we have discussed MKL based classification so far we can easily adapt our formulation to\\nhandle other convex loss functions such as regression, novelty detection, etc. We demonstrate this\\nfor the ?-insensitive loss function for regression. The primal, intermediate saddle point and final\\ndual problems are given by\\nX\\nX\\n? X p p2\\n+\\n?\\nt\\n1\\nw\\nw\\n/d\\n+\\nC\\n(?\\n+\\n?\\n)\\n+\\n(\\ndk )\\nPR ?\\nmin\\n(24)\\nk\\nk\\nk\\ni\\ni\\n2\\n2\\nw,b,?? ?0,d?0\\ni\\nk\\nk\\nX\\nsuch that ? (\\n(25)\\nwtk ?k (xi ) + b ? yi ) ? ? + ?i?\\nk\\n\\nIR ? min\\n\\nmax\\n\\nDR ?\\n\\nmax\\n\\nd?0 ?|?|?C1, 1t ?=0\\n\\n0?|?|?C1, 1t ?=0\\n\\nt\\n\\n1 (Y ? ? ?|?|) ?\\n1t (Y ? ? ?|?|) ?\\n\\n1\\n2\\n\\nX\\n\\nd k ? t Kk ? +\\n\\nk\\n\\n? X p p2\\n(\\ndk )\\n2\\n\\n2\\n1 X t\\n( (? Kk ?)q ) q\\n8?\\n\\n(26)\\n\\nk\\n\\n(27)\\n\\nk\\n\\nSMO has a slightly harder time optimising DR due to the |?| term which, though in itself not\\ndifferentiable, can be gotten around by substituting ? = ?+ ? ?? at the cost of doubling the\\nnumber of dual variables.\\n\\n6\\n\\nExperiments\\n\\nIn this Section, we empirically compare the performance of our proposed SMO-MKL algorithm\\nagainst the specialised lp -MKL solver of [12] which is referred to as Shogun. Code, scripts and\\nparameter settings were helpfully provided by the authors and we ensure that our stopping criteria\\nare compatible. All experiments are carried out on a single core of an AMD 2380 2.5 GHz processor\\nwith 32 Gb RAM. Our focus in these experiments is purely on training time and speed of optimisation as the prediction accuracy improvements of lp -MKL have already been documented [12].\\nWe carry out two sets of experiments. The first, on small scale UCI data sets, are carried out using\\npre-computed kernels. This performs a direct comparison of the algorithmic components of SMOMKL and Shogun. We also carry out a few large scale experiments with kernels computed on the\\nfly. This experiment compares the two methods in totality. In this case, kernel caching can have an\\neffect, but not a significant one as the two methods have very similar caching strategies.\\nFor each UCI data set we generated kernels as recommended in [16]. We generated RBF kernels\\nwith ten bandwidths for each individual dimension of the feature vector as well as the full feature\\nvector itself. Similarly, we also generated polynomial kernels of degrees 1, 2 and 3. All kernels\\nmatrices were pre-computed and normalised to have unit trace. We set C = 100 as it gives us a\\nreasonable accuracy on the test set. Note that for some value of ?, SMO-MKL and Shogun will\\nconverge to exactly the same solution [12]. Since this value is not known a priori we arbitrarily set\\n? = 1.\\nTraining times on the UCI data sets are presented in Table 1. Means and standard deviations are\\nreported for five fold cross-validation. As can be seen, SMO-MKL is significantly faster than Shogun\\nat converging to similar solutions and obtaining similar test accuracies. In many cases, SMO-MKL\\nis more than four times as fast and in some case more than ten or twenty times as fast. Note that our\\ntest classification accuracy on Liver is a lot lower than Shogun?s. This is due to the arbitrary choice\\nof ?. We can vary our ? on Liver to recover the same accuracy and solution as Shogun with a further\\ndecrease in our training time.\\nAnother very positive thing is that SMO-MKL appears to be relatively stable across a large operating\\nrange of p. The code is, in most of the cases as expected, fastest when p = 2 and gets slower as\\none increases or decreases p. Interestingly though, the algorithm doesn?t appear to be significantly\\nslower for other values of p. Therefore, it is hoped that SMO-MKL can be used to learn sparse\\nkernel combinations as well as non-sparse ones.\\nMoving on to the large scale experiments with kernels computed on the fly, we first tried combining\\na hundred thousand RBF kernels on the Sonar data set with 208 points and 59 dimensional features.\\n6\\n\\n\\fTable 1: Training times on UCI data sets with N training points, D dimensional features, M kernels\\nand T test points. Mean and standard deviations are reported for 5-fold cross validation.\\n(a) Australian: N =552, T =138, D=13, M =195.\\nTraining Time (s)\\nTest Accuracy (%)\\n# Kernels Selected\\np\\nSMO-MKL\\nShogun\\nSMO-MKL\\nShogun\\nSMO-MKL\\nShogun\\n1.10 4.89 ? 0.31 58.52 ? 16.49 85.22 ? 2.96 85.22 ? 2.81 26.4 ? 0.8 137.2 ? 53.8\\n62.4 ? 4.7\\n1.33 4.16 ? 0.16 33.58 ? 2.58 85.36 ? 3.79 85.07 ? 2.85 40.8 ? 1.3\\n1.66 4.31 ? 0.19 31.89 ? 1.25 85.65 ? 3.73 85.07 ? 2.85 72.2 ? 4.8\\n100.2 ? 3.7\\n2.00 4.27 ? 0.10 27.08 ? 7.18 85.80 ? 3.74 85.22 ? 2.99 126.4 ? 4.3 134.4 ? 5.6\\n2.33 4.88 ? 0.18 24.92 ? 6.46 85.80 ? 3.74 85.07 ? 2.85 162.8 ? 3.6 177.8 ? 8.3\\n2.66 5.19 ? 0.05 26.90 ? 2.05 85.80 ? 3.68 85.22 ? 2.85 188.2 ? 4.7 188.8 ? 5.1\\n3.00 5.48 ? 0.21 27.06 ? 2.20 85.51 ? 3.69 85.22 ? 2.85 192.0 ? 2.6 194.4 ? 1.2\\np\\n1.10\\n1.33\\n1.66\\n2.00\\n2.33\\n2.66\\n3.00\\n\\np\\n1.10\\n1.33\\n1.66\\n2.00\\n2.33\\n2.66\\n3.00\\n\\np\\n1.10\\n1.33\\n1.66\\n2.00\\n2.33\\n2.66\\n3.00\\n\\n(b) Ionosphere: N =280, T =71, D=33, M =442.\\nTraining Time (s)\\nTest Accuracy (%)\\n# Kernels Selected\\nSMO-MKL\\nShogun\\nSMO-MKL\\nShogun\\nSMO-MKL\\nShogun\\n2.85 ? 0.16 19.82 ? 4.02 92.60 ? 1.35 92.03 ? 1.68 50.0 ? 2.7\\n125.2 ? 7.3\\n2.78 ? 1.18 8.49 ? 0.61 92.03 ? 1.42 92.60 ? 1.86 120.8 ? 6.0 217.0 ? 23.4\\n2.42 ? 0.28 10.49 ? 2.27 91.74 ? 2.08 91.74 ? 1.37 200.8 ? 4.4 291.4 ? 33.0\\n2.16 ? 0.16 13.99 ? 4.68 92.03 ? 1.68 91.17 ? 2.45 328.0 ? 6.6 364.2 ? 15.4\\n2.35 ? 0.25 24.90 ? 9.43 92.03 ? 1.68 91.74 ? 2.08 413.6 ? 5.6 412.2 ? 6.6\\n2.50 ? 0.32 33.05 ? 3.66 92.03 ? 1.68 92.03 ? 1.68 430.6 ? 4.6 436.6 ? 4.3\\n3.03 ? 0.99 36.23 ? 3.62 92.31 ? 1.41 91.75 ? 2.05 434.4 ? 4.8 442.0 ? 0.0\\n(c) Liver: N =276, T =69, D=5, M =91.\\nTraining Time (s)\\nTest Accuracy (%)\\nSMO-MKL\\nShogun\\nSMO-MKL\\nShogun\\n0.53 ? 0.03 2.15 ? 0.12 62.90 ? 9.81 66.67 ? 9.91\\n0.54 ? 0.03 0.92 ? 0.05 66.09 ? 8.48 71.59 ? 8.92\\n0.56 ? 0.04 1.14 ? 0.23 66.96 ? 7.53 70.72 ? 9.28\\n0.54 ? 0.04 1.72 ? 0.57 66.96 ? 7.06 72.17 ? 6.94\\n0.63 ? 0.03 2.35 ? 0.36 66.38 ? 7.36 73.33 ? 6.71\\n0.65 ? 0.02 2.53 ? 0.44 65.22 ? 6.80 72.75 ? 7.96\\n0.67 ? 0.03 3.40 ? 0.55 65.22 ? 6.74 73.91 ? 7.28\\n(d) Sonar: N =166, T =42, D=59, M =793.\\nTraining Time (s)\\nTest Accuracy (%)\\nSMO-MKL\\nShogun\\nSMO-MKL\\nShogun\\n4.95 ? 0.29 47.19 ? 3.85 85.15 ? 7.99 81.25 ? 8.71\\n4.00 ? 0.76 18.28 ? 1.63 84.65 ? 9.37 87.03 ? 6.85\\n4.48 ? 1.63 20.27 ? 8.84 88.47 ? 6.68 87.51 ? 6.28\\n3.31 ? 0.31 31.52 ? 5.07 88.94 ? 6.00 88.95 ? 6.33\\n3.54 ? 0.35 51.83 ? 17.96 88.94 ? 4.97 88.94 ? 5.41\\n3.83 ? 0.38 64.59 ? 9.19 88.94 ? 4.97 88.94 ? 4.97\\n3.96 ? 0.45 70.08 ? 9.18 88.94 ? 4.97 89.92 ? 5.13\\n\\n# Kernels Selected\\nSMO-MKL\\nShogun\\n9.40 ? 1.02 39.40 ? 1.50\\n24.40 ? 2.06 43.60 ? 2.42\\n44.20 ? 2.23 57.00 ? 3.29\\n71.00 ? 5.29 78.00 ? 2.28\\n82.40 ? 2.42 88.20 ? 1.72\\n83.20 ? 2.32 90.80 ? 0.40\\n85.20 ? 3.37 91.00 ? 0.00\\n# Kernels Selected\\nSMO-MKL\\nShogun\\n91.2 ? 6.9\\n258.0 ? 24.8\\n247.8 ? 7.7 374.2 ? 20.9\\n383.0 ? 5.7 451.6 ? 12.0\\n661.2 ? 10.2 664.8 ? 35.2\\n770.8 ? 4.4\\n763.0 ? 7.0\\n782.0 ? 3.4\\n789.4 ? 2.8\\n786.0 ? 4.1\\n792.2 ? 1.1\\n\\nNote that these kernels do not form any special hierarchy so approaches such as [2] are not applicable. Timing results on a log-log scale are given in Figure (1a). As can be seen, SMO-MKL appears\\nto be scaling linearly with the number of kernels and we converge in less than half an hour on all\\nhundred thousand kernels for both p = 2 and p = 1.33. If we were to run the same experiment using\\npre-computed kernels then we converge in approximately seven minutes (see Fig (1b)). On the other\\nhand, Shogun took six hundred seconds to combine just ten thousand kernels computed on the fly.\\nThe trend was the same when we increased the number of training points. Figure (1c) and (1d) plot\\ntiming results on a log-log scale as the number of training points is varied on the Adult and Web\\ndata sets (please see [1] for data set details and downloads). We used 50 kernels computed on the\\n7\\n\\n\\fSonar\\n\\n7\\n\\n6\\n\\n9\\n8\\n\\n5.5\\n\\nlog(Time) (s)\\n\\nlog(Time) (s)\\n\\nlog(Time) (s)\\n\\n6\\n\\n8\\n\\n10\\n\\nSMO?MKL p=1.33\\nSMO?MKL p=2.00\\n\\n5\\n6.5\\n\\nWeb\\n\\nAdult\\n\\n7\\n\\nSMO?MKL p=1.33\\nSMO?MKL p=2.00\\n\\n4\\n3\\n2\\n\\nSMO?MKL p=1.33\\nSMO?MKL p=2.00\\nShogun p=1.33\\nShogun p=2.00\\n\\n7\\n\\nSMO?MKL p=1.33\\nSMO?MKL p=2.00\\n\\n6\\nlog(Time) (s)\\n\\nSonar\\n7.5\\n\\n7\\n6\\n5\\n4\\n\\n5\\n4\\n3\\n\\n3\\n\\n5\\n4.5\\n9\\n\\n1\\n9.5\\n\\n10\\n10.5\\n11\\nlog(# Kernels)\\n\\n11.5\\n\\n12\\n\\n(a) Sonar\\n\\n0\\n6\\n\\n2\\n\\n2\\n\\n7\\n\\n8\\n\\n9\\n10\\nlog(# Kernels)\\n\\n11\\n\\n12\\n\\n(b) Sonar Pre-computed\\n\\n1\\n7\\n\\n7.5\\n\\n8\\n8.5\\n9\\n9.5\\nlog(# Training Points)\\n\\n(c) Adult\\n\\n10\\n\\n10.5\\n\\n1\\n7.5\\n\\n8\\n\\n8.5\\n9\\n9.5\\n10\\nlog(# Training Points)\\n\\n10.5\\n\\n11\\n\\n(d) Web\\n\\nFigure 1: Large scale experiments varying the number of kernels and points. See text for details.\\nfly for these experiments. On Adult, till about six thousand points, SMO-MKL is roughly 1.5 times\\nfaster than Shogun for p = 1.33 and 5 times faster for p = 2. However, on reaching eleven thousand\\npoints, Shogun starts taking more and more time to converge and we could not get results for sixteen\\nthousand points or more. SMO-MKL was unaffected and converged on the full data set with 32,561\\npoints in 9245.80 seconds for p = 1.33 and 8511.12 seconds for p = 2. We tried the Web data set\\nto see whether the SMO-MKL algorithm would scale beyond 32K points. Training on all 49,749\\npoints and 50 kernels took 1574.73 seconds (i.e. less than half an hour) with p = 1.33 and 2023.35\\nseconds with p = 2.\\n\\n7\\n\\nConclusions\\n\\nWe developed the SMO-MKL algorithm for efficiently optimising the lp -MKL formulation. We\\nplaced the emphasis firmly back on optimising the MKL dual rather than the intermediate saddle\\npoint problem on which all state-of-the-art MKL solvers are based. We showed that the lp -MKL\\ndual is differentiable and that placing the p-norm squared regulariser in the primal objective lets us\\nanalytically solve the reduced variable problem for p = 2. We could also solve the convex, onedimensional reduced variable problem when p 6= 2 by the Newton-Raphson method. A second-order\\nworking set selection algorithm was implemented to speed up convergence. The resulting algorithm\\nis simple, easy to implement and efficiently scales to large problems. We also showed how to\\ngeneralise the algorithm to handle not just p-norms squared but also certain Bregman divergences.\\nIn terms of empirical performance, we compared the SMO-MKL algorithm to the specialised lp MKL solver of [12] referred to as Shogun. It was demonstrated that SMO-MKL was significantly\\nfaster than Shogun on both small and large scale data sets ? sometimes by an order of magnitude.\\nSMO-MKL was also found to be relatively stable for various values of p and could therefore be\\nused to learn both sparse, and non-sparse, kernel combinations. We demonstrated that the algorithm\\ncould combine a hundred thousand kernels on Sonar in approximately seven minutes using precomputed kernels and in less than half an hour using kernels computed on the fly. This is significant\\nas many non-linear kernel combination forms, which lead to performance improvements but are\\nnon-convex, can be recast as convex linear MKL with a much larger set of base kernels. The SMOMKL algorithm can now be used to tackle such problems as long as an appropriate regulariser can\\nbe found. We were also able to train on the entire Web data set with nearly fifty thousand points\\nand fifty kernels computed on the fly in less than half an hour. Other solvers were not able to\\nreturn results on these problems. All experiments were carried out on a single core and therefore,\\nwe believe, redefine the state-of-the-art in terms of MKL optimisation. The SMO-MKL code is\\navailable for download from [20].\\n\\nAcknowledgements\\nWe are grateful to Saurabh Gupta, Marius Kloft and Soren SSonnenburg for helpful discussions,\\nfeedback and help with Shogun.\\n\\nReferences\\n[1] http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html.\\n\\n8\\n\\n\\f[2] F. R. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In NIPS, pages\\n105?112, 2008.\\n[3] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO\\nalgorithm. In ICML, pages 6?13, 2004.\\n[4] A. Ben-Tal, T. Margalit, and A. Nemirovski. The ordered subsets mirror descent optimization method\\nwith applications to tomography. SIAM Journal of Opimization, 12(1):79?108, 2001.\\n[5] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at\\nhttp://www.csie.ntu.edu.tw/?cjlin/libsvm.\\n[6] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In UAI, 2009.\\n[7] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning non-linear combinations of kernels. In NIPS, 2009.\\n[8] R. E. Fan, P. H. Chen, and C. J. Lin. Working set selection using second order information for training\\nSVM. JMLR, 6:1889?1918, 2005.\\n[9] C. Gentile. Robustness of the p-norm algorithms. ML, 53(3):265?299, 2003.\\n[10] M. Gonen and E. Alpaydin. Localized multiple kernel learning. In ICML, 2008.\\n[11] J. Kivinen, M. K. Warmuth, and B. Hassibi. The p-norm generaliziation of the LMS algorithm for adaptive\\nfiltering. IEEE Trans. Signal Processing, 54(5):1782?1793, 2006.\\n[12] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. Muller, and A. Zien. Efficient and accurate lp -norm\\nMultiple Kernel Learning. In NIPS, 2009.\\n[13] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix\\nwith semidefinite programming. JMLR, 5:27?72, 2004.\\n[14] C. J. Lin, S. Lucidi, L. Palagi, A. Risi, and M. Sciandrone. Decomposition algorithm model for singly\\nlinearly-constrained problems subject to lower and upper bounds. JOTA, 141(1):107?126, 2009.\\n[15] J. Platt. Fast training of support vector machines using sequential minimal optimization. In Advances in\\nKernel Methods ? Support Vector Learning, pages 185?208, 1999.\\n[16] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu. SimpleMKL. JMLR, 9:2491?2521, 2008.\\n[17] S. Sonnenburg, G. Raetsch, C. Schaefer, and B. Schoelkopf. Large scale multiple kernel learning. JMLR,\\n7:1531?1565, 2006.\\n[18] M. Varma and B. R. Babu. More generality in efficient multiple kernel learning. In ICML, 2009.\\n[19] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple kernels for object detection. In ICCV,\\n2009.\\n[20] S. V. N. Vishwanathan, Z. Sun, N. Theera-Ampornpunt, and M. Varma, 2010. The SMO-MKL code\\nhttp://research.microsoft.com/?manik/code/SMO-MKL/download.html.\\n[21] J. Yang, Y. Li, Y. Tian, L. Duan, and W. Gao. Group-sensitive multiple kernel learning for object categorization. In ICCV, 2009.\\n\\n9\\n\\n\\f\",\n          \"Feedforward Learning of Mixture Models\\n\\nMatthew Lawlor?\\nApplied Math\\nYale University\\nNew Haven, CT 06520\\nmflawlor@gmail.com\\n\\nSteven W. Zucker\\nComputer Science\\nYale University\\nNew Haven, CT 06520\\nzucker@cs.yale.edu\\n\\nAbstract\\nWe develop a biologically-plausible learning rule that provably converges to the\\nclass means of general mixture models. This rule generalizes the classical BCM\\nneural rule within a tensor framework, substantially increasing the generality of\\nthe learning problem it solves. It achieves this by incorporating triplets of samples\\nfrom the mixtures, which provides a novel information processing interpretation\\nto spike-timing-dependent plasticity. We provide both proofs of convergence, and\\na close fit to experimental data on STDP.\\n\\n1\\n\\nIntroduction\\n\\nSpectral tensor methods and tensor decomposition are emerging themes in machine learning, but\\nthey remain global rather than ?online.? While incremental (online) learning can be useful for applications, it is essential for neurobiology. Error back propagation does operate incrementally, but\\nits neurobiological relevance remains a question for debate. We introduce a triplet learning rule\\nfor mixture distributions based on a tensor formulation of the BCM biological learning rule. It is\\nimplemented in a feedforward fashion, removing the need for backpropagation of error signals.\\nThe triplet requirement is natural biologically. Informally imagine your eyes microsaccading during\\na fixation, so that a tiny image fragment is ?sampled? repeatedly until the next fixation. Viewed\\nfrom visual cortex, edge selective neurons will fire repeatedly. Importantly, they exhibit strong\\nstatistical dependencies due to the geometry of objects and their relationships in the world. ?Hidden?\\ninformation such as edge curvatures, the presence of textures, and lighting discontinuities all affect\\nthe probability distribution of firing rates among orientation selective neurons, leading to complex\\nstatistical interdependencies between neurons.\\nLatent variable models are powerful tools in this context. They formalize the idea that highly coupled\\nrandom variables can be simply explained by a small number of hidden causes. Conditioned on these\\ncauses, the input distribution should be simple. For example, while the joint distribution of edges in\\na small patch of a scene might be quite complex, the distribution conditioned on the presence of a\\ncurved object at a particular location might be comparatively simple [14]. The specific question is\\nwhether brains can learn these mixture models, and how.\\nExample: Imagine a stimulus space of K inputs. These could be images of edges at particular\\norientations, or audio tones at K frequencies. These stimuli are fed into a network of n LinearNonlinear Poisson (LNP) spiking neurons. Let rij denote the firing rate of neuron i to stimulus j.\\nAssuming the stimuli are drawn independently with probability ?k , then the number of spikes d in\\nan interval where a single stimulus is shown is distributed according to a mixture model.\\nX\\nP (d) =\\n?k Pk (d)\\nk\\n1\\n\\nNow at Google Inc.\\n\\n1\\n\\n\\fwhere Pk (d) is a vector of independent Poisson distributions, and the rate parameter of the ith\\ncomponent is rik . We seek a filter that responds (in expectation) to one and only one stimulus. To\\ndo this, we must learn a set of weights that are orthogonal to all but one of the vectors of rates r ?j .\\nEach rate vector corresponds to the mean of one of the mixtures. Our problem is thus to learn the\\nmeans of mixtures. We will demonstrate that this can be done non-parametrically over a broad class\\nof firing patterns, not just Poisson spiking neurons.\\nAlthough fitting mixture models can be exponentially hard, under a certain multiview assumption,\\nnon-parametric estimation of mixture means can be done by tensor decomposition [2][1]. This\\nmultiview assumption requires access to at least 3 independent copies of the samples; i.e., multiple\\nsamples drawn from the same mixture component. For the LNP example above, this multiview\\nassumption requires only that we have access to the number of spikes in three disjoint intervals,\\nwhile the stimulus remains constant. After these intervals, the stimulus is free to change ? in vision,\\nsay, after a saccade ? after which point another sample triple is taken.\\nOur main result is that, with a slight modification of classical Bienenstock-Cooper-Munro [5] synaptic update rule a neuron can perform a tensor decomposition of the input data. By incorporating the\\ninteractions between input triplets, our online learning rule can provably learn the mixture means\\nunder an extremely broad class of mixture distributions and noise models. (The classical BCM\\nlearning rule will not converge properly in the presence of noise.) Specifically we show how the\\nclassical BCM neuron performs gradient ascent in a tensor objective function, when the data consists of discrete input vectors, and how our modified rule converges when the data are drawn from a\\ngeneral mixture model.\\nThe multiview requirement has an intriguing implication for neuroscience. Since spikes arrive in\\nwaves, and spike trains matter for learning [9], our model suggests that the waves of spikes arriving during adjacent epochs in time provide multiple samples of a given stimulus. This provides an\\nunusual information processing interpretation for the functional role of spike trains. To realize it\\nfully, we point out that classical BCM can be implemented via spike timing dependent plasticity\\n[17][10][6][18]. However, most of these approaches require much stronger distributional assumptions on the input data (generally Poisson), or learn a much simpler decomposition of the data than\\nour algorithm. Other, Bayesian methods [16], require the computation of a posterior distribution\\nwhich requires an implausible normalization step. Our learning rule successfully avoids these issues, and has provable guarantees of convergence to the true mixture means. At the end of this\\npaper we show how our rule predicts pair and triple spike timing dependent plasticity data.\\n\\n2\\n\\nTensor Notation\\n\\nLet ? denote the tensor product. We denote application of a k-tensor to k vectors by T (w1 , ..., wk ),\\nso in the simple case where T = v 1 ? ... ? v k ,\\nY\\nT (w1 , ..., wk ) =\\nhv j , wj i\\nj\\n\\nWe further denote the application of a k-tensor to k matrices by T (M1 , ..., Mk ) where\\nX\\nT (M1 , ..., Mk )i1 ,...,ik =\\nTj1 ,...,jk [M1 ]j1 ,i1 ...[Mk ]jk ,ik\\nj1 ,...,jk\\n\\nThus if T is a symmetric 2-tensor, T (M1 , M2 ) = M1T T M2 with ordinary matrix multiplication.\\nSimilarly, T (v 1 , v 2 ) = v T1 T v 2 .\\nWe say that T has an orthogonal tensor decomposition if\\nX\\nT =\\n?k v k ? v k ? ... ? v k and hv i , v j i = ?ij\\nk\\n\\n3\\n\\nConnection Between BCM Neuron and Tensor Decompositions\\n\\nThe BCM learning rule was introduced in 1982 in part to correct failings of the classical Hebbian\\nlearning rule [5]. The Hebbian learning rule [11] is one of the simplest and oldest learning rules. It\\n2\\n\\n\\fposits that the selectivity of a neuron to input i, mt (i) is increased in proportion to the post-synaptic\\nactivity of that neuron ct = hmt?1 , dt i, where m is a vector of synaptic weights.\\nmt ? mt?1 = ?t ct dt\\nThis learning rule will become increasingly correlated with its input. As formulated this rule does\\nnot converge for most input, as kmk ? ?. In addition, in the presence of multiple inputs Hebbian\\nlearning rule will always converge to an ?average? of the inputs, rather than becoming selective to\\none particular input. It is possible to choose a normalization of m such that m will converge to\\nthe first eigenvector of the input data. The BCM rule tries to correct for the lack of selectivity, and\\nfor the stabilization problems. Like the Hebbian learning rule, it always updates its weights in the\\ndirection of the input, however it also has a sliding threshold that controls the magnitude and sign of\\nthis weight update.\\nThe original formulation of the BCM rule is as follows: Let c be the post-synaptic firing rate, d ? RN\\nbe the vector of presynaptic firing rates, and m be the vector of synaptic weights. Then the BCM\\nsynaptic modification rule is\\nc = hm, di\\n? = ?(c, ?)d\\nm\\n? is a non-linear function of the firing rate, and ? is a sliding threshold that increases as a superlinear\\nfunction of the average firing rate.\\nThere are many different formulations of the BCM rule. The primary features that are required are\\n?(c, ?) is convex in c, ?(0, ?) = 0, ?(?, ?) = 0, and ? is a super-linear function of E[c].\\nThese properties guarantee that the BCM learning rule will not grow without bound. There have\\nbeen many variants of this rule. One of the most theoretically well analyzed variants is the Intrator\\nand Cooper model [12], which has the following form for ? and ?.\\n?(c, ?) = c(c ? ?) with ? = E[c2 ]\\nDefinition 3.1 (BCM Update Rule). With the Intrator and Cooper definition, the BCM rule is defined\\nas\\nmt = mt?1 + ?t ct (ct ? ?t?1 )dt\\n\\n(1)\\n\\n2\\nwhere\\nct = hmt?1\\nP\\nP, dt i2 and ? = E[c ]. ?t is a sequence of positive step sizes with the property that\\n?\\n?\\n?\\nand\\n<\\n?\\n?\\nt t\\nt t\\n\\nThe traditional application of this rule is a system where the input d is drawn from linearly independent vectors {d1 , ..., dk } with probabilities ?1 , ..., ?K , with K = N , the dimension of the\\nspace.\\nThese choices are quite convenient because they lead to the following objective function formulation\\nof the synaptic update rule.\\nR(m) =\\n\\ni 1 h\\ni2\\n1 h\\n3\\n2\\nE hm, di ? E hm, di\\n3\\n4\\n\\nThus,\\nh\\ni\\n2\\n2\\n?R = E hm, di d ? E[hm, di ] hm, di d\\n= E[c(c ? ?)d]\\n= E[?(c, ?)d]\\nSo in expectation, the BCM rule performs a gradient ascent in R(m). For random, discrete input\\nthis rule would then be a form of stochastic gradient ascent.\\nWith this model, we observe that the objective function can be rewritten in tensor notation. Note\\nthat this input model can be seen as a kind of degenerate mixture model.\\n3\\n\\n\\fThis objective function can be written as a tensor objective function, by noting the following:\\nX\\nT =\\n?k dk ? dk ? dk\\nk\\n\\nM=\\n\\nX\\nk\\n\\nR(m) =\\n\\n?k dk ? dk\\n\\n1\\n1\\nT (m, m, m) ? M (m, m)2\\n3\\n4\\n\\n(2)\\n\\nFor completeness, we present a proof that the stable points of the expected BCM update are selective\\nfor only one of the data vectors.\\n? = 0. Let ck = hm, dk i and ?k =\\nThe stable points of the expected update occur when E[m]\\n?(ck , ?). Let c = [c1 , . . . , cK ]T and ? = [?1 , . . . , ?K ]T .\\nDT = [d1 | ? ? ? |dK ]\\nP = diag(?)\\nTheorem 3.2. (Intrator 1992) Let K = N , let each dk be linearly independent, and let ?k > 0 and\\n? = ?R occur when\\ndistinct. Then stable points (in the sense of Lyapunov) of the expected update m\\nc = ?k?1 ek or m = ?k?1 D?1 ek . ek is the unit basis vector, so there is activity for only one stimuli.\\nP\\n? = DT P ? which is 0 only when ? = 0. Note ? = k ?k c2k . ?k = 0 if ck = 0 or\\nProof. E[m]\\nck = ?. Let S+ = {k : ck 6= 0}, and S? = {k : ck = 0}. Then for all k ? S+ , ck = ?S+\\n?\\n??1\\nX\\nX\\n?S+ ? ?S2 +\\n?i = 0\\n?S+ = ?\\n?i ?\\nk?S+\\n\\nk?S+\\n\\nTherefore the solutions of the BCM learning rule are c = 1S+ ?S+ , for all subsets S+ ? {1, . . . , K}.\\nWe now need to check which solutions are stable. The stable points (in the sense of Lyapunov) are\\npoints where the matrix\\n\\u0012\\n\\u0013\\n\\u0012\\n\\u0013\\n?\\n?? ?c\\n??\\n?E[m]\\nT\\nT\\n=D P\\n=D P\\nD\\nH=\\n?m\\n?c ?m\\n?c\\nis negative semidefinite.\\nLet S be an index set S ? {1, . . . , n}. We will use the following notation for the diagonal matrix\\nIS :\\n\\u001a\\n1 i?S\\n(IS )ii =\\n(3)\\n0 i?\\n/S\\nSo IS + IS c = I, and ei eTi = I{i}\\n\\na quick calculation shows\\n\\u0013\\n\\u0012\\n??i\\n= ?S+ IS+ ? ?S+ IS? ? 2?S2 + diag(?) 1S+ 1TS+\\n?cj\\nThis is negative semidefinite iff A = IS+ ? 2?S+ diag(?) 1S+ 1TS+ is negative semidefinite.\\nAssuming a non-degeneracy of the probabilities ?, and assume |S+ | > 1. Let j = arg mink?S+ ?k .\\nThen ?S+ ?j < 12 so A is not negative semi-definite. However, if |S+ | = 1 then A = ?IS+ so the\\nstable points occur when c = ?1i ei\\nThe triplet version of BCM can be viewed as a modification of the classical BCM rule which allows\\nit to converge in the presence of zero-mean noise. This indicates that the stable solutions of this\\nlearning rule are selective for only one data vector, dk .\\nBuilding off of the work of [2] we will use this characterization of the objective function to build a\\ntriplet BCM update rule which will converge for general mixtures, not just discrete data points.\\n4\\n\\n\\fhm1,di\\n\\nNoise sensitivity of m after 10e6 steps\\n\\n14\\n12\\n10\\n\\nTriplet Rule\\nBCM\\n\\n3\\n\\n8\\n6\\n\\nd1\\n4\\n\\n2\\n\\n0\\n\\n?10\\n\\n?5\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\nkm ? m0 k\\n\\n2\\n\\nm1\\n\\n25\\n\\nhm2,di\\n\\nm2\\n22\\n\\n1\\n\\n20\\n18\\n\\nd2\\n\\n16\\n14\\n12\\n10\\n\\n0\\n\\n8\\n6\\n\\n10?2\\n\\n4\\n2\\n0\\n\\n?10\\n\\n?5\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n100\\n\\n101\\n\\nNoise ?\\n\\n(a) Geometry of stable solutions. Each stable\\nsolution is selective in expectation for a single\\nmixture. Note that the classical BCM rule will\\nnot converge to these values in the presence of\\nnoise.\\n\\n4\\n\\n10?1\\n\\n25\\n\\n(b) Noise response of triplet BCM update\\nrule vs BCM update. Input data was a mixture of Gaussians with standard deviation ?.\\nThe selectivity of the triplet BCM rule remains unchanged in the presence of noise.\\n\\nTriplet BCM Rule\\n\\nWe now show that by modifying the update rule to incorporate information from triplets of input\\nvectors, the generality of the input data can be dramatically increased. Our new BCM rule will learn\\nselectivity for arbitrary mixture distributions, and learn weights which in expectation are selective\\nfor only one mixture component. Assume that\\nX\\nP (d) =\\n?k Pk (d)\\nk\\n\\nwhere EPk [d] = dk . For example, the data could be a mixture of axis-aligned Gaussians, a mixture\\nof independent Poisson variables, or mixtures of independent Bernoulli random variables to name a\\nfew. We also require EPk [kdk2 ] < ?. We emphasize that we do not require our data to come from\\nany parametric distribution.\\nWe interpret k to be a latent variable that signals the hidden cause of the underlying input distribution, with distribution Pk . Critically, we assume that the hidden variable k changes slowly compared\\nto the inter-spike period of the neuron. In particular, we need at least 3 samples from each Pk . This\\ncorresponds to the multi-view assumption of [2]. A particularly relevant model meeting this assumption is that of spike counts in disjoint intervals under a Poisson process, with a discrete, time\\nvarying rate parameter. For the purpose of this paper, we assume the number of mixed distributions,\\nk, is equal to the number of dimensions, n, however it is possible to relax this to k < n.\\nLet {d1 , d2 , d3 } be a triplet of independent copies from some Pk (d), i.e. each are drawn from\\nthe same mixture. It is critical to note that if {d1 , d2 , d3 } are not drawn from the same class, this\\nupdate will not converge to the global maximum. Numerical experiments show this assumption can\\nbe violated somewhat without severe changes to the fixed points of the algorithm. Our sample\\n\\n i\\n\\u000bis\\ni\\nthen a sequence of triplets, each triplet drawn from the same latent distribution. Let c = d , m .\\nWith these independent triples, we note that the tensors T and M from equation (2) can be written\\nas moments of the independent triplets\\nT = E[d1 ? d2 ? d3 ]\\n\\nM = E[d1 ? d2 ]\\n1\\n1\\nR(m) = T (m, m, m) ? M (m, m)2\\n3\\n4\\nThis is precisely the same objective function optimized by the classical BCM update, with the conditional means of the mixture distributions taking the place of discrete data points. With access to\\nindependent triplets, selectivity for significantly richer input distributions can be learned.\\n5\\n\\n\\fAs with classical BCM, we can perform gradient ascent in this objective function which leads to the\\nexpected update\\nE[?R] = E[c1 c2 d3 + (c1 d2 + c2 d1 )(c3 ? 2?)]\\nwhere ? = E[c1 c2 ]. This update is rather complicated, and couples pre and post synaptic firing rates\\nacross multiple time intervals. Since each ci and di are identically distributed, this expectation is\\nequal to\\nE[c2 (c3 ? ?)d1 ]\\nwhich suggests a much simpler update. This ordering was chosen to match the spike timing dependency of synaptic modification. This update depends on the presynaptic input, and the postsynaptic\\nexcitation in two disjoint time periods.\\nDefinition 4.1 (Full-rank Triplet BCM). The full-rank Triplet BCM update rule is:\\nmt = ?(mt?1 + ?t ?(c2 , c3 , ?t?1 )d1 )\\n(4)\\nP\\nP 2\\n2 3\\n2 3\\nwhere ?(c , c , ?) = c (c ? ?), the step size ?t obeys t ?t ? ?, and t ?t < ?. ? is a\\nprojection into an arbitrary large compact ball, which is needed for technical reasons to guarantee\\nconvergence.\\n\\n5\\n\\nStochastic Approximation\\n\\nHaving found the stable points of the expected update for BCM and triplet BCM, we now turn to\\na proof of convergence for the stochastic update generated by mixture models. For this, we turn to\\nresults from the theory of stochastic approximation.\\nWe will decompose our update into two parts, the expected update, and the (random) deviation.\\nThis deviation will be a L2 bounded martingale, while the expected update will be a ODE with the\\npreviously calculated stable points. Since the expected update is the gradient of a objective function\\nR, the Lyapunov functions required for the stability analysis are simply this objective function.\\nThe decomposition of the triplet BCM stochastic process is as follows:\\nmt ? mt?1 = ?t ?(c2t , c3t , ?t?1 )d1\\n\\n\\u0001\\n= ?n E[?(c2 , c3 , ?t?1 )d1 ] + ?n ?(c2 , c3 , ?t?1 )d1 ? E[?(c2 , c3 , ?t?1 )d1 ]\\n\\n= ?t h(mt ) ? ?t ?t\\n\\nHere, h(mt ) is the deterministic expected update, and ?t is a martingale. All our expectations are\\ntaken with respect to triplets of input data. The decomposition for classical BCM is similar.\\nThis is the Doob decomposition [8] of the sequence. Using a theorem of Delyon [7], we will show\\nthat our triplet BCM algorithm will converge with probability 1 to the stable points of the expected\\nupdate. As was shown previously, these stable points are selective for one and only one mixture\\ncomponent in expectation.\\nTheorem 5.1. For the full rank case, the projected update converges w.p. 1 to the zeros of ??\\nProof. See supplementary material, or an extended discussion in a forthcoming arXiv preprint [13].\\n\\n6\\n\\nTriplet BCM Explains STDP Up to Spike Triplets\\n\\nBiophysically synaptic efficiency in the brain is more closely modeled by spike timing dependent\\nplasticity (STDP). It depends precisely on the interval between pre- and post-synaptic spikes. Initial\\nresearch on spike pairs [15, 3] showed that a presynaptic spike followed in close succession by\\na postsynaptic spike tended to strengthen a synapse, while the reverse timing weakened it. Later\\nwork on natural spike chains [9], triplets of spikes [4, 19], and quadruplets have shown interaction\\neffects beyond pairs. Most closely to ours, recent work by Pfister and Gerstner [17] suggested that\\na synaptic modification function depending only on spike triplets is sufficient to explain all current\\nexperimental data. Furthermore, their rule resembles a BCM learning rule when the pre- and postsynaptic firing distributions are independent Poisson.\\n6\\n\\n\\fWe now demonstrate that our learning rule can model both the pairwise and triplet results from\\nPfister and Gerstner using a smaller number of free parameters and without the introduction of\\nhidden leaky timing variables. Instead, we work directly with the pre- and post-synaptic voltages,\\nand model the natural voltage decay during the falling phase of an action potential. Our (four)\\nfree variables are the voltage decay, which we set within reasonable biological limits; a bin width,\\ncontrolling the distance between spiking triplet periods; ?, our sliding voltage threshold; and an\\noverall multiplicative constant. We emphasize that our model was not designed to fit these data; it\\nwas designed to learn selectivity for the multi-view mixture task. Spike timing dependence falls out\\nas a natural consequence of our multi-view assumption.\\n\\nChange in EPSC Amplitude (%)\\n\\n100\\n\\n50\\n\\n0\\n\\n?50\\n?100\\n\\n?80\\n\\n?60\\n\\n?40\\n\\n?20\\n\\n0\\n20\\nSpike Timing (ms)\\n\\n40\\n\\n60\\n\\n80\\n\\n100\\n\\nFigure 2: Fit of triplet BCM learning rule to synaptic strength STDP curve from [3]. Data points\\nwere recreated from [3] . Spike timing measures the time between post synaptic and presynaptic\\nspikes, tpost ? tpre . A positive time means the presynaptic spike was followed by a postsynaptic\\nspike.\\nWe first model hippocampus data from Mu-ming Poo [3], who applied repeated electrical stimulation to the pre- and post-synaptic neurons in a pairing protocol within which the relative timing of\\nthe two spike chains was varied. After repeated stimulation at a fixed timing offset, the change in\\nsynaptic strength (postsynaptic current) was measured.\\nWe take the average voltage in triplet intervals to be the measure of pre- and post-synaptic activity,\\nand consider a one-dimensional version of our synaptic update:\\n?m = Ac2 (c3 ? ?)d1\\n(5)\\n2\\n3\\nwhere c and c are the postsynaptic voltage averaged over the second and third time bins, and d1\\nis the presynaptic voltage averaged over the first time bin. We assume our pre and post synaptic\\nvoltages are governed by the differential equation:\\ndV\\n= ?? V\\n(6)\\ndt\\nsuch that, if t = sk where sk is the kth spike, V (t) ? 1. That is, the voltage is set to 1 at each spike\\ntime before decaying again.\\nLet Vpre be the presynaptic voltage trace, and Vpost be the postsynaptic voltage trace. They are\\ndetermined by the timing of pre- and post-synaptic spikes, which occur at r1 , r2 , . . . , rn for the\\npresynaptic spikes, and o1 , o2 , . . . om for the postsynaptic spikes.\\nTo model the pairwise experiments, we let ri = r0 + iT where T = 1000ms, a large time constant.\\nThen oi = ri + ?t where ?t is the spike timing. Let ?b be the size of the bins. That is to say,\\nZ t+ ?2b\\nZ t+ ?2b\\n1\\n0\\n0\\n2\\nd (t) =\\nVpre (t + ?b )dt\\nc (t) =\\nVpost (t0 )dt0\\n?b\\n2\\n?\\nt+ 2b\\n\\nt?\\n\\nc3 (t) =\\n\\nZ\\n\\nt?\\n\\n?b\\n2\\n\\nt?\\n\\nVpost (t0 ? ?b )dt0\\n\\n?b\\n2\\n\\nVpost (t) = Vpre (t ? ?t )\\n\\nThen the overall synaptic modification is given by\\nZ\\nAc2 (t)(c3 (t) ? ?)d1 (t)dt\\nt\\n\\n7\\n\\n\\fWe fit A, ? , ?, and the bin size of integration. Recall that the sliding threshold, ? is a function of the\\nexpected firing rate of the neuron. Therefore we would not expect it to be a fixed constant. Instead,\\nit should vary slowly over a time period much longer than the data sampling period. For the purpose\\nof these experiments it would be at an unknown level that depends on the history of neural activity.\\nSee figure 2 for the fit for Mu-ming Poo?s synaptic modification data.\\nFroemke and Dan also investigated higher order spike chains, and found that two spikes in short\\nsuccession did not simply multiply in their effects. This would be the expected result if the spike\\ntiming dependence treated each pair in a triplet as an independent event. Instead, they found that a\\npresynaptic spike followed by two postsynaptic spikes resulted in significantly less excitation than\\nexpected if the two pairs were treated as independent events. They posited that repeated spikes\\ninteracted suppressively, and fit a model based on that hypothesis. They performed two triplet experiments with pre- pre-post triplets, and pre-post-post triplets. Results of their experiment along\\nwith the predictions based on our model are presented in figure 3.\\n\\nFigure 3: Measured excitation and inhibition for spike triplets from Froemke and Dan are demarcated in circles and triangles. A red circle or triangle indicates excitation, while a blue circle or\\ntriangle indicates inhibition. The predicted results from our model are indicated by the background\\ncolor. Numerical results for our model, with boundaries for the Froemke and Dan model are reproduced.\\nLeft figure is pairs of presynaptic spikes, and a single post-synaptic spike. The right figure is pairs of\\npostsynaptic spikes, and a presynaptic spike. For each figure, t1 measures the time between the first\\npaired spike with the singleton spike, with the convention that each t is positive if the presynaptic\\nspike happens before the post synaptic spike. See paired STDP experiments for our spiking model.\\nFor the top figure, ? = .65, our bin width was 2ms, and our spike voltage decay rate ? = 8ms. For\\nthe right figure ? = .45. Red is excitatory, blue is inhibitory, white is no modification. A positive t\\nindicates a presynaptic spike occurred before a postsynaptic spike.\\n\\n7\\n\\nConclusion\\n\\nWe introduced a modified formulation of the classical BCM neural update rule. This update rule\\ndrives the synaptic weights toward the components of a tensor decomposition of the input data.\\nBy further modifying the update to incorporate information from triplets of input data, this tensor decomposition can learn the mixture means for a broad class of mixture distributions. Unlike\\nother methods to fit mixture models, we incorporate a multiview assumption that allows us to learn\\nasymptotically exact mixture means, rather than local maxima of a similarity measure. This is in\\nstark contrast to EM and other gradient ascent based methods, which have limited guarantees about\\nthe quality of their results. Conceptually our model suggests a different view of spike waves during\\nadjacent time epochs: they provide multiple independent samples of the presynaptic ?image.?\\nDue to size constraints, this abstract has has skipped some details, particularly in the experimental\\nsections. More detailed explanations will be provided in future publications.\\nResearch supported by NSF, NIH, The Paul Allen Foundation, and The Simons Foundation.\\n8\\n\\n\\fReferences\\n[1] Animashree Anandkumar, Dean P Foster, Daniel Hsu, Sham M Kakade, and Yi-Kai Liu. Two\\nsvds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. CoRR, abs/1204.6703, 1, 2012.\\n[2] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. arXiv preprint arXiv:1210.7559, 2012.\\n[3] Guo-qiang Bi and Mu-ming Poo. Synaptic modifications in cultured hippocampal neurons:\\ndependence on spike timing, synaptic strength, and postsynaptic cell type. The Journal of\\nNeuroscience, 18(24):10464?10472, 1998.\\n[4] Guo-Qiang Bi and Huai-Xing Wang. Temporal asymmetry in spike timing-dependent synaptic\\nplasticity. Physiology & behavior, 77(4):551?555, 2002.\\n[5] Elie L Bienenstock, Leon N Cooper, and Paul W Munro. Theory for the development of neuron\\nselectivity: orientation specificity and binocular interaction in visual cortex. The Journal of\\nNeuroscience, 2(1):32?48, 1982.\\n[6] Natalia Caporale and Yang Dan. Spike timing-dependent plasticity: a hebbian learning rule.\\nAnnual Review Neuroscience, 31:25?46, 2008.\\n[7] Bernard Delyon. General results on the convergence of stochastic algorithms. Automatic\\nControl, IEEE Transactions on, 41(9):1245?1255, 1996.\\n[8] Joseph L Doob. Stochastic processes, volume 101. New York Wiley, 1953.\\n[9] Robert C Froemke and Yang Dan. Spike-timing-dependent synaptic modification induced by\\nnatural spike trains. Nature, 416(6879):433?438, 2002.\\n[10] Julijana Gjorgjieva, Claudia Clopath, Juliette Audet, and Jean-Pascal Pfister. A triplet\\nspike-timing?dependent plasticity model generalizes the bienenstock?cooper?munro rule to\\nhigher-order spatiotemporal correlations. Proceedings of the National Academy of Sciences,\\n108(48):19383?19388, 2011.\\n[11] DO Hebb. The organization of behavior; a neuropsychological theory. 1949.\\n[12] Nathan Intrator and Leon N Cooper. Objective function formulation of the bcm theory of visual\\ncortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1):3?17,\\n1992.\\n[13] Matthew Lawlor and Steven S. W. Zucker. An online algorithm for learning selectivity to\\nmixture means. arXiv preprint, 2014.\\n[14] Matthew Lawlor and Steven W Zucker. Third-order edge statistics: Contour continuation,\\ncurvature, and cortical connections. In Advances in Neural Information Processing Systems,\\npages 1763?1771, 2013.\\n[15] WB Levy and O Steward. Temporal contiguity requirements for long-term associative potentiation/depression in the hippocampus. Neuroscience, 8(4):791?797, 1983.\\n[16] Bernhard Nessler, Michael Pfeiffer, and Wolfgang Maass. Stdp enables spiking neurons to\\ndetect hidden causes of their inputs. In Advances in neural information processing systems,\\npages 1357?1365, 2009.\\n[17] Jean-Pascal Pfister and Wulfram Gerstner. Triplets of spikes in a model of spike timingdependent plasticity. The Journal of neuroscience, 26(38):9673?9682, 2006.\\n[18] Sen Song, Kenneth D Miller, and Larry F Abbott. Competitive hebbian learning through spiketiming-dependent synaptic plasticity. Nature neuroscience, 3(9):919?926, 2000.\\n[19] Huai-Xing Wang, Richard C Gerkin, David W Nauen, and Guo-Qiang Bi. Coactivation and\\ntiming-dependent integration of synaptic potentiation and depression. Nature neuroscience,\\n8(2):187?193, 2005.\\n\\n9\\n\\n\\f\",\n          \"Gradient Flow Independent Component\\nAnalysis in Micropower VLSI\\n\\nAbdullah Celik, Milutin Stanacevic and Gert Cauwenberghs\\nJohns Hopkins University, Baltimore, MD 21218\\n{acelik,miki,gert}@jhu.edu\\n\\nAbstract\\nWe present micropower mixed-signal VLSI hardware for real-time blind\\nseparation and localization of acoustic sources. Gradient flow representation of the traveling wave signals acquired over a miniature (1cm diameter) array of four microphones yields linearly mixed instantaneous\\nobservations of the time-differentiated sources, separated and localized\\nby independent component analysis (ICA). The gradient flow and ICA\\nprocessors each measure 3mm ? 3mm in 0.5 ?m CMOS, and consume\\n54 ?W and 180 ?W power, respectively, from a 3 V supply at 16 ks/s\\nsampling rate. Experiments demonstrate perceptually clear (12dB) separation and precise localization of two speech sources presented through\\nspeakers positioned at 1.5m from the array on a conference room table.\\nAnalysis of the multipath residuals shows that they are spectrally diffuse,\\nand void of the direct path.\\n\\n1 Introduction\\nTime lags in acoustic wave propagation provide cues to localize an acoustic source from\\nobservations across an array. The time lags also complicate the task of separating multiple\\nco-existing sources using independent component analysis (ICA), which conventionally\\nassumes instantaneous mixture observations.\\nInspiration from biology suggests that for very small aperture (spacing between acoustic\\nsensors i.e., tympanal membranes), small differences (gradients) in sound pressure level\\nare more effective in resolving source direction than actual (microsecond scale) time differences. The remarkable auditory localization capability of certain insects at a small (1%)\\nfraction of the wavelength of the source owes to highly sensitive differential processing\\nof sound pressure through inter-tympanal mechanical coupling [1] or inter-aural coupled\\nneural circuits [2].\\nWe present a mixed-signal VLSI system that operates on spatial and temporal differences\\n(gradients) of the acoustic field at very small aperture to separate and localize mixtures of\\ntraveling wave sources. The real-time performance of the system is characterized through\\nexperiments with speech sources presented through speakers in a conference room setting.\\n\\n\\fs(t)\\ns\\n\\nILD\\n\\n?\\n?\\n\\nx01\\n\\n? s s(t + ?)\\n\\nITD\\n\\n?\\n\\nx-10\\n\\nt\\n\\nu\\n\\ns(t)\\n\\nx10\\n\\nx0-1\\n\\n(a)\\n\\n?2 ?1\\n(b)\\n\\nFigure 1: (a) Gradient flow principle. At low aperture, interaural level differences (ILD)\\nand interaural time differences (ITD) are directly related, scaled by the temporal derivative\\nof the signal. (b) 3-D localization (azimuth ? and elevation ?) of an acoustic source using\\na planar geometry of four microphones.\\n\\n2 Gradient Flow Independent Component Analysis\\nGradient flow [3, 4] is a signal conditioning technique for source separation and localization\\nsuited for arrays of very small aperture, i.e., of dimensions significantly smaller than the\\nshortest wavelength in the sources. The principle is illustrated in Figure 1 (a). Consider a\\ntraveling acoustic wave impinging on an array of four microphones, in the configuration of\\nFigure 1 (b). The 3-D direction cosines of the traveling wave u are implied by propagation\\ndelays ?1 and ?2 in the source along directions p and q in the sensor plane. Direct measurement of these delays is problematic as they require sampling in excess of the bandwidth\\nof the signal, increasing noise floor and power requirements. However, indirect estimates\\nof the delays are obtained, to first order, by relating spatial and temporal derivatives of the\\nacoustic field:\\n?10 (t) ?\\n?01 (t) ?\\n\\n?1 ??00 (t)\\n?2 ??00 (t)\\n\\n(1)\\n\\nwhere ?10 and ?01 represent spatial gradients in p and q directions around the origin (p =\\nq = 0), ?00 the spatial common mode, and ??00 its time derivative. Estimates of ?00 , ?10 and\\n?01 for the sensor geometry of Figure 1 can be obtained as:\\n?00\\n\\n?\\n\\n?10\\n\\n?\\n\\n?01\\n\\n?\\n\\n?\\n\\n1\\n4 x?1,0 + x1,0 +\\n?\\n?\\n1\\n2 x1,0 ? x?1,0\\n?\\n?\\n1\\n2 x0,1 ? x0,?1\\n\\nx0,?1 + x0,1\\n\\n?\\n(2)\\n\\nA single source can be localized by estimating direction cosines ?1 and ?2 from (1), a\\nprinciple known for years in monopulse radar, exploited by parasite insects [1], and implemented in mixed-signal VLSI hardware [6]. As shown in Figure 1 (b), the planar geometry\\nof four microphones allows to localize a source in 3-D, with both azimuth and elevation 1 .\\nMore significantly, multiple coexisting sources s` (t) can be jointly separated and localized\\n1\\n\\nAn alternative using two microphones, exploiting shape of the pinna, is presented in [5]\\n\\n\\fusing essentially the same principle [3, 4]:\\nX\\n?00 (t) =\\ns` (t) + ?00 (t)\\n`\\n\\n?10 (t) =\\n\\nX\\n\\n?1` s? ` (t) + ?10 (t)\\n\\n(3)\\n\\n`\\n\\n?01 (t) =\\n\\nX\\n\\n?2` s? ` (t) + ?01 (t)\\n\\n`\\n\\nwhere ?00 , ?10 and ?01 represent common mode and spatial derivative components of additive noise in the sensor observations. Taking the time derivative of ?00 , we thus obtain\\nfrom the sensors a linear instantaneous mixture of the time-differentiated source signals,\\n?\\n?? 1 ? \\\"\\n? ?\\n#\\ns?\\n1 ??? 1\\n?? 00\\n??00\\n?\\n?\\n.\\n1\\nL\\n? ?10 ? ? ? ?1 ? ? ? ?1 ? ? . ? + ?10 ,\\n(4)\\n.\\n?\\nL\\n?21 ? ? ? ?2L\\n?01\\n01\\ns?\\nan equation in the standard form x = As + n, where x is given and the mixing matrix A\\nand sources s are unknown. Ignoring the noise term n, the problem setting is standard in\\nIndependent Component Analysis (ICA), and three independent sources can be identified\\nfrom the three gradient observations.\\nVarious formulations of ICA exist to arrive at estimates of the unknown s and A from\\nobservations x. ICA algorithms typically specify some sort of statistical independence assumption on the sources s either in distribution over amplitude [7] or over time [8]. Most\\nforms specify ICA to be static, in assuming that the observations contain static (instantaneous) linear mixtures of the sources. Note that this definition of static ICA includes\\nmethods for blind source separation that make use of temporal structure in the dynamics\\nwithin the sources themselves [8], as long as the observed mixture of the sources is static.\\nIn contrast, ?convolutive? ICA techniques explicitly assume convolutive or delayed mixtures in the source observations. Convolutive ICA techniques (e.g., [10]) are usually much\\nmore involved and require a large number of parameters and long adaptation time horizons\\nfor proper convergence.\\nThe instantaneous static formulation of gradient flow (4) is convenient,2 and avoids the need\\nfor non-static (convolutive)\\nP ICA to separate delayed mixtures of traveling wave sources (in\\nfree space) xpq (t) = ` s` (t + p?1 + q?2 ). Reverberation in multipath wave propagation\\ncontributes delayed mixture components in the observations which limit the effectiveness\\nof a static ICA formulation. As shown in the experiments below, static ICA still produces\\nreasonable results (12 dB of perceptually clear separation) in typical enclosed acoustic\\nenvironments (conference room).\\n\\n3 Micropower VLSI Implementation\\nVarious analog VLSI implementations of ICA exist in the literature, e.g., [11, 12], and\\ndigital implementations using DSP are common practice in the field. By adopting a mixedsignal architecture in the implementation, we combine advantages of both approaches: an\\nanalog datapath directly interfaces with inputs and outputs without the need for data conversion; and digital adaptation offers the flexibility of reconfigurable ICA learning rules.\\n2\\nThe time-derivative in the source signals (4) is immaterial, and can be removed by timeintegrating the separated signals obtained by applying ICA directly to the gradient flow signals.\\n\\n\\fW12\\n\\nW13\\n\\nMULTIPLYING DAC\\n\\nW21\\n\\nW22\\n\\nW23\\n\\n?00 ?10\\n.\\n?00 ?01\\n\\n?1\\n?2\\n\\nW31\\n\\nW32\\n\\nW33\\n\\nMULTIPLYING DAC\\n\\nS/H OUTPUT BUFFERS\\n\\nW11\\nLMS REGISTERS\\n\\nICA REGISTERS\\nMULTIPLYING DAC\\n\\nLMS REGISTERS\\n\\n(a)\\n\\n(b)\\n\\nFigure 2: (a) Gradient flow processor. (b) Reconfigurable ICA processor.\\nDimensions of both processors are 3mm ? 3mm in 0.5 ?m CMOS technology.\\n?C\\n\\n-1, 0, +1\\n\\nW11\\n\\nW12\\n\\nW13\\n\\nW21\\n\\nW22\\n\\nW23\\n\\nW31\\n\\nW32\\n\\nW33\\n\\nx2\\n\\ny1\\ny2\\n\\nWij\\n\\ny3\\n\\nyj\\nxi\\n\\nlevel comparison\\n\\nx1\\n\\nlevel comparison\\n\\nupdate bits\\n\\noutput bits\\n\\n-1, 0, +1\\n\\nupdate bits\\n\\nx3\\n\\nFigure 3: Reconfigurable mixed-signal ICA architecture implementing general outerproduct forms of ICA update rules.\\n3.1 Gradient Flow Processor\\nThe mixed-signal VLSI processor implementing gradient flow is presented in [6]. A micrograph of the chip is shown in Figure 2 (a). Precise analog gradients ??00 , ?10 and ?01 are acquired from the microphone signals by correlated double sampling (CDS) in fully differential switched-capacitor circuits. Least-mean-squares (LMS) cancellation of common-mode\\nleakage in the gradient signals further increases differential sensitivity. The adaptation\\nis performed in the digital domain using counting registers, and couples to the switchedcapacitor circuits using capacitive multiplying DAC arrays. An additional stage of LMS\\nadaptation produces digital estimates of direction cosines ?1 and ?2 for a single source.\\nIn the present setup this stage is bypassed, and the common-mode corrected gradient signals are presented as inputs to the ICA chip for localization and separation of up to three\\nindependent sources.\\n3.2 Reconfigurable ICA Processor\\nA general mixed-signal parallel architecture, that can be configured for implementation of\\nvarious ICA update rules in conjunction with gradient flow, is shown in Figure 3 [9]. Here\\n\\n\\fwe briefly illustrate the architecture with a simple configuration designed to separate two\\nsources, and present CMOS circuits that implement the architecture. The micrograph of\\nthe reconfigurable ICA chip is shown in Figure 2 (a).\\n3.2.1 ICA update rule\\nEfficient implementation in parallel architecture requires a simple form of the update rule,\\nthat avoids excessive matrix multiplications and inversions. A variety of ICA update algorithms can be cast in a common, unifying framework of outer-product rules [9].\\nTo obtain estimates y = ?s of the sources s, a linear transformation with matrix W is applied\\nto the gradient signals x, y = Wx. Diagonal terms are fixed wii ? 1, and off-diagonal\\nterms adapt according to\\n?wij = ?? f (yi )g(yj ),\\n\\ni 6= j\\n\\n(5)\\n\\nThe implemented update rule can be seen as the gradient of InfoMax [7] multiplied by\\nWT , rather than the natural gradient multiplication factor WT W. To obtain the full natural gradient in outer-product form, it is necessary to include a back-propagation path in\\nthe network architecture, and thus additional silicon resources, to implement the vector\\ncontribution yT . Other equivalences with standard ICA algorithms are outlined in [9].\\n3.2.2 Architecture\\nLevel comparison provides implementation of discrete approximations of any scalar function f (y) and g(y) appearing in different learning rules. Since speech signals are approximately Laplacian distributed, the nonlinear scalar function f (y) is approximated by sign(y)\\nand implemented using single bit quantization. Conversely, a linear function g(y) ? y in\\nthe learning rule is approximated by a 3-level staircase function (?1, 0, +1) using 2-bit\\nquantization. The quantization of the f and g terms in the update rule (5) simplifies the\\nimplementation to that of discrete counting operations.\\nThe functional block diagram of a 3 ? 3 outer-product incremental ICA architecture, supporting a quantized form of the general update rule (5), is shown in Figure 3 [9]. Un-mixing\\ncoefficients are stored digitally in each cell of the architecture. The update is performed\\nlocally by once or repeatedly incrementing, decrementing or holding the current value of\\ncounter based on the learning rule served by the micro-controller. The 8 most significant\\nbits of the 14-bit counter holding and updating the coefficients are presented to a multiplying D/A capacitor array [6] to linearly unmix the separated signal. The remaining 6\\nbits in the coefficient registers provide flexibility in programming the update rate to tailor\\nconvergence.\\n3.2.3 Circuit implementation\\nAs in the implementation of the gradient flow processor [6], the mixed-signal ICA architecture is implemented using fully differential switched-capacitor sampled-data circuits.\\nCorrelated double sampling performs common mode offset rejection and 1/f noise reduction. An external micro-controller provides flexibility in the implementation of different\\nlearning rules. The ICA architecture is integrated on a single 3mm ? 3mm chip fabricated\\nin 0.5 ?m 3M2P CMOS technology.\\nThe block diagram of ICA prototype in Figure 3 indicates its main functionality is a\\nvector(3x1)-matrix(3x3) multiplication with adaptive matrix elements.\\nEach cell in the implemented architecture contains a 14-bit counter, decoder and D/A capacitor arrays. Adaptation is performed in outer-product fashion by incrementing, decrementing or holding the current value of the counters. The most significant 8 bits of the\\n\\n\\f?1e\\n+ ^\\n?\\n\\nyi\\n\\nA\\nC2\\nWijC\\n\\n(1-Wij)C\\n\\n1\\n\\n?2\\nC3\\n\\nVref\\n\\nC2\\n\\n?1\\n\\n(1-Wij)C\\n\\nx +j\\n\\nWijC\\n\\ny -i\\n\\nC3\\n\\nA\\n\\nA\\n\\nsgn(yi-Vth)\\n\\nC4\\n\\n?2\\nA\\n\\n?^1e\\n\\n?^1e\\n\\n?1\\n\\nVth\\n\\n?^2\\n\\n?1e\\n\\nx -j\\n\\nFigure 4: Correlated double sampling (CDS) switched-capacitor fully differential circuits\\nimplementing linearly weighted summing in the mixed-signal ICA architecture.\\n\\n1 cm\\n1.5 m\\n\\n1.5 m\\n\\nFigure 5: Experimental setup for separation of two acoustic sources in a conference room\\nenviroment.\\ncounter are presented to the multiplying D/A capacitor arrays to construct the source estimation. Figure 4 shows the circuits one output component in the architecture, linearly\\nsumming the input contributions. The implementation of the multiplying capacitor arrays\\nare identical to those discussed in [6]. Each output signal yi is is computed by accumulating outputs from the all the cells in the ith row. The accumulation is performed on C2 by\\nswitch-cap amplifier yielding the estimated signals during ?2 phase. While the estimation\\n? 1 by the comparator circuit. The sign of the comsignals are valid, yi + is sampled at ?\\nparison of yi with variable level threshold Vth is computed in the evaluate phase, through\\ncapacitive coupling into the amplifier input node.\\n\\n4 Experimental Results\\nTo demonstrate source separation and localization in a real environment, the mixed-signal\\nVLSI ASICs were interfaced with four omnidirectional miniature microphones (Knowles\\nFG-3629), arranged in a circular array with radius 0.5 cm. At the front-end, the microphone\\nsignals were passed through second-order bandpass filters with low-frequency cutoff at\\n130 Hz and high-frequency cutoff at 4.3 kHz. The signals were also amplified by a factor\\nof 20.\\nThe experimental setup is shown in Figure 5. The speech signals were presented through\\nloudspeakers positioned at 1.5 m distance from the array. The system sampling frequency\\nof both chips was set to 16 kHz. A male and female speakers from TIMIT database were\\nchosen as sound sources. To provide the ground truth data and full characterization of the\\nsystems, speech segments were presented individually through either loudspeaker at different time instances. The data was recorded for both speakers, archived, and presented to the\\n\\n\\f?10\\n\\n5\\n\\n?01\\n\\n2\\n\\ns^ 1\\n\\n2\\n\\ns^ 2\\n\\n1\\n\\n0\\n0\\n2\\n\\n1\\n\\n2\\n\\n3\\n\\n0\\n0\\n2\\n\\n1\\n\\n2\\n\\n3\\n\\n0\\n0\\n1\\n\\n1\\n\\n2\\n\\n3\\n\\n0\\n0\\n1\\n\\n1\\n\\n2\\n\\n3\\n\\n0\\n\\n1\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\nFrequency Frequency Frequency Frequency Frequency\\n\\n?00\\n\\n5\\n\\n1\\n0.5\\n0\\n0\\n1\\n\\n1\\n\\n2\\n4\\n\\nx 10\\n0.5\\n0\\n0\\n1\\n\\n1\\n\\n2\\n4\\n\\nx 10\\n0.5\\n0\\n0\\n1\\n\\n1\\n\\n2\\n4\\n\\nx 10\\n0.5\\n0\\n0\\n1\\n\\n1\\n\\n2\\n4\\n\\nx 10\\n0.5\\n0\\n0\\n\\n1\\n\\nTime (s)\\n\\n2\\n\\nTime\\n\\n4\\n\\nx 10\\n\\nFigure 6: Time waveforms and spectrograms of the presented sources s1 and s2 , observed\\ncommon-mode and gradient signals ?00 , ?10 and ?01 by the gradient flow chip, and recovered sources s?1 and s?2 by the ICA chip.\\nTable 1: Localization Performance\\n\\nSingle-source LMS localization\\nDual-source ICA localization\\n\\nMale speaker\\n-31.11\\n-30.35\\n\\nFemale speaker\\n40.95\\n43.55\\n\\ngradient flow chip. Localization results obtained by gradient flow chip through LMS adaptation are reported in Table 1. The two recorded datasets were then added, and presented to\\nthe gradient flow ASIC. The gradient signals obtained from the chip were then presented\\nto the ICA processor, configured to implement the outerproduct update algorithm in (5).\\nThe observed convergence time was around 2 seconds. From the recorded 14-bit digital\\nweights, the angles of incidence of the sources relative to the array were derived. These\\nestimated angles are reported in Table 1. As seen, the angles obtained through LMS bearing estimation under individual source presentation are very close to the angles produced\\nby ICA under joint presentation of both sources. The original sources and the recorded\\nsource signal estimates, along with recorded common-mode signal and first-order spatial\\ngradients, are shown in Figure 6.\\n\\n5 Conclusions\\nWe presented a mixed-signal VLSI system that operates on spatial and temporal differences\\n(gradients) of the acoustic field at very small aperture to separate and localize mixtures of\\ntraveling wave sources. The real-time performance of the system was characterized through\\nexperiments with speech sources presented through speakers in a conference room setting.\\nAlthough application of static ICA is limited by reverberation, the perceptual quality of the\\nseparated outputs owes to the elimination of the direct path in the residuals. Miniature size\\nof the microphone array enclosure (1 cm diameter) and micropower consumption of the\\nVLSI hardware (250 ?W) are key advantages of the approach, with applications to hearing\\n\\n\\faids, conferencing, multimedia, and surveillance.\\nAcknowledgments\\nThis work was supported by grants of the Catalyst Foundation (New York), the National\\nScience Foundation, and the Defense Intelligence Agency.\\n\\nReferences\\n[1] D. Robert, R.N. Miles, and R.R. Hoy, ?Tympanal Hearing in the Sarcophagid Parasitoid Fly Emblemasoma sp.: the Biomechanics of Directional Hearing,? J. Experimental Biology, vol. 202, pp 1865-1876, 1999.\\n[2] R. Reeve and B. Webb, ?New neural circuits for robot phonotaxis?, Philosophical\\nTransactions of the Royal Society A, vol. 361, pp. 2245-2266, 2002.\\n[3] G. Cauwenberghs, M. Stanacevic, and G. Zweig, ?Blind Broadband Source Localization and Separation in Miniature Sensor Arrays,? Proc. IEEE Int. Symp. Circuits and\\nSystems (ISCAS?2001), Sydney, Australia, May 6-9, 2001.\\n[4] J. Barr`ere and G. Chabriel, ?A Compact Sensor Array for Blind Separation of Sources?,\\nIEEE Transactions Circuits and Systems, Part I, vol. 49 (5), pp. 565-574, 2002.\\n[5] J.G. Harris, C.-J. Pu, J.C. Principe, ?A Neuromorphic Monaural Sound Localizer,?\\nProc. Neural Inf. Proc. Sys. (NIPS*1998), Cambridge MA: MIT Press, vol. 10, pp.\\n692-698, 1999.\\n[6] G. Cauwenberghs and M. Stanacevic, ?Micropower Mixed-Signal Acoustic Localizer,?\\nProc. IEEE Eur. Solid State Circuits Conf. (ESSCIRC?2003), Estoril Portugal, Sept. 1618, 2003.\\n[7] A.J. Bell and T.J. Sejnowski, ?An Information Maximization Approach to Blind Separation and Blind Deconvolution,? Neural Comp, vol. 7 (6), pp 1129-1159, Nov 1995.\\n[8] L. Molgedey and G. Schuster, ?Separation of a mixture of independent signals using\\ntime delayed correlations,? Physical Review Letters, vol. 72, no. 23, pp. 3634?3637,\\n1994.\\n[9] A. Celik, M. Stanacevic and G. Cauwenberghs, ?Mixed-Signal Real-Time Adaptive\\nBlind Source Separation,? Proc. IEEE Int. Symp. Circuits and Systems (ISCAS?2004),\\nVancouver Canada, May 23-26, 2004.\\n[10] R. Lambert and A. Bell, ?Blind separation of multiple speakers in a multipath environment,? Proc. ICASSP?97, M?unich, 1997.\\n[11] Cohen, M.H., Andreou, A.G. ?Analog CMOS Integration and Experimentation with\\nan Autoadaptive Independent Component Analyzer,? IEEE Trans. Circuits and Systems\\nII, vol 42 (2), pp 65-77, Feb. 1995.\\n[12] Gharbi, A.B.A., Salam, F.M.A. ?Implementation and Test Results of a Chip for the\\nSeparation of Mixed Signals,? Proc. Int. Symp. Circuits and Systems (ISCAS?95), May\\n1995.\\n[13] M. Cohen and G. Cauwenberghs, ?Blind Separation of Linear Convolutive Mixtures\\nthrough Parallel Stochastic Optimization,? Proc. IEEE Int. Symp. Circuits and Systems\\n(ISCAS?98), Monterey CA, vol. 3, pp. 17-20, 1998.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHDICRO_vSki"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of `paper_text` column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "1KmUzopjvSkj",
        "outputId": "a29b58b1-c279-4ef9-c081-63df35a7069b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "831     building predictive models from fractal\\nrepre...\n",
              "1136    minimax probability machine\\n\\ngert rg lanckri...\n",
              "4086    on the sample complexity of robust pca\\n\\nmatt...\n",
              "4601    lexical and hierarchical topic regression\\n\\nv...\n",
              "5397    efficient non-greedy optimization of decision ...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>831</th>\n",
              "      <td>building predictive models from fractal\\nrepre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1136</th>\n",
              "      <td>minimax probability machine\\n\\ngert rg lanckri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4086</th>\n",
              "      <td>on the sample complexity of robust pca\\n\\nmatt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4601</th>\n",
              "      <td>lexical and hierarchical topic regression\\n\\nv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5397</th>\n",
              "      <td>efficient non-greedy optimization of decision ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7iLJbjXvSkj"
      },
      "source": [
        "** **\n",
        "#### Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
        "** **\n",
        "\n",
        "To verify whether the preprocessing, we’ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "oD8aJ9aEvSkj",
        "outputId": "236f7f1b-cd56-4b17-bcd6-599745e22d84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buz6NcNVvSkj"
      },
      "source": [
        "** **\n",
        "#### Step 4: Prepare text for LDA analysis <a class=\"anchor\\\" id=\"data_preparation\"></a>\n",
        "** **\n",
        "\n",
        "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui5TI_emvSkj",
        "outputId": "0b3a5c4d-9681-4bb1-e6a9-379ee6c694f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['building', 'predictive', 'models', 'fractal', 'representations', 'symbolic', 'sequences', 'peter', 'tioo', 'georg', 'dorffner', 'austrian', 'research', 'institute', 'artificial', 'intelligence', 'schottengasse', 'vienna', 'austria', 'petertgeorg', 'aiunivieacat', 'abstract', 'propose', 'novel', 'approach', 'building', 'finite', 'memory', 'predictive', 'models']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_i_FZpUvSkj",
        "outputId": "434a764c-bbe4-4ee1-bdc0-bb912b298dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 2), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 5), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 9), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1), (23, 5), (24, 2), (25, 1), (26, 1), (27, 2), (28, 2), (29, 1)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ol98wu4vSkj"
      },
      "source": [
        "** **\n",
        "#### Step 5: LDA model tranining <a class=\"anchor\\\" id=\"train_model\"></a>\n",
        "** **\n",
        "\n",
        "To keep things simple, we'll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxCrfOWpvSkj",
        "outputId": "32ce0665-8a7d-4949-e773-35073ccd0f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.007*\"algorithm\" + 0.006*\"model\" + 0.005*\"time\" + 0.005*\"learning\" + '\n",
            "  '0.004*\"data\" + 0.004*\"set\" + 0.004*\"function\" + 0.003*\"using\" + '\n",
            "  '0.003*\"training\" + 0.003*\"models\"'),\n",
            " (1,\n",
            "  '0.006*\"model\" + 0.005*\"learning\" + 0.005*\"function\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"data\" + 0.004*\"number\" + 0.004*\"figure\" + 0.004*\"one\" + 0.004*\"set\" '\n",
            "  '+ 0.004*\"training\"'),\n",
            " (2,\n",
            "  '0.005*\"model\" + 0.005*\"data\" + 0.005*\"set\" + 0.005*\"time\" + '\n",
            "  '0.004*\"learning\" + 0.004*\"algorithm\" + 0.003*\"training\" + 0.003*\"two\" + '\n",
            "  '0.003*\"number\" + 0.003*\"used\"'),\n",
            " (3,\n",
            "  '0.006*\"model\" + 0.005*\"learning\" + 0.005*\"data\" + 0.005*\"time\" + '\n",
            "  '0.004*\"algorithm\" + 0.004*\"set\" + 0.004*\"using\" + 0.004*\"one\" + '\n",
            "  '0.003*\"based\" + 0.003*\"figure\"'),\n",
            " (4,\n",
            "  '0.005*\"using\" + 0.005*\"algorithm\" + 0.004*\"set\" + 0.004*\"number\" + '\n",
            "  '0.004*\"learning\" + 0.004*\"one\" + 0.004*\"data\" + 0.004*\"time\" + '\n",
            "  '0.004*\"figure\" + 0.004*\"function\"'),\n",
            " (5,\n",
            "  '0.006*\"algorithm\" + 0.006*\"learning\" + 0.005*\"model\" + 0.004*\"time\" + '\n",
            "  '0.004*\"function\" + 0.004*\"set\" + 0.004*\"number\" + 0.004*\"data\" + '\n",
            "  '0.003*\"one\" + 0.003*\"using\"'),\n",
            " (6,\n",
            "  '0.007*\"model\" + 0.006*\"algorithm\" + 0.005*\"data\" + 0.005*\"set\" + '\n",
            "  '0.004*\"learning\" + 0.004*\"number\" + 0.004*\"time\" + 0.004*\"problem\" + '\n",
            "  '0.004*\"function\" + 0.003*\"using\"'),\n",
            " (7,\n",
            "  '0.008*\"model\" + 0.007*\"learning\" + 0.006*\"data\" + 0.005*\"set\" + '\n",
            "  '0.004*\"time\" + 0.004*\"using\" + 0.004*\"function\" + 0.004*\"two\" + '\n",
            "  '0.004*\"algorithm\" + 0.003*\"network\"'),\n",
            " (8,\n",
            "  '0.007*\"algorithm\" + 0.007*\"data\" + 0.007*\"learning\" + 0.006*\"model\" + '\n",
            "  '0.005*\"set\" + 0.004*\"problem\" + 0.004*\"one\" + 0.004*\"function\" + '\n",
            "  '0.003*\"given\" + 0.003*\"using\"'),\n",
            " (9,\n",
            "  '0.007*\"algorithm\" + 0.005*\"set\" + 0.004*\"learning\" + 0.004*\"data\" + '\n",
            "  '0.004*\"function\" + 0.003*\"using\" + 0.003*\"two\" + 0.003*\"number\" + '\n",
            "  '0.003*\"time\" + 0.003*\"model\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rQV2kXNvSkj"
      },
      "source": [
        "** **\n",
        "#### Step 6: Analyzing our LDA model <a class=\"anchor\\\" id=\"results\"></a>\n",
        "** **\n",
        "\n",
        "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
        "\n",
        "1. Better understanding and interpreting individual topics, and\n",
        "2. Better understanding the relationships between the topics.\n",
        "\n",
        "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
        "\n",
        "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KHDIvivYvSkk",
        "outputId": "393b534b-c686-42c0-a758-bdd2fc347fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "8      0.010748 -0.001351       1        1  25.398878\n",
              "1     -0.000381 -0.009348       2        1  15.848127\n",
              "7     -0.006757  0.001485       3        1  13.414655\n",
              "5      0.004397  0.004653       4        1  10.908174\n",
              "6      0.001622  0.000923       5        1   9.290260\n",
              "3     -0.006361 -0.003070       6        1   6.652055\n",
              "2     -0.007995  0.004275       7        1   5.858240\n",
              "9      0.005932  0.005331       8        1   5.716513\n",
              "4      0.001823 -0.005507       9        1   3.467610\n",
              "0     -0.003027  0.002610      10        1   3.445487, topic_info=            Term         Freq        Total Category  logprob  loglift\n",
              "17     algorithm  1242.000000  1242.000000  Default  30.0000  30.0000\n",
              "683         time   849.000000   849.000000  Default  29.0000  29.0000\n",
              "403        model  1328.000000  1328.000000  Default  28.0000  28.0000\n",
              "583          set  1006.000000  1006.000000  Default  27.0000  27.0000\n",
              "724        using   777.000000   777.000000  Default  26.0000  26.0000\n",
              "...          ...          ...          ...      ...      ...      ...\n",
              "427          one    20.002522   786.426510  Topic10  -5.9511  -0.3035\n",
              "421       number    19.582434   797.176995  Topic10  -5.9723  -0.3383\n",
              "1225  algorithms    15.314764   435.925389  Topic10  -6.2181   0.0195\n",
              "173    different    15.646226   477.907993  Topic10  -6.1967  -0.0511\n",
              "495      problem    17.014523   689.451989  Topic10  -6.1129  -0.3337\n",
              "\n",
              "[865 rows x 6 columns], token_table=       Topic      Freq           Term\n",
              "term                                 \n",
              "9844       4  0.366267  acceptability\n",
              "11125      3  0.280127         accrue\n",
              "773        1  0.293387       accuracy\n",
              "773        2  0.119528       accuracy\n",
              "773        3  0.097796       accuracy\n",
              "...      ...       ...            ...\n",
              "5642       6  0.018072             zt\n",
              "5642       7  0.027108             zt\n",
              "5642       8  0.108433             zt\n",
              "5642       9  0.036144             zt\n",
              "5642      10  0.018072             zt\n",
              "\n",
              "[3875 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[9, 2, 8, 6, 7, 4, 3, 10, 5, 1])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el22081377346738212321297887883\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el22081377346738212321297887883_data = {\"mdsDat\": {\"x\": [0.010747912982228618, -0.0003813566056109533, -0.006756962513934988, 0.0043967483325792625, 0.0016215063575844711, -0.00636139014731274, -0.007994506310805822, 0.005931683228385231, 0.0018232358204580768, -0.003026871143571162], \"y\": [-0.0013510628958841278, -0.009348207473064345, 0.0014849626595912168, 0.004652650045513144, 0.0009231204870262264, -0.0030701804429290056, 0.004274820668118492, 0.00533102291347212, -0.005507122928074148, 0.002609996966230421], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [25.3988775573455, 15.84812734497531, 13.414655283546029, 10.9081738195694, 9.2902603229277, 6.652055493195793, 5.858239737641364, 5.716513469877081, 3.467610447858479, 3.4454865230633422]}, \"tinfo\": {\"Term\": [\"algorithm\", \"time\", \"model\", \"set\", \"using\", \"data\", \"learning\", \"number\", \"function\", \"training\", \"figure\", \"used\", \"one\", \"two\", \"network\", \"neural\", \"based\", \"models\", \"results\", \"different\", \"given\", \"space\", \"log\", \"large\", \"analysis\", \"also\", \"input\", \"case\", \"problem\", \"method\", \"xjd\", \"mnl\", \"purchase\", \"permutations\", \"hourly\", \"lem\", \"subsquares\", \"jd\", \"consumers\", \"lle\", \"msc\", \"ajd\", \"kfd\", \"smce\", \"cart\", \"prefers\", \"revenues\", \"regularizer\", \"adversaries\", \"nq\", \"cij\", \"jagabathula\", \"assortment\", \"marketing\", \"connor\", \"customer\", \"casasent\", \"xjik\", \"load\", \"miiller\", \"revenue\", \"ham\", \"mlp\", \"sparsest\", \"fs\", \"queries\", \"zt\", \"embedding\", \"timescales\", \"epiphany\", \"points\", \"regularization\", \"cover\", \"oracle\", \"algorithm\", \"data\", \"solve\", \"problem\", \"manifold\", \"space\", \"algorithms\", \"solution\", \"optimization\", \"learning\", \"loss\", \"yt\", \"given\", \"classification\", \"examples\", \"set\", \"one\", \"results\", \"model\", \"information\", \"yi\", \"convergence\", \"used\", \"convex\", \"approach\", \"error\", \"also\", \"function\", \"distribution\", \"using\", \"two\", \"section\", \"number\", \"state\", \"figure\", \"rate\", \"training\", \"time\", \"different\", \"models\", \"based\", \"neural\", \"fluctuating\", \"shinomoto\", \"bin\", \"ramp\", \"jpu\", \"superfluous\", \"rudemo\", \"ordinary\", \"kyoto\", \"hinge\", \"fluctuation\", \"pu\", \"mise\", \"psth\", \"extrapolated\", \"inlier\", \"sugiyama\", \"shimazaki\", \"elkan\", \"blanchard\", \"tokyo\", \"practically\", \"tokyoacjp\", \"anequal\", \"plessis\", \"cn\", \"scandinavian\", \"spotting\", \"discontinuous\", \"extrapolating\", \"prior\", \"penalty\", \"obeys\", \"cost\", \"dt\", \"fom\", \"matrix\", \"atoms\", \"completion\", \"dps\", \"posterior\", \"mixture\", \"class\", \"function\", \"ti\", \"figure\", \"mean\", \"samples\", \"training\", \"latent\", \"spike\", \"number\", \"distribution\", \"rate\", \"model\", \"rule\", \"classification\", \"speech\", \"loss\", \"one\", \"learning\", \"analysis\", \"case\", \"results\", \"data\", \"algorithm\", \"using\", \"two\", \"set\", \"probability\", \"time\", \"problem\", \"also\", \"neural\", \"given\", \"based\", \"new\", \"log\", \"models\", \"used\", \"oscillator\", \"tempo\", \"delay\", \"vmax\", \"kca\", \"anticipation\", \"bodenhausen\", \"adjusting\", \"trill\", \"ernst\", \"mobility\", \"ringach\", \"rodent\", \"ext\", \"capacitance\", \"wee\", \"ykpred\", \"waibel\", \"accrue\", \"nvm\", \"hat\", \"rescaled\", \"calcium\", \"rodents\", \"lag\", \"pain\", \"std\", \"phoneme\", \"inh\", \"telephone\", \"delays\", \"exc\", \"channels\", \"anticipatory\", \"network\", \"correlogram\", \"input\", \"cells\", \"windows\", \"units\", \"receptive\", \"networks\", \"model\", \"weights\", \"action\", \"speed\", \"neural\", \"driven\", \"orientation\", \"learning\", \"goal\", \"layer\", \"tracking\", \"experiments\", \"data\", \"set\", \"two\", \"time\", \"different\", \"using\", \"used\", \"distribution\", \"training\", \"see\", \"function\", \"number\", \"probability\", \"also\", \"one\", \"error\", \"figure\", \"based\", \"state\", \"algorithm\", \"log\", \"results\", \"problem\", \"given\", \"first\", \"models\", \"mkc\", \"kl\", \"contrastive\", \"ctt\", \"grafting\", \"ink\", \"qai\", \"tlh\", \"xsi\", \"contraction\", \"bb\", \"lumping\", \"leaf\", \"iti\", \"gutmann\", \"erfinv\", \"shields\", \"qsq\", \"irhl\", \"ffl\", \"hyvarinen\", \"irh\", \"lel\", \"tangential\", \"pai\", \"operators\", \"bg\", \"db_html\", \"acceptability\", \"ross\", \"games\", \"ce\", \"cbmpi\", \"argmax\", \"tensors\", \"growth\", \"weak\", \"pkq\", \"tree\", \"trees\", \"branching\", \"xa\", \"node\", \"branch\", \"operator\", \"modes\", \"board\", \"policy\", \"decision\", \"gradient\", \"recovery\", \"mcts\", \"tetris\", \"structure\", \"max\", \"time\", \"function\", \"algorithm\", \"divergence\", \"learning\", \"small\", \"given\", \"non\", \"number\", \"log\", \"used\", \"one\", \"using\", \"two\", \"also\", \"value\", \"parameters\", \"model\", \"set\", \"space\", \"models\", \"training\", \"data\", \"case\", \"large\", \"based\", \"problem\", \"figure\", \"probability\", \"distribution\", \"first\", \"results\", \"flid\", \"supermodular\", \"krause\", \"fragment\", \"flic\", \"fldc\", \"pmg\", \"submodular\", \"maxia\", \"registries\", \"tarantula\", \"psms\", \"furniture\", \"tsm\", \"diaper\", \"mrr\", \"toys\", \"coin\", \"aki\", \"revisions\", \"complements\", \"andreas\", \"failing\", \"substitutes\", \"rishabh\", \"basket\", \"unigram\", \"matroids\", \"dpps\", \"inspect\", \"executions\", \"defect\", \"defective\", \"modular\", \"bug\", \"execution\", \"trace\", \"wn\", \"fj\", \"code\", \"bernoulli\", \"som\", \"models\", \"beliefs\", \"gain\", \"model\", \"parameters\", \"performance\", \"first\", \"based\", \"problem\", \"number\", \"may\", \"algorithm\", \"data\", \"time\", \"test\", \"linear\", \"figure\", \"large\", \"set\", \"sets\", \"using\", \"functions\", \"computer\", \"different\", \"two\", \"log\", \"function\", \"learning\", \"space\", \"optimal\", \"network\", \"point\", \"new\", \"one\", \"probability\", \"used\", \"results\", \"also\", \"given\", \"distribution\", \"cgm\", \"rowley\", \"feraud\", \"cgms\", \"raviv\", \"cg\", \"cgml\", \"usenix\", \"pun\", \"sung\", \"lannioncnetfr\", \"pix\", \"bernier\", \"bfl\", \"listen\", \"mirroring\", \"alarms\", \"swn\", \"fun\", \"funlx\", \"vlx\", \"nix\", \"faces\", \"dimensionnality\", \"wnw\", \"fjj\", \"collobert\", \"sussex\", \"funx\", \"ouput\", \"alarm\", \"turned\", \"face\", \"cmu\", \"perceived\", \"efficacy\", \"ensemble\", \"vlmms\", \"array\", \"inhibitory\", \"detector\", \"network\", \"presynaptic\", \"vm\", \"iv\", \"chip\", \"cell\", \"ganglion\", \"time\", \"based\", \"experiments\", \"type\", \"case\", \"using\", \"independence\", \"conditional\", \"cells\", \"neural\", \"stimulus\", \"likelihood\", \"model\", \"inputs\", \"thus\", \"map\", \"non\", \"unit\", \"one\", \"first\", \"figure\", \"set\", \"data\", \"results\", \"learning\", \"however\", \"two\", \"given\", \"different\", \"points\", \"algorithm\", \"used\", \"training\", \"problem\", \"linear\", \"error\", \"space\", \"models\", \"number\", \"function\", \"distribution\", \"also\", \"pooling\", \"rcl\", \"rcnn\", \"cnn\", \"rcls\", \"rcpn\", \"tighe\", \"na\", \"unfolded\", \"zijk\", \"bauer\", \"ycp\", \"shelhamer\", \"socher\", \"stitch\", \"finetuned\", \"superpixel\", \"kosecka\", \"gpu\", \"cnns\", \"hyperacuity\", \"bptt\", \"sutskever\", \"feed\", \"unbalanced\", \"pointer\", \"converter\", \"rcnns\", \"semantic\", \"downsampled\", \"periphery\", \"organizing\", \"convolutional\", \"object\", \"retina\", \"rnn\", \"stimulus\", \"eye\", \"attentional\", \"recurrent\", \"map\", \"patch\", \"flow\", \"mixing\", \"recognition\", \"neurons\", \"image\", \"target\", \"window\", \"neighborhood\", \"time\", \"network\", \"images\", \"multi\", \"training\", \"used\", \"set\", \"cells\", \"data\", \"two\", \"different\", \"model\", \"figure\", \"number\", \"learning\", \"output\", \"methods\", \"input\", \"algorithm\", \"let\", \"parameters\", \"problem\", \"also\", \"space\", \"using\", \"one\", \"neural\", \"distribution\", \"given\", \"models\", \"based\", \"function\", \"kyt\", \"kzt\", \"smo\", \"folos\", \"lt\", \"rcv\", \"pcmac\", \"loading\", \"rahim\", \"jat\", \"kxzt\", \"raphson\", \"reserve\", \"alpha\", \"belgium\", \"shogun\", \"rearranging\", \"ya\", \"kernelization\", \"bertsimas\", \"facing\", \"adp\", \"leighton\", \"ye\", \"ionosphere\", \"boils\", \"rounds\", \"sage\", \"twelve\", \"valuation\", \"offered\", \"cbmpi\", \"buyer\", \"optimisation\", \"board\", \"price\", \"zt\", \"modes\", \"cp\", \"yt\", \"correlations\", \"ce\", \"cpu\", \"xt\", \"tetris\", \"factor\", \"kernels\", \"labeling\", \"algorithm\", \"analysis\", \"linear\", \"kernel\", \"set\", \"log\", \"result\", \"function\", \"features\", \"value\", \"let\", \"convex\", \"two\", \"mkl\", \"optimization\", \"large\", \"tree\", \"matrix\", \"using\", \"results\", \"number\", \"data\", \"learning\", \"time\", \"problem\", \"one\", \"used\", \"given\", \"algorithms\", \"training\", \"error\", \"model\", \"parameters\", \"performance\", \"approach\", \"based\", \"also\", \"models\", \"figure\", \"annotators\", \"annotator\", \"etal\", \"bluebird\", \"spammers\", \"valence\", \"yij\", \"wsd\", \"categorical\", \"turk\", \"ev\", \"crowdsourced\", \"lesion\", \"raykar\", \"jc\", \"ordinal\", \"spammer\", \"radiologist\", \"wosi\", \"monetary\", \"bonuses\", \"aucj\", \"jck\", \"bot\", \"prevalence\", \"rosales\", \"temp\", \"florin\", \"flipping\", \"kaj\", \"pr\", \"score\", \"crowdsourcing\", \"gold\", \"kj\", \"ipeirotis\", \"rankings\", \"median\", \"talkers\", \"labels\", \"label\", \"instances\", \"using\", \"dimension\", \"number\", \"large\", \"one\", \"accuracy\", \"figure\", \"points\", \"used\", \"training\", \"space\", \"time\", \"yi\", \"set\", \"different\", \"neural\", \"weight\", \"function\", \"algorithm\", \"two\", \"results\", \"learning\", \"given\", \"data\", \"also\", \"method\", \"error\", \"rate\", \"model\", \"non\", \"first\", \"distribution\", \"based\", \"problem\", \"arm\", \"restless\", \"ortner\", \"pulls\", \"ibs\", \"timepoints\", \"ucb\", \"withdrawn\", \"bandits\", \"pulling\", \"mink\", \"schervish\", \"remix\", \"summable\", \"rested\", \"waiting\", \"pulled\", \"mixing\", \"instants\", \"reads\", \"devoted\", \"mcdonald\", \"kulkarni\", \"memorization\", \"pointer\", \"subscripted\", \"hoeffding\", \"pandey\", \"pivotal\", \"attentional\", \"nonstochastic\", \"bandit\", \"pull\", \"modes\", \"durbin\", \"dx\", \"conductances\", \"digital\", \"filter\", \"map\", \"neurons\", \"mode\", \"architecture\", \"ti\", \"submodular\", \"time\", \"algorithm\", \"variational\", \"regret\", \"models\", \"kl\", \"maximum\", \"however\", \"functions\", \"model\", \"analysis\", \"mean\", \"based\", \"neural\", \"case\", \"distance\", \"function\", \"learning\", \"using\", \"training\", \"set\", \"log\", \"network\", \"data\", \"method\", \"results\", \"input\", \"distribution\", \"probability\", \"also\", \"order\", \"two\", \"given\", \"used\", \"figure\", \"one\", \"number\", \"algorithms\", \"different\", \"problem\"], \"Freq\": [1242.0, 849.0, 1328.0, 1006.0, 777.0, 1173.0, 1255.0, 797.0, 864.0, 642.0, 682.0, 630.0, 786.0, 716.0, 410.0, 476.0, 534.0, 503.0, 558.0, 477.0, 597.0, 469.0, 427.0, 357.0, 361.0, 544.0, 423.0, 422.0, 689.0, 373.0, 10.163893195264329, 10.89010642037114, 2.851584454268702, 7.461407222770377, 3.212053506984655, 7.183638931914555, 2.5150966091349236, 11.400720199982146, 1.5161138656527247, 10.758458820864941, 4.036259529680799, 10.109954179552608, 27.40016930204754, 21.593999756120045, 28.414330945830333, 1.4393857889061892, 4.310232734829415, 27.201449909790696, 2.7888503439165664, 1.4261109228362512, 2.8639984400963585, 1.4219287243737782, 3.7810944197931153, 2.3662954942517613, 2.8080124768211228, 8.88869246662653, 1.8707818941148842, 3.2365785477109066, 6.159251823709979, 3.7999198253928506, 17.113749407222766, 25.121286279017855, 26.36570840096117, 5.1150212971236515, 16.31226918039462, 20.99196595577849, 46.518271299102274, 34.25166075020783, 14.939732416664713, 31.95411388474058, 131.93015699078984, 51.68621854496672, 29.382163528560163, 41.84426880088291, 423.10783420799584, 398.6886371362075, 40.01549250462935, 239.64121783194506, 37.905141291586894, 160.9349518362262, 150.3592250732803, 84.14064408482467, 96.16906766899028, 376.6312956628433, 116.85611612825427, 64.37228333212552, 192.3445582979785, 96.62960840855568, 62.78508682700628, 293.67928165369636, 236.1407178218929, 170.47575114323988, 348.58200514170125, 118.7438728703206, 68.19660208006191, 74.75350783106785, 178.83348606065206, 80.79482926232063, 121.20729564522152, 128.90156649138774, 154.45905187640588, 220.54492686178273, 153.39616410179676, 191.513912623172, 179.44064701848683, 103.37661135449062, 191.17163611831214, 110.00907591007201, 165.1009997525955, 113.46214229459518, 152.47011043392013, 175.679707191023, 126.1976058428881, 128.6424851234212, 127.6307172522044, 115.87216349858913, 3.1495584436520865, 2.679278742910062, 25.731645810085666, 12.779873723616722, 3.0010715194409845, 4.433654057990099, 1.724867155662677, 6.483611933712383, 2.00457823448831, 18.33049887321449, 5.610011278382133, 22.68314449300002, 5.126696419839236, 5.043488666829825, 3.0948237422385336, 3.0202833777777216, 2.5226346547262275, 1.0716772164503336, 1.6070576126603546, 1.1419993558074935, 2.3046522629821204, 3.155102764030665, 1.7564056694280534, 0.7277182166998356, 1.1216402636839924, 12.508104043431588, 0.9602374440949301, 3.828553942316551, 1.7240141239499005, 1.3387423637329436, 99.50823494118315, 17.791080399237206, 2.543527373863298, 54.13978573016432, 29.815647903891826, 8.590507683197737, 107.9533404500627, 9.355382155975203, 19.303713161893267, 14.082135544514522, 40.49154578217359, 60.949732570679274, 79.19496839108076, 175.30690990186764, 34.543829098300705, 140.33126210909694, 53.7070659132579, 50.72644503833407, 128.3896616466639, 63.136900738844936, 41.67562066361489, 148.45127226886325, 109.17705648927733, 82.16673851549905, 209.93326782531472, 48.7722067331943, 59.72441590759356, 46.725136358247134, 69.26536447871844, 131.6704879637702, 182.8332326204704, 71.86735345649198, 79.0245451533128, 93.098415265434, 152.7029078241274, 158.05387983409582, 115.14414113346153, 108.45763160787732, 129.7150769650488, 76.88458921727205, 112.39936333930322, 98.23473908810698, 86.23467676459406, 78.79505107674535, 87.14327152221081, 81.42637796763121, 69.90651103486621, 72.22223933729288, 73.08490565365302, 72.27673461376685, 9.507057259187485, 3.8732403901845514, 14.705648083772235, 2.0846071767319847, 3.2611952484377134, 2.36319035734834, 1.9532708610831924, 4.090938308129567, 1.8895017555669733, 1.9423998532576132, 1.3908777523755118, 1.1394878732227314, 1.68937431625939, 2.752744454972521, 1.3685582280387052, 1.371900834379628, 0.8553772183130034, 2.2857571742946874, 1.0626564703279453, 2.693658111177244, 2.8729017436512967, 3.493655664134177, 1.0490221062980845, 1.308605294692895, 4.565959555283545, 4.861368417715642, 6.6142516014084425, 1.4388873794466277, 3.856951292956115, 2.2180802527495374, 12.838089046083397, 5.894090216194521, 7.479889884336627, 3.2533415221393738, 102.01648397031711, 3.891966648078089, 96.35001837809207, 32.907255553661386, 20.489884523771124, 30.8875903496896, 19.340831622127087, 51.6665209093261, 250.1090306277882, 52.36143299658839, 36.12847666527375, 34.12072298609603, 95.50234969260084, 10.321149028077194, 17.54318400528916, 208.31778721219914, 43.74034850821493, 28.095224184036493, 23.868438766822777, 53.0089880655393, 165.33813584396881, 143.50644243245998, 107.91025071198963, 123.36135127375125, 76.73858442756948, 111.74118516246882, 93.42547434059767, 82.59511894700557, 92.15570579755014, 55.28361910943789, 109.01902559360998, 100.16418613840791, 63.151498179999855, 74.711061796716, 94.49858430587555, 62.44579954528516, 83.6225262724157, 70.39791260039232, 56.15229317681012, 107.41803307163971, 60.574136778330654, 65.21440012923291, 67.13945578364154, 60.75147918168278, 55.97118227350852, 56.560816682422775, 4.867738672988363, 55.48389949618088, 6.950909114255516, 1.816936577072627, 1.8228599851682374, 0.777793629424921, 1.0245816195974646, 1.5410552256848453, 0.7558282182635365, 21.123041780916015, 2.8689182073131723, 1.2081560266382645, 18.420781025204676, 5.609622171858754, 1.4620530269690653, 3.8375103093776874, 0.7286371660013088, 4.137738454033425, 1.7209468850824097, 0.718396700768361, 1.872357543720951, 0.946593324825375, 0.700055801496463, 0.4550790052184421, 0.6925342290796939, 7.461515954925705, 2.260717801811674, 0.7027662421629713, 0.686969351437635, 0.6867106537047957, 5.009397055478575, 11.277378140645336, 13.53337176606854, 7.600372648934645, 4.840448230526348, 8.784687506714024, 10.705517199499942, 4.890746928961942, 70.75732584686281, 24.640403423671675, 9.395842889945264, 10.732801269660635, 25.042405008809265, 5.650176205505932, 17.432793939024567, 19.296272919911797, 7.402664920082403, 36.47713155386673, 32.40715119585791, 41.64335448504902, 14.13866109191607, 11.015807915976719, 8.99413089050109, 30.535158820404387, 28.02835495511163, 109.33165256917312, 107.93468551987426, 144.4164334520321, 13.43178737052591, 138.64270929841004, 40.40237287571285, 74.21075065028667, 55.59717352448212, 89.43668211511473, 54.36002379234096, 72.31622864949222, 85.08569176520409, 83.01526592658108, 76.48604739248141, 62.57052730006095, 48.49877537202669, 49.19221203402903, 109.96140586782957, 90.34628423076545, 54.4689929770824, 56.68996939067383, 64.25656459440059, 87.81386371415795, 48.78956755449139, 44.6831659542595, 54.38814121790789, 61.6060724146298, 59.20225909937141, 48.24199711795227, 48.93642502682694, 45.735688876095686, 46.40986985956967, 4.945081833487154, 6.353930283497971, 3.1733934714625796, 5.9968078781228344, 3.6469611925259215, 3.7364097341322013, 1.0436351380665323, 21.42887116180671, 1.0341289503624536, 1.3874660666433234, 2.491630930603427, 2.3568778649890274, 0.9834985793225028, 0.5847407451759561, 0.9847168246198372, 0.7708516088963828, 0.5903260403152878, 1.9680804788660977, 0.9590707334659491, 3.024924329633928, 1.6791101616665731, 1.709017397784179, 4.340292767212909, 1.4701992590638893, 0.7367815196024446, 0.7545817552614104, 1.7614182270312693, 0.5304453492647246, 1.058746671228535, 0.7382503185050469, 1.6811245402908748, 3.0031180252447665, 2.785421314241718, 5.753444648761618, 2.5140693476682134, 8.158898556283384, 6.506350091860708, 12.208540699146157, 7.464005167306039, 20.45057239894203, 10.342279959212147, 5.465829009478421, 69.265115431144, 5.219623157505333, 18.528764135776118, 146.82688746915198, 51.06672506819796, 50.43032976512538, 50.33359134979086, 64.01013496896653, 78.48955002819072, 87.95506121375347, 38.99750905805955, 121.42845362939792, 113.69999350360563, 86.28486786595235, 40.12043544231399, 42.09977394893833, 71.4522198616514, 42.453566493884374, 95.99986893461765, 25.06357120762767, 72.50591264656045, 33.47895946951649, 21.352644976163628, 48.79145471345709, 65.34876539421533, 44.59553147631492, 73.94831507860428, 90.94301577098845, 45.63829759136246, 33.07553379306049, 41.49113672903709, 34.23335315881967, 39.05904574713386, 61.53424806108641, 41.92887890264973, 51.140539185438946, 44.9650024658746, 43.34870437444994, 43.57177728441271, 41.93613922854004, 6.31886904606871, 1.9691810055796, 1.9375010177552663, 2.8219040420990202, 0.7395820347745126, 0.8725071881601101, 1.2227803242226902, 0.4826927887327046, 0.9757720357920252, 0.6494327880251195, 0.4828210259417548, 1.4560291271311159, 1.2898821709586688, 0.47404206636697016, 0.4661561796449438, 0.47820130207447686, 1.0621742301527315, 0.7775021444995057, 1.0784671561601633, 0.7774684120686739, 0.7557406247493997, 0.459903914069185, 5.773828092696624, 0.6061471168941848, 0.30148154962749896, 0.4361967077087003, 1.3475327842663571, 0.5945471546078529, 0.5901895722070551, 0.2865595096090719, 1.459415323467736, 2.213495341126203, 13.003471475898834, 1.2759522422099223, 3.752687917483057, 3.7611351658260137, 9.535358764527654, 3.9946434655783425, 2.8072295171650645, 7.563158571458884, 2.3182541306547972, 41.68835709050766, 5.3259594242012245, 9.731197276177706, 4.251229281658332, 5.366581690537458, 15.755335765438039, 5.185168785467767, 70.78350592000169, 47.38597169416768, 28.385942659781946, 14.543507093167037, 37.019797298887575, 60.478830117851, 8.105805763595367, 12.327891952115474, 14.161007018153644, 38.30028339568885, 19.576341732211255, 17.258643727904126, 82.62576399321914, 10.923065848448799, 25.40327816400948, 20.416545017506365, 32.84989631427395, 13.008278860025316, 52.924085116276125, 31.66862177621584, 47.074547847876815, 63.109162665870464, 70.8057509739083, 40.27267702400898, 73.14269060442457, 26.33021988853583, 45.85221667771714, 39.90451447345256, 34.16428539307732, 26.86190776471234, 63.31410731468843, 40.160190971141105, 40.59017335551224, 41.650633762291164, 27.088113020175523, 30.234268299558906, 31.631509841561687, 31.89868882830443, 38.85592615522598, 39.969460359097326, 32.23034738356812, 30.500376484017323, 1.9008618076076407, 2.2687353346679506, 13.12449802216789, 5.233661107216418, 2.0414684518945307, 1.4187195369671421, 0.9109529041900819, 4.287480064232834, 0.7283894599982604, 1.1807469138936872, 1.254130924079037, 0.9901960870067399, 0.4305536981941468, 0.5814270369662314, 0.5656489557355348, 0.4176158921298041, 0.41735290192805974, 0.5656347914340492, 1.9010732264432377, 0.9509697176348295, 1.7223446480506095, 0.40679916644732333, 0.40558270011201814, 2.7250269945978225, 0.5443583945414361, 6.731365827575842, 0.49705804060424297, 0.6694321235944845, 2.0070507202833423, 0.6427866484082075, 1.0770170760557671, 2.786173808799023, 9.635859845492066, 29.143097837833174, 3.3414332493948713, 1.494232184997227, 25.30109285044137, 9.832211666805561, 3.5996482753524903, 8.372487047412166, 23.87137677721282, 2.8525988691457704, 4.459635075862178, 8.34093772944545, 18.724414123578217, 19.175270113984958, 29.592677710232245, 13.585526452664368, 12.469564296955456, 8.74041742180327, 60.1689427676103, 32.38011185667621, 16.637565721346903, 13.665772829263426, 44.77606665537166, 43.880024958148866, 60.43265354254433, 12.92802756294665, 64.92111496824064, 44.07169514510288, 32.527256324254175, 66.77620421769181, 41.12253623641883, 43.99351276389883, 57.638505707321045, 19.607617069784634, 25.35691959102955, 27.179546162724236, 52.63956057458551, 23.210534732094796, 25.194361321437615, 34.796729533473346, 30.224889994613257, 27.62268612613109, 36.33542365095872, 34.108462358681564, 26.45595835339838, 27.3766889034227, 27.897089522750356, 25.8021052978456, 25.486066079603052, 27.019531003154736, 1.2315264064024167, 1.976190452674011, 9.265606445556417, 2.1287177947189146, 12.269695542406474, 0.6771264617083635, 0.5497348200076986, 0.3747853624041029, 0.6239318327237479, 2.0634928749178783, 0.6855120166067962, 0.38416027697608174, 1.1488082198443574, 0.4840204382669997, 0.2562429959157853, 3.2094024485591532, 0.3794567949482982, 1.5280027748791856, 0.497576349436051, 0.49444338486670586, 0.8349451688555526, 3.6380926305306547, 0.9573651054262767, 0.3746775954357711, 0.7115948696346169, 0.35670057163992774, 3.6563123213281385, 3.454653445611638, 0.21983568699505956, 1.9340055402122198, 3.060953322133563, 6.928756614299717, 6.340606012415341, 2.6242681158740675, 3.950075197781976, 6.975811749590305, 12.32991851646498, 10.441458678267088, 4.87477969951039, 17.94480028958883, 4.670088228374306, 5.259188348388954, 3.011444430142901, 17.696598699268694, 4.626246019027457, 14.82818865617074, 11.577749510802052, 7.898431689827563, 84.92827961495446, 29.69648935140164, 29.159089547401646, 19.466307547856445, 63.861809532060136, 31.377730864822514, 17.512407766115235, 53.70828972220751, 19.04283062486514, 27.899328705490692, 25.10053960253313, 18.772370459477095, 44.24341878956087, 12.926281949258088, 20.716985074476632, 25.544342669790286, 23.678914675821492, 27.748771270344427, 44.584126228267856, 33.93726366197913, 43.66567244260922, 55.40504114490755, 55.89285079686587, 42.36075921217284, 35.913407176769795, 38.31927758852062, 33.536808193804006, 32.36830201812465, 27.023198419641115, 33.104436449156246, 26.412915399845648, 41.44362339775247, 24.54397281590814, 24.147515787503174, 24.079686278989573, 26.583016804885506, 25.91797861062813, 25.384541625554007, 26.664072036038974, 12.477651865005491, 14.990576225793976, 0.8255641710561576, 0.9445631983356686, 2.4298307062417495, 0.4411521173850078, 6.809502684926514, 0.5409727144673644, 1.685085781779538, 0.7159767024054331, 0.4002440881394492, 0.8314784757739656, 0.3005573840240312, 0.5096191399167996, 0.9785528240827773, 1.547563778064329, 8.916364310787472, 0.29415004708251374, 0.28955072941697446, 0.28644319716559574, 0.2780835381476317, 0.2781092532499183, 0.6785401275645105, 0.2775098156467253, 1.4820727581305353, 0.17998651408358024, 0.27368943487605146, 0.27182644042516285, 0.36706766733379775, 0.26888768815038955, 8.044335819637311, 14.475683576749828, 0.7414839456376695, 0.8843026414608445, 1.2046411287740229, 1.3579709992629312, 1.1776246284881073, 5.027290117769909, 2.2410881576275523, 6.4655392773607305, 7.810277746395812, 5.4266383412627395, 35.95852252930925, 7.956531293398158, 33.90061210463783, 17.42383306146248, 32.142432860824414, 10.13374808865038, 28.50780425934262, 16.27998209687777, 25.77713364056791, 25.920050638981227, 20.134165678988897, 30.857204427801356, 9.865478238187313, 34.071145209748366, 19.617618260299135, 19.138977490770873, 10.128992651616581, 28.170110610107653, 35.312697807598454, 23.63486417574752, 19.814733916310832, 33.46134008759232, 20.537513529331907, 31.36780619973398, 18.95653328532767, 14.944838680888449, 16.102149278880063, 15.170465351664685, 27.218468612433124, 15.482777463440957, 15.2298930093184, 16.135592191424074, 15.943651516872105, 14.96566014856748, 8.254249001795142, 1.349566009632936, 0.31244614366023765, 1.8065146938796963, 0.232409425263553, 0.6053621267416335, 4.106989166553999, 0.3573805193328943, 1.0565173233059204, 0.5910308471390656, 0.36416104855662274, 0.1436730264918796, 1.8920985868564415, 0.4323920143395256, 0.21350706816870166, 1.6085480647478174, 0.6446780700771022, 6.125468207944899, 0.2828225691876349, 0.28811884601554955, 0.27216485200420093, 0.13286760810805984, 0.19971571493771526, 0.43921794277768134, 3.430866046256487, 0.13054834842447793, 0.19576008473928874, 0.13289970065618947, 0.19342435945392253, 2.0158011586640425, 0.4079945771449619, 3.3702267851930245, 1.3207952079658158, 6.490878210136354, 0.3869051604188849, 2.4918655958820892, 2.386759705239441, 1.3629109186186, 3.9015391590302078, 13.34027334568053, 11.003162042425593, 3.7863566555577584, 5.808662649447572, 7.7793937938993025, 5.462208049307481, 38.146507878375374, 51.681027242198525, 3.713825931294656, 7.614539629731508, 22.328405345162995, 9.147859407041732, 7.402962293750188, 14.58859161515206, 13.080546005432254, 45.40870040303216, 15.83716930870603, 10.677943539847847, 20.95928490985385, 19.086881754608083, 17.031704493058417, 9.334690369255531, 28.66248227247686, 37.595469657690266, 26.267065853976085, 22.82592706133781, 31.395345013496396, 16.82678630490363, 16.322249403713272, 32.44350060661403, 14.661282243052185, 19.140863433335163, 15.740067769213956, 18.567580939429394, 15.669801414635959, 18.000032612311383, 13.357243258935414, 20.672449542122035, 18.423063834193414, 18.759155784088666, 18.96863089043641, 20.00252173728297, 19.58243363688795, 15.314764350196059, 15.646226027502632, 17.014523462274376], \"Total\": [1242.0, 849.0, 1328.0, 1006.0, 777.0, 1173.0, 1255.0, 797.0, 864.0, 642.0, 682.0, 630.0, 786.0, 716.0, 410.0, 476.0, 534.0, 503.0, 558.0, 477.0, 597.0, 469.0, 427.0, 357.0, 361.0, 544.0, 423.0, 422.0, 689.0, 373.0, 17.700993919827695, 19.873726508706945, 5.4257056394314, 14.843332416898692, 6.470691003455943, 14.633110154929003, 5.126966350972436, 23.28724077819827, 3.1150279298271264, 22.518184837309374, 8.465633713396377, 21.366526250733752, 58.10488873196739, 45.92778963257093, 61.02666129959833, 3.097589315096046, 9.285708987318474, 58.81365612423562, 6.033353159200271, 3.0999683856420432, 6.2356617981361815, 3.0962639649695154, 8.235604174024191, 5.158889906694264, 6.128661351474395, 19.419920611250717, 4.095207895368624, 7.086766063444455, 13.506468015855798, 8.38170763651312, 38.20727999521388, 56.586717182531466, 60.0533581699117, 11.325892301358325, 37.29324882773634, 48.448430363930925, 110.66725087391835, 81.01947005379728, 34.48339448775758, 76.37182997820075, 339.68316995641175, 126.7870714842605, 70.34602745468574, 102.52214446354598, 1242.3003067491866, 1173.1867519154714, 99.45236317114116, 689.4519892298903, 94.15450317698075, 469.7943057867548, 435.92538886574465, 230.6559158238962, 268.6466105911363, 1255.0988974188053, 337.79707009844276, 171.59419133668973, 597.1523203144243, 273.53975962814366, 167.9514603822629, 1006.1170701803079, 786.426509579415, 558.4339293469486, 1328.8853575559142, 369.6002329956444, 188.8076895034533, 211.6317428207934, 630.1057763976984, 234.73846115620523, 391.79985707201905, 431.7366774191536, 544.9238330991247, 864.2837369227829, 552.9600816191452, 777.5443858726068, 716.1179864553011, 334.30837954752786, 797.1769949577113, 370.62229328986587, 682.0468583652446, 391.34576548956136, 642.1054050738308, 849.3738624451645, 477.9079928712408, 503.88297548276387, 534.2112750124846, 476.6592803052793, 6.53365319382931, 5.614718976632855, 54.212122614204425, 27.139281453881747, 6.514368788360554, 9.640469107376832, 3.761687810705036, 14.545732562057648, 4.545362143098407, 41.75360468352358, 13.154139564402987, 53.355318499027746, 12.129113504450176, 11.958435007442592, 7.555368452306748, 7.425392933169008, 6.288176700340053, 2.7245817286019114, 4.210709763318314, 3.023549903683105, 6.126202849245112, 8.482411912334346, 4.780601941303702, 1.9834545421472716, 3.060765299989365, 34.74695063199043, 2.668309573675887, 10.656799998044075, 4.800134049933743, 3.766802165926735, 299.52583757250784, 52.30164601338477, 7.21053372237306, 177.72527067109255, 99.81003141273109, 26.569957513730337, 406.0125626591069, 29.26762914888833, 65.42996030265611, 46.52624893440656, 150.8811268380668, 242.565658543322, 327.2162327385256, 864.2837369227829, 131.80364074616307, 682.0468583652446, 221.04355660901868, 210.20669407310123, 642.1054050738308, 277.91892040693483, 169.52577732113983, 797.1769949577113, 552.9600816191452, 391.34576548956136, 1328.8853575559142, 210.59029664627153, 273.53975962814366, 200.71357283587514, 337.79707009844276, 786.426509579415, 1255.0988974188053, 361.3023210155122, 422.35418632908755, 558.4339293469486, 1173.1867519154714, 1242.3003067491866, 777.5443858726068, 716.1179864553011, 1006.1170701803079, 426.94571829379004, 849.3738624451645, 689.4519892298903, 544.9238330991247, 476.6592803052793, 597.1523203144243, 534.2112750124846, 379.1992338807151, 427.603329830786, 503.88297548276387, 630.1057763976984, 27.078497288268487, 11.135407047070885, 44.49105610665206, 6.3960452271587025, 10.04311125445513, 7.291352790367277, 6.1059153730090205, 13.10355703163809, 6.052453944157008, 6.222120040958566, 4.472549213784632, 3.6655944600354746, 5.446656478040318, 8.92327109430127, 4.44756192249128, 4.510599024472965, 2.814908430246731, 7.666806058465135, 3.569810023590181, 9.110661041465518, 9.752695970988963, 11.908498943518188, 3.5773156278662266, 4.470579629633938, 15.623750342060022, 16.671519026079416, 22.829007840653695, 4.989559653985855, 13.429263768983356, 7.73112539066918, 45.123725536766614, 20.597030957423485, 26.555312022337873, 11.382352247082686, 410.8673416185963, 13.782984671318086, 423.4488421359238, 135.84090082905908, 82.1474747491237, 128.91455701500078, 77.60059738649373, 227.18459100394188, 1328.8853575559142, 238.4114785311947, 159.0907407084985, 150.50496574455047, 476.6592803052793, 40.99953308274189, 74.96340834682687, 1255.0988974188053, 213.83577700671577, 128.900257890105, 108.79272588360844, 293.3188010245041, 1173.1867519154714, 1006.1170701803079, 716.1179864553011, 849.3738624451645, 477.9079928712408, 777.5443858726068, 630.1057763976984, 552.9600816191452, 642.1054050738308, 341.3709534072782, 864.2837369227829, 797.1769949577113, 426.94571829379004, 544.9238330991247, 786.426509579415, 431.7366774191536, 682.0468583652446, 534.2112750124846, 370.62229328986587, 1242.3003067491866, 427.603329830786, 558.4339293469486, 689.4519892298903, 597.1523203144243, 401.3102182683596, 503.88297548276387, 15.224867281733545, 178.23855808637504, 22.652109789155528, 6.097007227158646, 6.2549227497281805, 2.7343848351549753, 3.638291237662872, 5.531076790469391, 2.726705891533098, 76.78020768018692, 10.568854427480465, 4.466882068413265, 68.53531083319729, 21.044561096989025, 5.5079892906850745, 14.553537964534398, 2.766408193070081, 15.844830773865914, 6.599478650794452, 2.7700214056306174, 7.241506601200932, 3.691851230972292, 2.7496345010281003, 1.7896692534432497, 2.729964975112108, 29.495787377422936, 8.97345502583592, 2.7909516093308513, 2.7302504523429936, 2.7407909924705884, 20.085815208230706, 45.774012232028696, 55.52938586903504, 30.959097611721884, 19.622232073662865, 36.10318268143396, 44.33502490172512, 19.88824712029992, 325.8300474726167, 110.62972381418044, 40.18380943082916, 46.8599424340168, 116.74019824622212, 24.19995754576354, 83.5780297608829, 93.70437219866143, 32.67290733983568, 198.01574504596326, 173.4740916796699, 247.5494304707843, 69.6523339432389, 51.9490835922614, 40.999891830545096, 178.1193504566111, 161.9764360368928, 849.3738624451645, 864.2837369227829, 1242.3003067491866, 67.61764381175735, 1255.0988974188053, 271.65147529180877, 597.1523203144243, 416.32771463247144, 797.1769949577113, 427.603329830786, 630.1057763976984, 786.426509579415, 777.5443858726068, 716.1179864553011, 544.9238330991247, 381.1546897840192, 394.9887582067225, 1328.8853575559142, 1006.1170701803079, 469.7943057867548, 503.88297548276387, 642.1054050738308, 1173.1867519154714, 422.35418632908755, 357.72073601561755, 534.2112750124846, 689.4519892298903, 682.0468583652446, 426.94571829379004, 552.9600816191452, 401.3102182683596, 558.4339293469486, 17.80113940741055, 23.446137590355395, 11.863937135989252, 22.995683576695225, 14.04599311487412, 15.1406893965017, 4.42239804008101, 91.12626429231584, 4.478390492638614, 6.129833924449356, 11.042109964348692, 10.468057911208687, 4.411078341043116, 2.6394687115465625, 4.46527977499926, 3.527302044919806, 2.711602930606756, 9.08467499049485, 4.444169663262778, 14.038794384441836, 7.840971276051954, 8.107571849481511, 20.648839665333895, 7.001818999218817, 3.523865241184153, 3.6246824482871727, 8.501892023705894, 2.6101736446949926, 5.220094599973339, 3.645524230324969, 8.310971380202519, 14.968313320904292, 13.930142180634002, 29.32307589265703, 12.553861967450425, 43.734642299841674, 34.57498315107077, 68.01703880789393, 40.38416022456275, 120.45679132139422, 58.69639589026717, 29.745660226533346, 503.88297548276387, 28.4733858011582, 121.2693408577661, 1328.8853575559142, 394.9887582067225, 392.81376797762323, 401.3102182683596, 534.2112750124846, 689.4519892298903, 797.1769949577113, 305.7483910595739, 1242.3003067491866, 1173.1867519154714, 849.3738624451645, 332.0430577173549, 353.9858310248971, 682.0468583652446, 357.72073601561755, 1006.1170701803079, 190.04124644781322, 777.5443858726068, 282.1957835945642, 155.8545980176801, 477.9079928712408, 716.1179864553011, 427.603329830786, 864.2837369227829, 1255.0988974188053, 469.7943057867548, 290.75824678380724, 410.8673416185963, 307.6417194671055, 379.1992338807151, 786.426509579415, 426.94571829379004, 630.1057763976984, 558.4339293469486, 544.9238330991247, 597.1523203144243, 552.9600816191452, 26.11669050016067, 8.261755818485513, 8.283041034091953, 12.378400018540479, 3.335800924081933, 4.180414036482345, 5.913801935277897, 2.4978738578456348, 5.078334847252492, 3.4100497992262104, 2.5662319137572696, 7.757261548440802, 6.882697666860048, 2.5538519362556755, 2.525886054494195, 2.5977723767133387, 5.805959557840488, 4.255520131682392, 5.970824706428906, 4.320020356924105, 4.2095991032602935, 2.5865120229861, 32.59689728594852, 3.427653046492518, 1.7054307421066468, 2.4803951492984124, 7.7519131712575255, 3.451343536737584, 3.4333923263726867, 1.7025569224273707, 8.716152597856619, 13.409013146001906, 88.16339656211328, 7.762602918619795, 24.78424540417011, 25.16180108451984, 69.26015440151316, 27.644760624150454, 18.75824317940029, 57.17088441650495, 15.155834123080556, 410.8673416185963, 39.52414199932043, 79.46172910038317, 30.615767001039465, 40.19264986598924, 141.82386336533355, 38.80493052350912, 849.3738624451645, 534.2112750124846, 293.3188010245041, 134.48044341525863, 422.35418632908755, 777.5443858726068, 67.19449558229694, 113.86888820380474, 135.84090082905908, 476.6592803052793, 206.20961598182276, 175.8616127564356, 1328.8853575559142, 100.02975116734065, 297.0396915147652, 224.35526020308146, 416.32771463247144, 125.7481898554076, 786.426509579415, 401.3102182683596, 682.0468583652446, 1006.1170701803079, 1173.1867519154714, 558.4339293469486, 1255.0988974188053, 320.0411015843467, 716.1179864553011, 597.1523203144243, 477.9079928712408, 339.68316995641175, 1242.3003067491866, 630.1057763976984, 642.1054050738308, 689.4519892298903, 353.9858310248971, 431.7366774191536, 469.7943057867548, 503.88297548276387, 797.1769949577113, 864.2837369227829, 552.9600816191452, 544.9238330991247, 8.349985656076942, 10.234675051956534, 59.791781167295696, 26.156279953080194, 11.064244223048037, 7.784220269623101, 5.159589258177416, 24.339055240463985, 4.14778956510457, 6.919334293965651, 7.358454608996617, 5.815640026183821, 2.568320506660465, 3.5055875351765717, 3.436474845792619, 2.547550411376414, 2.5568816569010497, 3.495878200659719, 11.766431481527631, 5.93157441906103, 10.765990714285145, 2.5510083134227477, 2.5503310577036604, 17.276513641290062, 3.4530917096213836, 42.964590624691326, 3.1830456651010657, 4.322346213375618, 12.994016973273329, 4.188036014773375, 7.024999309467032, 18.809913451735387, 68.92988478281274, 226.0227766059614, 23.583385801135996, 10.081564263945078, 206.20961598182276, 75.7033212326611, 25.73956979117471, 65.52135481464376, 224.35526020308146, 20.840461126615182, 34.72142092766727, 71.01758183302684, 179.27661216774962, 184.7114201586301, 313.07228443575707, 130.67870085751633, 118.88622788508286, 77.55393836169354, 849.3738624451645, 410.8673416185963, 180.85936577918903, 142.08497542618628, 642.1054050738308, 630.1057763976984, 1006.1170701803079, 135.84090082905908, 1173.1867519154714, 716.1179864553011, 477.9079928712408, 1328.8853575559142, 682.0468583652446, 797.1769949577113, 1255.0988974188053, 253.47844969181463, 376.3187897263132, 423.4488421359238, 1242.3003067491866, 336.6998841682948, 394.9887582067225, 689.4519892298903, 544.9238330991247, 469.7943057867548, 777.5443858726068, 786.426509579415, 476.6592803052793, 552.9600816191452, 597.1523203144243, 503.88297548276387, 534.2112750124846, 864.2837369227829, 7.553883114778583, 12.2414689589868, 57.53500495763952, 13.565006798982568, 80.72129214339758, 4.521909419338464, 3.7007269431093857, 2.536267641245988, 4.24162137494588, 14.126198767341371, 4.711322099858916, 2.640367981199347, 7.944111611901186, 3.3650790349641415, 1.8060426343974627, 23.00319886918523, 2.720249465122186, 11.001727987672403, 3.621494976582958, 3.660987498613401, 6.206515617390347, 27.221133621099064, 7.254744400088564, 2.8464945586924224, 5.446833110441563, 2.744791511020335, 28.135127246933404, 26.609305185119666, 1.702268612563374, 14.980535644016163, 23.854098853577558, 55.52938586903504, 51.29671414031632, 20.910014874327363, 32.67290733983568, 60.28736745627437, 110.66725087391835, 93.70437219866143, 41.41220821246064, 171.59419133668973, 40.02376628976434, 45.774012232028696, 25.266327687589833, 181.27462363025393, 40.999891830545096, 154.51525135142006, 119.10113478403093, 76.79536902183524, 1242.3003067491866, 361.3023210155122, 353.9858310248971, 229.94504608145894, 1006.1170701803079, 427.603329830786, 207.93845801735543, 864.2837369227829, 234.5082639885235, 381.1546897840192, 336.6998841682948, 234.73846115620523, 716.1179864553011, 144.954741886743, 268.6466105911363, 357.72073601561755, 325.8300474726167, 406.0125626591069, 777.5443858726068, 558.4339293469486, 797.1769949577113, 1173.1867519154714, 1255.0988974188053, 849.3738624451645, 689.4519892298903, 786.426509579415, 630.1057763976984, 597.1523203144243, 435.92538886574465, 642.1054050738308, 431.7366774191536, 1328.8853575559142, 394.9887582067225, 392.81376797762323, 391.79985707201905, 534.2112750124846, 544.9238330991247, 503.88297548276387, 682.0468583652446, 76.57201856015345, 102.12324867068017, 5.890748947692371, 6.805078911775345, 18.51208394697741, 3.389577416085524, 53.79235723871963, 4.370698709968204, 13.857740542409383, 5.984838190003482, 3.4249238371825133, 7.123391637895292, 2.5760718692451015, 4.377427906349791, 8.487752279904267, 13.42338177005356, 77.45314570313471, 2.5993265595786927, 2.6065210748861176, 2.6241579910086745, 2.5733873096757454, 2.586306054203965, 6.337031080839045, 2.602726928673184, 13.954866269496485, 1.7118468356684333, 2.6403242539161185, 2.6456004537365727, 3.5767275114038926, 2.6267460811469627, 79.64322952198334, 145.55418394352733, 7.265382434622459, 8.740885421464423, 12.195730503975245, 13.879779030971514, 12.409386400629169, 68.21129474631242, 27.405189431568203, 94.93770954658989, 120.70336074267395, 79.62617789713649, 777.5443858726068, 130.36774457656875, 797.1769949577113, 357.72073601561755, 786.426509579415, 184.05707162766447, 682.0468583652446, 339.68316995641175, 630.1057763976984, 642.1054050738308, 469.7943057867548, 849.3738624451645, 188.8076895034533, 1006.1170701803079, 477.9079928712408, 476.6592803052793, 199.429901893571, 864.2837369227829, 1242.3003067491866, 716.1179864553011, 558.4339293469486, 1255.0988974188053, 597.1523203144243, 1173.1867519154714, 544.9238330991247, 373.9546320374131, 431.7366774191536, 391.34576548956136, 1328.8853575559142, 416.32771463247144, 401.3102182683596, 552.9600816191452, 534.2112750124846, 689.4519892298903, 65.57416081227198, 13.301951554984921, 3.1967316345108427, 18.49442633877028, 2.4350801745641997, 6.365856753608723, 43.639929736538534, 3.847733304397223, 11.65912482686984, 6.527260196610891, 4.054735464944804, 1.6085565298036337, 21.23861063639599, 4.871563720301436, 2.415353145736108, 18.468035508215266, 7.431429159928596, 71.01758183302684, 3.285183827066126, 3.351722950969557, 3.3324541553478224, 1.6334931076781447, 2.4640874996521, 5.434272341771342, 42.964590624691326, 1.6364487258524694, 2.4544599012534456, 1.6677221945023666, 2.460175580234522, 25.73956979117471, 5.237296563924993, 45.3790640980714, 17.50901137602833, 93.70437219866143, 4.982026952356507, 35.49370643294797, 33.945403742613884, 18.884156883224748, 58.90386209388455, 224.35526020308146, 184.7114201586301, 57.34968486464663, 94.88075335204284, 131.80364074616307, 91.12626429231584, 849.3738624451645, 1242.3003067491866, 58.98009009866653, 136.98008724238989, 503.88297548276387, 178.23855808637504, 139.25866219565987, 320.0411015843467, 282.1957835945642, 1328.8853575559142, 361.3023210155122, 221.04355660901868, 534.2112750124846, 476.6592803052793, 422.35418632908755, 190.1434646744647, 864.2837369227829, 1255.0988974188053, 777.5443858726068, 642.1054050738308, 1006.1170701803079, 427.603329830786, 410.8673416185963, 1173.1867519154714, 373.9546320374131, 558.4339293469486, 423.4488421359238, 552.9600816191452, 426.94571829379004, 544.9238330991247, 327.9406825602808, 716.1179864553011, 597.1523203144243, 630.1057763976984, 682.0468583652446, 786.426509579415, 797.1769949577113, 435.92538886574465, 477.9079928712408, 689.4519892298903], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.6257, -8.5567, -9.8967, -8.9348, -9.7777, -8.9728, -10.0223, -8.5109, -10.5284, -8.5689, -9.5493, -8.631, -7.634, -7.8722, -7.5977, -10.5804, -9.4836, -7.6413, -9.9189, -10.5896, -9.8924, -10.5926, -9.6146, -10.0832, -9.9121, -8.7598, -10.3182, -9.7701, -9.1266, -9.6096, -8.1047, -7.7209, -7.6725, -9.3124, -8.1527, -7.9004, -7.1047, -7.4108, -8.2405, -7.4803, -6.0623, -6.9994, -7.5642, -7.2106, -4.8969, -4.9564, -7.2553, -5.4654, -7.3095, -5.8636, -5.9315, -6.5121, -6.3785, -5.0133, -6.1836, -6.7799, -5.6853, -6.3737, -6.8049, -5.2621, -5.4801, -5.806, -5.0907, -6.1676, -6.7222, -6.6304, -5.7581, -6.5527, -6.1471, -6.0855, -5.9046, -5.5485, -5.9115, -5.6896, -5.7547, -6.3062, -5.6914, -6.244, -5.838, -6.2131, -5.9176, -5.7759, -6.1067, -6.0875, -6.0954, -6.1921, -9.3257, -9.4874, -7.2252, -7.925, -9.3739, -8.9837, -9.9278, -8.6036, -9.7775, -7.5643, -8.7484, -7.3513, -8.8385, -8.8548, -9.3432, -9.3676, -9.5476, -10.4037, -9.9985, -10.3401, -9.638, -9.3239, -9.9096, -10.7908, -10.3581, -7.9465, -10.5135, -9.1304, -9.9283, -10.1812, -5.8727, -7.5942, -9.5394, -6.4813, -7.0779, -8.3223, -5.7912, -8.237, -7.5126, -7.828, -6.7718, -6.3629, -6.101, -5.3064, -6.9307, -5.5289, -6.4894, -6.5465, -5.6178, -6.3276, -6.743, -5.4727, -5.7799, -6.0642, -5.1261, -6.5858, -6.3832, -6.6286, -6.235, -5.5926, -5.2643, -6.1981, -6.1032, -5.9393, -5.4444, -5.41, -5.7267, -5.7866, -5.6076, -6.1306, -5.7509, -5.8856, -6.0158, -6.1061, -6.0054, -6.0732, -6.2258, -6.1932, -6.1813, -6.1924, -8.0542, -8.9521, -7.618, -9.5716, -9.1241, -9.4462, -9.6367, -8.8974, -9.6699, -9.6423, -9.9763, -10.1756, -9.7819, -9.2936, -9.9925, -9.99, -10.4624, -9.4795, -10.2454, -9.3153, -9.2509, -9.0553, -10.2584, -10.0373, -8.7876, -8.7249, -8.417, -9.9423, -8.9563, -9.5096, -7.7538, -8.5323, -8.294, -9.1265, -5.6811, -8.9473, -5.7382, -6.8125, -7.2863, -6.8759, -7.344, -6.3614, -4.7843, -6.348, -6.7191, -6.7763, -5.7471, -7.972, -7.4415, -4.9671, -6.5279, -6.9706, -7.1337, -6.3358, -5.1982, -5.3398, -5.6249, -5.4911, -5.9658, -5.59, -5.769, -5.8923, -5.7827, -6.2937, -5.6147, -5.6994, -6.1607, -5.9926, -5.7576, -6.1719, -5.8799, -6.052, -6.2781, -5.6295, -6.2023, -6.1285, -6.0994, -6.1994, -6.2814, -6.2709, -8.5167, -6.0833, -8.1605, -9.5022, -9.499, -10.3507, -10.0751, -9.6669, -10.3793, -7.049, -9.0454, -9.9103, -7.1859, -8.3749, -9.7195, -8.7546, -10.416, -8.6792, -9.5565, -10.4301, -9.4722, -10.1543, -10.456, -10.8867, -10.4668, -8.0896, -9.2837, -10.4521, -10.4748, -10.4752, -8.4881, -7.6766, -7.4942, -8.0712, -8.5224, -7.9264, -7.7286, -8.512, -5.8401, -6.895, -7.8591, -7.7261, -6.8788, -8.3677, -7.241, -7.1395, -8.0975, -6.5027, -6.621, -6.3702, -7.4505, -7.7, -7.9028, -6.6805, -6.7662, -5.405, -5.4179, -5.1267, -7.5018, -5.1675, -6.4005, -5.7925, -6.0812, -5.6058, -6.1037, -5.8183, -5.6557, -5.6804, -5.7623, -5.9631, -6.2178, -6.2036, -5.3992, -5.5957, -6.1017, -6.0618, -5.9365, -5.6242, -6.2119, -6.2998, -6.1032, -5.9786, -6.0184, -6.2231, -6.2089, -6.2765, -6.2619, -8.3404, -8.0898, -8.784, -8.1476, -8.6449, -8.6207, -9.8961, -6.8741, -9.9053, -9.6114, -9.0259, -9.0815, -9.9555, -10.4754, -9.9542, -10.1991, -10.4659, -9.2618, -9.9806, -8.8319, -9.4206, -9.4029, -8.4709, -9.5534, -10.2443, -10.2204, -9.3727, -10.5729, -9.8817, -10.2423, -9.4194, -8.8392, -8.9144, -8.189, -9.0169, -7.8397, -8.0661, -7.4367, -7.9287, -6.9208, -7.6026, -8.2403, -5.7009, -8.2864, -7.0195, -4.9496, -6.0057, -6.0182, -6.0202, -5.7798, -5.5759, -5.462, -6.2753, -5.1395, -5.2053, -5.4812, -6.2469, -6.1988, -5.6698, -6.1904, -5.3745, -6.7174, -5.6552, -6.4279, -6.8777, -6.0513, -5.7591, -6.1412, -5.6355, -5.4286, -6.1181, -6.44, -6.2134, -6.4056, -6.2738, -5.8192, -6.2029, -6.0043, -6.1329, -6.1696, -6.1644, -6.2027, -7.7613, -8.9272, -8.9434, -8.5674, -9.9065, -9.7412, -9.4037, -10.3332, -9.6293, -10.0364, -10.3329, -9.2291, -9.3502, -10.3512, -10.368, -10.3425, -9.5445, -9.8565, -9.5292, -9.8565, -9.8848, -10.3815, -7.8515, -10.1054, -10.8038, -10.4345, -9.3065, -10.1247, -10.1321, -10.8546, -9.2268, -8.8102, -7.0396, -9.3611, -8.2823, -8.2801, -7.3498, -8.2198, -8.5726, -7.5815, -8.764, -5.8746, -7.9322, -7.3295, -8.1576, -7.9246, -6.8476, -7.959, -5.3452, -5.7465, -6.2589, -6.9276, -5.9933, -5.5025, -7.5122, -7.0929, -6.9543, -5.9593, -6.6305, -6.7565, -5.1905, -7.2139, -6.3699, -6.5884, -6.1128, -7.0392, -5.6359, -6.1495, -5.7531, -5.4599, -5.3449, -5.9091, -5.3124, -6.3341, -5.7794, -5.9183, -6.0736, -6.3141, -5.4567, -5.9119, -5.9013, -5.8755, -6.3057, -6.1958, -6.1506, -6.1422, -5.9449, -5.9167, -6.1319, -6.1871, -8.8354, -8.6585, -6.9032, -7.8226, -8.764, -9.128, -9.571, -8.022, -9.7946, -9.3116, -9.2513, -9.4876, -10.3204, -10.02, -10.0475, -10.3509, -10.3515, -10.0475, -8.8353, -9.528, -8.934, -10.3771, -10.3801, -8.4752, -10.0859, -7.5709, -10.1768, -9.879, -8.781, -9.9197, -9.4035, -8.453, -7.2122, -6.1055, -8.2713, -9.0761, -6.2469, -7.192, -8.1969, -7.3528, -6.305, -8.4295, -7.9826, -7.3565, -6.5479, -6.5241, -6.0902, -6.8687, -6.9544, -7.3098, -5.3806, -6.0002, -6.6661, -6.8628, -5.676, -5.6963, -5.3762, -6.9183, -5.3045, -5.6919, -5.9956, -5.2764, -5.7612, -5.6937, -5.4235, -6.5018, -6.2447, -6.1752, -5.5142, -6.3331, -6.2511, -5.9282, -6.069, -6.1591, -5.8849, -5.9482, -6.2022, -6.168, -6.1492, -6.2273, -6.2396, -6.1812, -9.245, -8.7721, -7.2269, -8.6977, -6.9461, -9.8431, -10.0515, -10.4346, -9.9249, -8.7288, -9.8308, -10.4099, -9.3145, -10.1789, -10.8149, -8.2871, -10.4222, -9.0293, -10.1512, -10.1575, -9.6336, -8.1618, -9.4968, -10.4349, -9.7935, -10.4841, -8.1568, -8.2135, -10.9681, -8.7936, -8.3345, -7.5175, -7.6062, -8.4884, -8.0795, -7.5108, -6.9412, -7.1074, -7.8691, -6.5659, -7.912, -7.7932, -8.3508, -6.5799, -7.9215, -6.7567, -7.0041, -7.3866, -5.0114, -6.0622, -6.0805, -6.4845, -5.2965, -6.0071, -6.5903, -5.4697, -6.5065, -6.1246, -6.2303, -6.5208, -5.6635, -6.894, -6.4223, -6.2128, -6.2886, -6.13, -5.6558, -5.9287, -5.6767, -5.4386, -5.4298, -5.707, -5.8721, -5.8073, -5.9406, -5.976, -6.1565, -5.9536, -6.1794, -5.7289, -6.2528, -6.269, -6.2719, -6.173, -6.1983, -6.2191, -6.1699, -6.4294, -6.2459, -9.145, -9.0104, -8.0655, -9.7717, -7.035, -9.5677, -8.4315, -9.2874, -9.869, -9.1379, -10.1554, -9.6274, -8.975, -8.5166, -6.7654, -10.177, -10.1928, -10.2035, -10.2332, -10.2331, -9.3411, -10.2352, -8.5599, -10.6682, -10.2491, -10.2559, -9.9555, -10.2668, -6.8684, -6.2809, -9.2524, -9.0763, -8.7671, -8.6473, -8.7898, -7.3384, -8.1464, -7.0868, -6.8979, -7.262, -5.371, -6.8793, -5.4299, -6.0955, -5.4832, -6.6375, -5.6032, -6.1634, -5.7038, -5.6983, -5.9509, -5.524, -6.6643, -5.4249, -5.9769, -6.0016, -6.6379, -5.6151, -5.3891, -5.7906, -5.9669, -5.4429, -5.9311, -5.5075, -6.0112, -6.249, -6.1744, -6.234, -5.6494, -6.2136, -6.2301, -6.1723, -6.1843, -6.2476, -6.8362, -8.6471, -10.1103, -8.3555, -10.4062, -9.4489, -7.5342, -9.9759, -8.892, -9.4728, -9.9571, -10.8871, -8.3092, -9.7854, -10.491, -8.4716, -9.3859, -7.1345, -10.2099, -10.1913, -10.2483, -10.9653, -10.5578, -9.7697, -7.7141, -10.9829, -10.5778, -10.9651, -10.5898, -8.2459, -9.8434, -7.7319, -8.6687, -7.0765, -9.8965, -8.0339, -8.077, -8.6373, -7.5856, -6.3561, -6.5487, -7.6155, -7.1876, -6.8955, -7.2491, -5.3055, -5.0018, -7.6349, -6.9169, -5.8411, -6.7334, -6.945, -6.2667, -6.3758, -5.1312, -6.1846, -6.5787, -5.9043, -5.9979, -6.1119, -6.7132, -5.5913, -5.32, -5.6786, -5.819, -5.5003, -6.124, -6.1544, -5.4674, -6.2617, -5.9951, -6.1907, -6.0255, -6.1952, -6.0566, -6.3549, -5.9181, -6.0333, -6.0152, -6.0041, -5.9511, -5.9723, -6.2181, -6.1967, -6.1129], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8157, 0.7689, 0.7272, 0.6827, 0.6701, 0.659, 0.6583, 0.6562, 0.6504, 0.6318, 0.6298, 0.6222, 0.6188, 0.6158, 0.606, 0.6041, 0.603, 0.5994, 0.5988, 0.594, 0.5924, 0.5923, 0.592, 0.5911, 0.59, 0.5889, 0.587, 0.5868, 0.5853, 0.5794, 0.5673, 0.5584, 0.5473, 0.5756, 0.5436, 0.5341, 0.5038, 0.5095, 0.534, 0.4992, 0.4247, 0.4731, 0.4974, 0.4743, 0.2934, 0.2912, 0.4601, 0.3137, 0.4606, 0.2992, 0.306, 0.362, 0.3432, 0.1668, 0.309, 0.39, 0.2376, 0.3299, 0.3865, 0.1391, 0.1674, 0.1839, 0.0322, 0.235, 0.3521, 0.3298, 0.111, 0.3039, 0.1972, 0.1617, 0.1097, 0.0047, 0.0882, -0.0307, -0.0135, 0.1968, -0.0574, 0.1558, -0.0481, 0.1323, -0.0673, -0.2054, 0.0389, 0.0052, -0.0612, -0.0438, 1.1124, 1.1023, 1.0969, 1.089, 1.0671, 1.0654, 1.0624, 1.0341, 1.0234, 1.0189, 0.9899, 0.9868, 0.981, 0.9788, 0.9496, 0.9426, 0.9288, 0.909, 0.8789, 0.8685, 0.8645, 0.8531, 0.8408, 0.8394, 0.8382, 0.8204, 0.8201, 0.8184, 0.8181, 0.8076, 0.7402, 0.7638, 0.8001, 0.6534, 0.6339, 0.713, 0.5174, 0.7016, 0.6214, 0.647, 0.5267, 0.4609, 0.4234, 0.2468, 0.503, 0.261, 0.4273, 0.4205, 0.2324, 0.3601, 0.439, 0.1613, 0.2198, 0.2813, -0.0032, 0.3794, 0.3204, 0.3845, 0.2576, 0.0549, -0.0843, 0.2272, 0.166, 0.0506, -0.1969, -0.2197, -0.0678, -0.0454, -0.2064, 0.1278, -0.1803, -0.1064, -0.0015, 0.0422, -0.0825, -0.039, 0.1512, 0.0637, -0.0886, -0.3233, 0.9621, 0.9528, 0.9018, 0.8877, 0.884, 0.8821, 0.8691, 0.8447, 0.8447, 0.8446, 0.8408, 0.8404, 0.8382, 0.8328, 0.8302, 0.8186, 0.8177, 0.7986, 0.7971, 0.7903, 0.7866, 0.7825, 0.7821, 0.7803, 0.7787, 0.7764, 0.77, 0.7653, 0.7613, 0.7602, 0.7518, 0.7576, 0.7418, 0.7564, 0.6157, 0.7443, 0.5284, 0.591, 0.6202, 0.58, 0.6195, 0.5279, 0.3386, 0.493, 0.5264, 0.5247, 0.4012, 0.6295, 0.5565, 0.2129, 0.4219, 0.4854, 0.4919, 0.298, 0.0493, 0.0613, 0.1163, 0.0794, 0.1798, 0.0689, 0.1001, 0.1075, 0.0675, 0.1883, -0.0616, -0.0654, 0.0977, 0.0218, -0.1101, 0.0753, -0.09, -0.0178, 0.1217, -0.4392, 0.0545, -0.1386, -0.3203, -0.2766, 0.0389, -0.1782, 1.0754, 1.0486, 1.0343, 1.005, 0.9827, 0.9585, 0.9484, 0.9377, 0.9326, 0.9251, 0.9117, 0.9081, 0.9018, 0.8935, 0.8893, 0.8826, 0.8815, 0.873, 0.8715, 0.8661, 0.863, 0.8546, 0.8476, 0.8463, 0.844, 0.8412, 0.8371, 0.8365, 0.8358, 0.8316, 0.827, 0.8147, 0.8039, 0.8112, 0.816, 0.8023, 0.7946, 0.8129, 0.6885, 0.7139, 0.7625, 0.7418, 0.6763, 0.761, 0.6482, 0.6354, 0.731, 0.524, 0.538, 0.4332, 0.6211, 0.6647, 0.6987, 0.4521, 0.4614, 0.1655, 0.1353, 0.0636, 0.5994, 0.0126, 0.31, 0.1304, 0.2023, 0.0281, 0.1531, 0.0508, -0.0082, -0.0215, -0.0211, 0.0513, 0.154, 0.1325, -0.2763, -0.1945, 0.061, 0.0309, -0.0862, -0.3766, 0.0573, 0.1355, -0.069, -0.1995, -0.2285, 0.0352, -0.2091, 0.0438, -0.272, 1.0953, 1.0706, 1.0575, 1.0321, 1.0278, 0.9769, 0.9322, 0.9287, 0.9105, 0.8905, 0.8874, 0.8852, 0.8754, 0.869, 0.8645, 0.8554, 0.8516, 0.8467, 0.8428, 0.8413, 0.8351, 0.8193, 0.8165, 0.8154, 0.8112, 0.8068, 0.802, 0.7827, 0.7808, 0.7792, 0.7781, 0.7699, 0.7665, 0.7476, 0.7681, 0.6972, 0.7059, 0.6586, 0.6879, 0.6029, 0.6401, 0.682, 0.3918, 0.6797, 0.4975, 0.1734, 0.3305, 0.3235, 0.3001, 0.2545, 0.2033, 0.172, 0.3169, 0.0508, 0.0423, 0.0894, 0.2628, 0.247, 0.1201, 0.2449, 0.0267, 0.3504, 0.0037, 0.2445, 0.3885, 0.0943, -0.0179, 0.1156, -0.0823, -0.2485, 0.0447, 0.2025, 0.0834, 0.1805, 0.1032, -0.1717, 0.0555, -0.1351, -0.143, -0.1552, -0.2416, -0.2029, 1.2912, 1.2762, 1.2574, 1.2317, 1.2039, 1.1434, 1.1341, 1.0664, 1.0607, 1.0519, 1.0397, 1.0373, 1.0358, 1.0262, 1.0204, 1.0179, 1.0117, 1.0104, 0.9989, 0.9953, 0.9928, 0.9832, 0.9794, 0.9777, 0.9774, 0.9722, 0.9606, 0.9515, 0.9494, 0.9283, 0.9231, 0.9089, 0.7963, 0.9046, 0.8225, 0.8096, 0.7274, 0.7758, 0.8108, 0.6875, 0.8327, 0.4222, 0.7059, 0.6103, 0.7359, 0.6968, 0.5128, 0.6975, 0.2254, 0.2878, 0.3749, 0.486, 0.2759, 0.1564, 0.5952, 0.4871, 0.4493, 0.1889, 0.3557, 0.3889, -0.0675, 0.4957, 0.2513, 0.3134, 0.1707, 0.4415, 0.0116, 0.1708, 0.0369, -0.0587, -0.0973, 0.0808, -0.1323, 0.2125, -0.0382, 0.0046, 0.072, 0.1729, -0.2664, -0.0428, -0.051, -0.0963, 0.1401, 0.0514, 0.0121, -0.0495, -0.311, -0.3635, -0.1321, -0.1727, 1.3574, 1.3308, 1.3209, 1.2283, 1.1473, 1.135, 1.1032, 1.1009, 1.0978, 1.0691, 1.0679, 1.0669, 1.0514, 1.0407, 1.0331, 1.029, 1.0247, 1.0159, 1.0145, 1.0068, 1.0046, 1.0014, 0.9987, 0.9905, 0.9899, 0.9837, 0.9804, 0.9722, 0.9695, 0.9631, 0.962, 0.9276, 0.8697, 0.7889, 0.8832, 0.9282, 0.7393, 0.7962, 0.8701, 0.7799, 0.5968, 0.8487, 0.785, 0.6956, 0.5782, 0.5721, 0.4784, 0.5736, 0.5824, 0.6543, 0.19, 0.2966, 0.4513, 0.4958, 0.1742, 0.1729, 0.025, 0.4852, -0.057, 0.0493, 0.15, -0.1534, 0.0288, -0.0597, -0.2435, 0.278, 0.1399, 0.0914, -0.3239, 0.1627, 0.0851, -0.1491, -0.0547, 0.0037, -0.226, -0.3006, -0.054, -0.1683, -0.2263, -0.1346, -0.2053, -0.628, 1.048, 1.0382, 1.0357, 1.0098, 0.9779, 0.963, 0.955, 0.9497, 0.9452, 0.9382, 0.9343, 0.9342, 0.9281, 0.9227, 0.909, 0.8923, 0.8921, 0.8877, 0.8769, 0.8598, 0.8558, 0.8493, 0.8366, 0.834, 0.8265, 0.8212, 0.8212, 0.8203, 0.815, 0.8147, 0.8086, 0.7806, 0.7712, 0.7864, 0.749, 0.7051, 0.6673, 0.6675, 0.7223, 0.604, 0.7135, 0.6981, 0.7348, 0.5352, 0.68, 0.518, 0.5309, 0.5873, 0.1789, 0.3631, 0.3653, 0.3927, 0.1047, 0.2497, 0.3875, 0.0835, 0.351, 0.2472, 0.2655, 0.3357, 0.0777, 0.4447, 0.2994, 0.2225, 0.24, 0.1786, 0.003, 0.0612, -0.0427, -0.191, -0.2497, -0.1365, -0.093, -0.1597, -0.0714, -0.0532, 0.081, -0.1033, 0.0678, -0.606, 0.0834, 0.0727, 0.0724, -0.1387, -0.1839, -0.1264, -0.38, 1.5474, 1.4429, 1.3966, 1.387, 1.3311, 1.3226, 1.2949, 1.2724, 1.2547, 1.2384, 1.2149, 1.2138, 1.2133, 1.2112, 1.2014, 1.2014, 1.1999, 1.1828, 1.1643, 1.1467, 1.1366, 1.1317, 1.1275, 1.1232, 1.1193, 1.1093, 1.095, 1.0862, 1.085, 1.0825, 1.0691, 1.0536, 1.0795, 1.0707, 1.0468, 1.0373, 1.0068, 0.754, 0.8579, 0.675, 0.6238, 0.6757, 0.2879, 0.5653, 0.2041, 0.3398, 0.1644, 0.4623, 0.1868, 0.3236, 0.1653, 0.152, 0.2118, 0.0466, 0.41, -0.0237, 0.1687, 0.1466, 0.3816, -0.0619, -0.1988, -0.0494, 0.023, -0.2629, -0.0082, -0.26, 0.0032, 0.1419, 0.0728, 0.1115, -0.5265, 0.07, 0.0902, -0.1726, -0.15, -0.4684, 1.2957, 1.08, 1.0427, 1.042, 1.0189, 1.0152, 1.0048, 0.9917, 0.967, 0.9662, 0.9581, 0.9526, 0.95, 0.9463, 0.9422, 0.9274, 0.9234, 0.9176, 0.9157, 0.9142, 0.863, 0.859, 0.8554, 0.8526, 0.8405, 0.8396, 0.8393, 0.8385, 0.825, 0.8211, 0.8158, 0.768, 0.7836, 0.6984, 0.8127, 0.7118, 0.7133, 0.7394, 0.6536, 0.5457, 0.5475, 0.6503, 0.5748, 0.5383, 0.5537, 0.265, 0.1885, 0.603, 0.4783, 0.2516, 0.3985, 0.4337, 0.2799, 0.2966, -0.0083, 0.2407, 0.3379, 0.1299, 0.1503, 0.1573, 0.3541, -0.0382, -0.14, -0.0197, 0.0312, -0.0991, 0.1329, 0.1424, -0.2199, 0.1292, -0.0052, 0.0759, -0.0258, 0.0632, -0.0422, 0.1673, -0.1769, -0.1105, -0.1461, -0.2142, -0.3035, -0.3383, 0.0195, -0.0511, -0.3337]}, \"token.table\": {\"Topic\": [4, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 6, 7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 1, 2, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 9, 1, 2, 3, 4, 9, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 5, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 5, 6, 8, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 9, 10, 1, 2, 3, 4, 5, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 1, 2, 6, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 8, 1, 2, 3, 4, 6, 7, 8, 9, 1, 4, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 6, 7, 9, 1, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 1, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 2, 3, 4, 5, 7, 8, 1, 8, 1, 2, 1, 3, 4, 5, 8, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 4, 1, 2, 3, 4, 5, 6, 7, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 8, 1, 3, 4, 1, 2, 3, 5, 6, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 3, 4, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 6, 1, 5, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 5, 8, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 1, 2, 3, 5, 7, 8, 1, 3, 4, 8, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 3, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 5, 1, 3, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 2, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 7, 1, 2, 1, 2, 3, 4, 6, 2, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 1, 2, 4, 1, 2, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 5, 1, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 3, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 10, 1, 3, 5, 6, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 5, 6, 7, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"Freq\": [0.3662667646998609, 0.28012695168419455, 0.29338726038866086, 0.11952814312130627, 0.09779575346288695, 0.11952814312130627, 0.08149646121907246, 0.05976407156065314, 0.07606336380446763, 0.05433097414604831, 0.05433097414604831, 0.03803168190223381, 0.19485734909488675, 0.15714302346361836, 0.2262859537876104, 0.12571441877089468, 0.07542865126253681, 0.05028576750835787, 0.06914293032399207, 0.0565714884469026, 0.018857162815634203, 0.025142883754178936, 0.15263031214891257, 0.07631515607445628, 0.30526062429782513, 0.07631515607445628, 0.07631515607445628, 0.07631515607445628, 0.07631515607445628, 0.07631515607445628, 0.07631515607445628, 0.2571531405501169, 0.11020848880719296, 0.07347232587146196, 0.14694465174292393, 0.11020848880719296, 0.03673616293573098, 0.03673616293573098, 0.14694465174292393, 0.07347232587146196, 0.03673616293573098, 0.4972359351159968, 0.16574531170533227, 0.16574531170533227, 0.4680217964610222, 0.04680217964610222, 0.09360435929220444, 0.04680217964610222, 0.14040653893830665, 0.09360435929220444, 0.04680217964610222, 0.04680217964610222, 0.04680217964610222, 0.22501391165742077, 0.22501391165742077, 0.22501391165742077, 0.22501391165742077, 0.11472951956416058, 0.22945903912832116, 0.11472951956416058, 0.11472951956416058, 0.11472951956416058, 0.11472951956416058, 0.11472951956416058, 0.17223681805526517, 0.17223681805526517, 0.17223681805526517, 0.17223681805526517, 0.17223681805526517, 0.3404973803048423, 0.1271834186481444, 0.08613054300855348, 0.11591400180590375, 0.09739995985079412, 0.05071237579008289, 0.04266279233133958, 0.0684214593993182, 0.028173542105601607, 0.04185783398546524, 0.34409558110458366, 0.13075632081974178, 0.11011058595346676, 0.10322867433137509, 0.08717088054649452, 0.05276132243603616, 0.04129146973255004, 0.061937204598825055, 0.03211558756976114, 0.034409558110458364, 0.2971698404731989, 0.2826083034837394, 0.15782022142598434, 0.13763391403428865, 0.11561248778880248, 0.07891011071299217, 0.05688868446750598, 0.055053565613715466, 0.0477130901985534, 0.0348672582220198, 0.03303213936822928, 0.18544027010865347, 0.1992790962361649, 0.1439237917261191, 0.09410401766707788, 0.10240731334358474, 0.05535530451004581, 0.06089083496105039, 0.08303295676506871, 0.030445417480525195, 0.04428424360803665, 0.24668298192484134, 0.12334149096242067, 0.12334149096242067, 0.12334149096242067, 0.24668298192484134, 0.12334149096242067, 0.5041708689312376, 0.3231389564037085, 0.22521806052379684, 0.039168358351964666, 0.08812880629192049, 0.0293762687639735, 0.06854462711593816, 0.04896044793995583, 0.039168358351964666, 0.1468813438198675, 0.009792089587991167, 0.30037081994817283, 0.19589401300967793, 0.05223840346924745, 0.09141720607118303, 0.03917880260193558, 0.05223840346924745, 0.05223840346924745, 0.03917880260193558, 0.15671521040774233, 0.013059600867311862, 0.1371487608336708, 0.1371487608336708, 0.2742975216673416, 0.1371487608336708, 0.1371487608336708, 0.1371487608336708, 0.1757106050300458, 0.0878553025150229, 0.2635659075450687, 0.0878553025150229, 0.0878553025150229, 0.0878553025150229, 0.0878553025150229, 0.0878553025150229, 0.30883114890406477, 0.14037779495639308, 0.13527314786706968, 0.07146505925052737, 0.0944359711524826, 0.058703441527218916, 0.058703441527218916, 0.06125576507188061, 0.033180206080601994, 0.03573252962526369, 0.16863272512849953, 0.16863272512849953, 0.16863272512849953, 0.09485590788478097, 0.08431636256424976, 0.08431636256424976, 0.07377681724371854, 0.04215818128212488, 0.03161863596159366, 0.06323727192318732, 0.22610478146978116, 0.09690204920133477, 0.16150341533555795, 0.2584054645368927, 0.06460136613422318, 0.06460136613422318, 0.03230068306711159, 0.03230068306711159, 0.03230068306711159, 0.03230068306711159, 0.1067493645864542, 0.12199927381309052, 0.16774900149299946, 0.13724918303972683, 0.07624954613318158, 0.1067493645864542, 0.09149945535981789, 0.045749727679908946, 0.015249909226636315, 0.12199927381309052, 0.1599296891136642, 0.1599296891136642, 0.10661979274244279, 0.10661979274244279, 0.053309896371221396, 0.1599296891136642, 0.053309896371221396, 0.053309896371221396, 0.053309896371221396, 0.053309896371221396, 0.4856959994041902, 0.12142399985104756, 0.12142399985104756, 0.12142399985104756, 0.12142399985104756, 0.1366697650722396, 0.30750697141253913, 0.1708372063402995, 0.0683348825361198, 0.1025023238041797, 0.0341674412680599, 0.1025023238041797, 0.0341674412680599, 0.0341674412680599, 0.0341674412680599, 0.19425344093024985, 0.07770137637209994, 0.11655206455814991, 0.07770137637209994, 0.07770137637209994, 0.11655206455814991, 0.15540275274419987, 0.03885068818604997, 0.03885068818604997, 0.07770137637209994, 0.3866518420642945, 0.28647571866852356, 0.1101829687186629, 0.1101829687186629, 0.1101829687186629, 0.08814637497493032, 0.06610978123119773, 0.06610978123119773, 0.06610978123119773, 0.02203659374373258, 0.06610978123119773, 0.17153946198352402, 0.08576973099176201, 0.08576973099176201, 0.17153946198352402, 0.08576973099176201, 0.08576973099176201, 0.08576973099176201, 0.08576973099176201, 0.08576973099176201, 0.23960557552254697, 0.15162540326036175, 0.1310342991138929, 0.10108360217357451, 0.11980278776127348, 0.08798017226218521, 0.04679796396924746, 0.050541801086787255, 0.02995069694031837, 0.039310289734167864, 0.2758862367302122, 0.2758862367302122, 0.2758862367302122, 0.2717961999187647, 0.13589809995938235, 0.13589809995938235, 0.13589809995938235, 0.13589809995938235, 0.13589809995938235, 0.18923526799647528, 0.09461763399823764, 0.09461763399823764, 0.2838529019947129, 0.09461763399823764, 0.09461763399823764, 0.5536967848677742, 0.24584361160572846, 0.14048206377470196, 0.07024103188735098, 0.17560257971837745, 0.17560257971837745, 0.07024103188735098, 0.03512051594367549, 0.03512051594367549, 0.03512051594367549, 0.03512051594367549, 0.14529186786962406, 0.14529186786962406, 0.14529186786962406, 0.14529186786962406, 0.14529186786962406, 0.14529186786962406, 0.18740503285013418, 0.11925774817735811, 0.18740503285013418, 0.11925774817735811, 0.17036821168194016, 0.05111046350458205, 0.03407364233638804, 0.05111046350458205, 0.01703682116819402, 0.03407364233638804, 0.2731503454679236, 0.2731503454679236, 0.2228795925584628, 0.1114397962792314, 0.1114397962792314, 0.2228795925584628, 0.1114397962792314, 0.1114397962792314, 0.0922302938695472, 0.47959752812164547, 0.07378423509563777, 0.03689211754781888, 0.0922302938695472, 0.07378423509563777, 0.01844605877390944, 0.01844605877390944, 0.05533817632172833, 0.03689211754781888, 0.3307370580461929, 0.3307370580461929, 0.2938981348973409, 0.14694906744867045, 0.14694906744867045, 0.14694906744867045, 0.21424478474449726, 0.09181919346192739, 0.09181919346192739, 0.21424478474449726, 0.09181919346192739, 0.030606397820642464, 0.06121279564128493, 0.12242559128256986, 0.06121279564128493, 0.030606397820642464, 0.16377560757236564, 0.3275512151447313, 0.16377560757236564, 0.3643263963710909, 0.3643263963710909, 0.38859288543161546, 0.3842124154414383, 0.206611932708754, 0.1239671596252524, 0.1239671596252524, 0.2479343192505048, 0.0826447730835016, 0.0413223865417508, 0.0413223865417508, 0.0826447730835016, 0.0413223865417508, 0.0413223865417508, 0.22397080136198258, 0.12442822297887922, 0.14931386757465506, 0.22397080136198258, 0.07465693378732753, 0.024885644595775844, 0.04977128919155169, 0.07465693378732753, 0.024885644595775844, 0.024885644595775844, 0.15931352480898608, 0.07965676240449304, 0.15931352480898608, 0.15931352480898608, 0.2389702872134791, 0.07965676240449304, 0.07965676240449304, 0.07965676240449304, 0.35089966875389034, 0.11696655625129679, 0.058483278125648394, 0.11696655625129679, 0.07797770416753119, 0.058483278125648394, 0.019494426041882798, 0.11696655625129679, 0.038988852083765596, 0.038988852083765596, 0.2795392143232476, 0.2248422882980019, 0.2248422882980019, 0.45881585857269064, 0.0819314033165519, 0.11470396464317266, 0.06554512265324151, 0.0819314033165519, 0.04915884198993114, 0.0819314033165519, 0.03277256132662076, 0.03277256132662076, 0.01638628066331038, 0.4883756944944972, 0.23913578524661144, 0.18704680232160698, 0.12785477627046554, 0.11601637106023724, 0.07339811230341539, 0.08760419855568935, 0.0449859397988675, 0.052088982925004475, 0.030779853546593555, 0.040250577714776185, 0.2886473438984259, 0.2164855079238194, 0.07216183597460647, 0.07216183597460647, 0.07216183597460647, 0.07216183597460647, 0.07216183597460647, 0.07216183597460647, 0.14432367194921294, 0.1800848297437469, 0.12605938082062282, 0.07203393189749877, 0.25211876164124564, 0.10805089784624813, 0.03601696594874938, 0.03601696594874938, 0.12605938082062282, 0.05402544892312407, 0.01800848297437469, 0.19661811497709564, 0.13107874331806377, 0.06553937165903188, 0.24031102941645022, 0.10923228609838646, 0.04369291443935459, 0.04369291443935459, 0.10923228609838646, 0.04369291443935459, 0.021846457219677293, 0.2044789876108294, 0.15512199060131884, 0.18332598889246773, 0.09871399401902109, 0.07756099530065942, 0.11281599316459552, 0.07756099530065942, 0.04230599743672332, 0.02115299871836166, 0.02820399829114888, 0.16195416745420876, 0.14723106132200797, 0.24293125118131315, 0.08097708372710438, 0.06625397759490359, 0.10306174292540557, 0.09570018985930517, 0.03680776533050199, 0.029446212264401594, 0.029446212264401594, 0.239210755507237, 0.239210755507237, 0.239210755507237, 0.239210755507237, 0.11486907194391817, 0.11486907194391817, 0.19144845323986362, 0.038289690647972725, 0.11486907194391817, 0.22973814388783634, 0.07657938129594545, 0.07657938129594545, 0.038289690647972725, 0.1690959573797442, 0.1690959573797442, 0.1690959573797442, 0.1690959573797442, 0.1690959573797442, 0.16157176993831046, 0.08078588496915523, 0.16157176993831046, 0.08078588496915523, 0.08078588496915523, 0.24235765490746566, 0.08078588496915523, 0.08078588496915523, 0.1129717473278586, 0.188286245546431, 0.2636007437650034, 0.1129717473278586, 0.0753144982185724, 0.0753144982185724, 0.0376572491092862, 0.0376572491092862, 0.0376572491092862, 0.0376572491092862, 0.1492810257598159, 0.1492810257598159, 0.1492810257598159, 0.09952068383987725, 0.049760341919938625, 0.12440085479984657, 0.09952068383987725, 0.07464051287990794, 0.024880170959969312, 0.049760341919938625, 0.4811037059284854, 0.1603679019761618, 0.1603679019761618, 0.2658792299877146, 0.2414305651612581, 0.11307507482236138, 0.0855703268925978, 0.08251424378929074, 0.04889732965291303, 0.04584124654960597, 0.0519534127562201, 0.03361691413637771, 0.030560831033070644, 0.3546102406899241, 0.21934654063294273, 0.10236171896203994, 0.06580396218988283, 0.06945973786709853, 0.040213532449372834, 0.03655775677215712, 0.05483663515823568, 0.03290198109494141, 0.025590429740509985, 0.1288227686619583, 0.1288227686619583, 0.1288227686619583, 0.1288227686619583, 0.1288227686619583, 0.1288227686619583, 0.11511801545881051, 0.37413355024113415, 0.17267702318821576, 0.057559007729405254, 0.08633851159410788, 0.08633851159410788, 0.028779503864702627, 0.028779503864702627, 0.028779503864702627, 0.057559007729405254, 0.15292694554329983, 0.07646347277164992, 0.15292694554329983, 0.07646347277164992, 0.11469520915747487, 0.03823173638582496, 0.1911586819291248, 0.07646347277164992, 0.03823173638582496, 0.03823173638582496, 0.168589303505409, 0.168589303505409, 0.168589303505409, 0.168589303505409, 0.168589303505409, 0.168589303505409, 0.20754330018053346, 0.09962078408665606, 0.1660346401444268, 0.17433637215164813, 0.1660346401444268, 0.0415086600361067, 0.04981039204332803, 0.04981039204332803, 0.024905196021664015, 0.024905196021664015, 0.2201509687570076, 0.1100754843785038, 0.1100754843785038, 0.1100754843785038, 0.2201509687570076, 0.12900041291842523, 0.12900041291842523, 0.12900041291842523, 0.12900041291842523, 0.12900041291842523, 0.12900041291842523, 0.12900041291842523, 0.12900041291842523, 0.1275352204202328, 0.1275352204202328, 0.1275352204202328, 0.1275352204202328, 0.2550704408404656, 0.1275352204202328, 0.2139692571299279, 0.290386848962045, 0.12226814693138736, 0.0764175918321171, 0.0764175918321171, 0.04585055509927026, 0.04585055509927026, 0.06113407346569368, 0.03056703673284684, 0.03056703673284684, 0.2758981803996703, 0.12190849831613339, 0.11549226156265267, 0.10907602480917197, 0.1347409718230948, 0.05774613078132634, 0.05774613078132634, 0.05774613078132634, 0.03208118376740352, 0.03208118376740352, 0.18442263142513338, 0.19320466149299687, 0.1405124810858159, 0.12294842095008891, 0.09660233074649843, 0.10538436081436194, 0.04391015033931747, 0.04391015033931747, 0.026346090203590484, 0.035128120271453976, 0.14729534631291402, 0.11783627705033123, 0.23567255410066246, 0.08837720778774842, 0.08837720778774842, 0.08837720778774842, 0.08837720778774842, 0.058918138525165614, 0.029459069262582807, 0.058918138525165614, 0.4895033071583045, 0.16316776905276817, 0.6420488178772102, 0.16931446778770098, 0.13024189829823152, 0.13024189829823152, 0.2735079864262862, 0.07814513897893892, 0.05209675931929261, 0.03907256948946946, 0.03907256948946946, 0.03907256948946946, 0.03907256948946946, 0.1765839931570065, 0.08829199657850326, 0.1765839931570065, 0.3090219880247614, 0.08829199657850326, 0.04414599828925163, 0.04414599828925163, 0.04414599828925163, 0.04414599828925163, 0.04414599828925163, 0.35438918094394223, 0.11812972698131408, 0.12285491606056664, 0.08505340342654615, 0.09922897066430383, 0.051977079871778194, 0.051977079871778194, 0.06615264710953589, 0.023625945396262817, 0.023625945396262817, 0.31416451575420573, 0.34506488455719686, 0.1831825930365366, 0.06390090454762905, 0.10224144727620647, 0.07242102515397959, 0.04686066333492797, 0.04686066333492797, 0.08094114576033012, 0.02982042212222689, 0.02982042212222689, 0.18859744276319282, 0.11605996477734942, 0.18859744276319282, 0.0725374779858434, 0.10155246918018075, 0.0725374779858434, 0.1450749559716868, 0.05802998238867471, 0.029014991194337356, 0.04352248679150603, 0.14991092933536437, 0.14991092933536437, 0.14991092933536437, 0.09994061955690957, 0.07495546466768219, 0.07495546466768219, 0.07495546466768219, 0.12492577444613696, 0.024985154889227392, 0.049970309778454784, 0.1451064517369689, 0.1451064517369689, 0.2902129034739378, 0.1451064517369689, 0.07255322586848445, 0.07255322586848445, 0.07255322586848445, 0.2531997831826589, 0.30383973981919066, 0.10127991327306356, 0.07877326587904944, 0.061893280333538844, 0.056266618485035315, 0.03938663293952472, 0.03938663293952472, 0.03375997109102119, 0.04501329478802825, 0.41224787026787985, 0.12793899422106617, 0.0852926628140441, 0.0852926628140441, 0.09950810661638479, 0.02843088760468137, 0.04264633140702205, 0.04264633140702205, 0.014215443802340685, 0.05686177520936274, 0.2173272179504778, 0.24147468661164198, 0.0724424059834926, 0.09658987464465679, 0.048294937322328396, 0.12073734330582099, 0.024147468661164198, 0.12073734330582099, 0.024147468661164198, 0.024147468661164198, 0.23747020438379912, 0.11873510219189956, 0.11873510219189956, 0.0791567347945997, 0.0791567347945997, 0.03957836739729985, 0.0791567347945997, 0.11873510219189956, 0.03957836739729985, 0.03957836739729985, 0.28076513291229466, 0.14038256645614733, 0.14038256645614733, 0.14038256645614733, 0.2752780074548036, 0.1376390037274018, 0.1376390037274018, 0.1376390037274018, 0.1376390037274018, 0.16401489497102406, 0.16401489497102406, 0.16401489497102406, 0.3280297899420481, 0.46344164737655774, 0.05149351637517308, 0.10298703275034617, 0.05149351637517308, 0.15448054912551923, 0.05149351637517308, 0.05149351637517308, 0.05149351637517308, 0.05149351637517308, 0.3400993058850601, 0.13041401954991025, 0.1406425701028444, 0.075009370721517, 0.0971712302528743, 0.06051892410486032, 0.055404648828393244, 0.04688085670094813, 0.026423755595079857, 0.027276134807824367, 0.3583007303518804, 0.3583007303518804, 0.22481743309551833, 0.1441137391637938, 0.10952644176448328, 0.18446558612965608, 0.08070369393172452, 0.08070369393172452, 0.04611639653241402, 0.06917459479862102, 0.02305819826620701, 0.04035184696586226, 0.20042338342893257, 0.06680779447631086, 0.20042338342893257, 0.1336155889526217, 0.20042338342893257, 0.06680779447631086, 0.06680779447631086, 0.21536032878190345, 0.14357355252126897, 0.14357355252126897, 0.14357355252126897, 0.21536032878190345, 0.07178677626063448, 0.07178677626063448, 0.07178677626063448, 0.13485856540732719, 0.13485856540732719, 0.3371464135183179, 0.06742928270366359, 0.08990571027155145, 0.044952855135775724, 0.06742928270366359, 0.022476427567887862, 0.044952855135775724, 0.044952855135775724, 0.1551290350415865, 0.06648387216067993, 0.28809677936294636, 0.11080645360113323, 0.06648387216067993, 0.11080645360113323, 0.08864516288090657, 0.044322581440453285, 0.06648387216067993, 0.044322581440453285, 0.19794357576343174, 0.1319623838422878, 0.1319623838422878, 0.0659811919211439, 0.0659811919211439, 0.1319623838422878, 0.0659811919211439, 0.0659811919211439, 0.30007914689395804, 0.30007914689395804, 0.22395013311347678, 0.22395013311347678, 0.22395013311347678, 0.2636490744651497, 0.13810189614841176, 0.1611188788398137, 0.0774207599619884, 0.102530195625336, 0.07114340104615151, 0.06905094807420588, 0.04603396538280392, 0.04184905943891265, 0.03347924755113012, 0.15886332752641832, 0.15886332752641832, 0.10590888501761221, 0.10590888501761221, 0.052954442508806106, 0.10590888501761221, 0.10590888501761221, 0.052954442508806106, 0.052954442508806106, 0.052954442508806106, 0.3221655796563404, 0.12272974463098682, 0.09971791751267679, 0.09204730847324011, 0.07670609039436677, 0.09971791751267679, 0.05369426327605673, 0.05369426327605673, 0.06136487231549341, 0.023011827118310027, 0.29174481385252504, 0.29174481385252504, 0.29174481385252504, 0.208327515356327, 0.416655030712654, 0.208327515356327, 0.2787369005331774, 0.1419980436678451, 0.08414698884020451, 0.10518373605025563, 0.08940617564271729, 0.06311024163015337, 0.06836942843266616, 0.07362861523517894, 0.042073494420102255, 0.04733268122261503, 0.27669266749237015, 0.1971209199782245, 0.15010125099259297, 0.08861399154984403, 0.07595484989986633, 0.05787036182846957, 0.04882811779277121, 0.041594322564212506, 0.028935180914234786, 0.03436052733565381, 0.16267943364934762, 0.14789039422667966, 0.14789039422667966, 0.19225751249468356, 0.10352327595867576, 0.05915615769067186, 0.044367118268003894, 0.05915615769067186, 0.044367118268003894, 0.044367118268003894, 0.2387754060548862, 0.2387754060548862, 0.2387754060548862, 0.1915674095264686, 0.1915674095264686, 0.1915674095264686, 0.1915674095264686, 0.1915674095264686, 0.15045270487781448, 0.30090540975562896, 0.12895946132384098, 0.08597297421589398, 0.08597297421589398, 0.04298648710794699, 0.08597297421589398, 0.04298648710794699, 0.021493243553973494, 0.04298648710794699, 0.19512417333765866, 0.12195260833603667, 0.24390521667207335, 0.09756208666882933, 0.073171565001622, 0.073171565001622, 0.048781043334414666, 0.048781043334414666, 0.024390521667207333, 0.024390521667207333, 0.1502854952321626, 0.3005709904643252, 0.14026646221668512, 0.08015226412382007, 0.1102093631702526, 0.06011419809286505, 0.04007613206191003, 0.06011419809286505, 0.04007613206191003, 0.030057099046432525, 0.20072151547213096, 0.20072151547213096, 0.1690440532417979, 0.11269603549453192, 0.19721806211543086, 0.19721806211543086, 0.08452202662089894, 0.05634801774726596, 0.05634801774726596, 0.05634801774726596, 0.02817400887363298, 0.05634801774726596, 0.15897113193780466, 0.23845669790670698, 0.07948556596890233, 0.11922834895335349, 0.07948556596890233, 0.15897113193780466, 0.039742782984451164, 0.039742782984451164, 0.039742782984451164, 0.039742782984451164, 0.23748965286363855, 0.4749793057272771, 0.41965221418288534, 0.12342712181849569, 0.07405627309109741, 0.07405627309109741, 0.07405627309109741, 0.07405627309109741, 0.061713560909247844, 0.03702813654554871, 0.03702813654554871, 0.012342712181849569, 0.2021364249181364, 0.14438316065581172, 0.14438316065581172, 0.08662989639348703, 0.1010682124590682, 0.14438316065581172, 0.07219158032790586, 0.07219158032790586, 0.014438316065581172, 0.028876632131162344, 0.41900266117931106, 0.06546916580926736, 0.11784449845668124, 0.09165683213297429, 0.10475066529482777, 0.03928149948556041, 0.05237533264741388, 0.06546916580926736, 0.03928149948556041, 0.01309383316185347, 0.20613544330668718, 0.1374236288711248, 0.0687118144355624, 0.2748472577422496, 0.0687118144355624, 0.0687118144355624, 0.0687118144355624, 0.0687118144355624, 0.16071692500582843, 0.16071692500582843, 0.32143385001165686, 0.16071692500582843, 0.29879323844140243, 0.12507623934756382, 0.14360605258423992, 0.10191397280171866, 0.08801661287421157, 0.06948679963753544, 0.04169207978252127, 0.06022189301919739, 0.03705962647335224, 0.030110946509598696, 0.339515402499622, 0.169757701249811, 0.169757701249811, 0.291977295711966, 0.291977295711966, 0.37510837867446933, 0.1131279237272209, 0.0833574174832154, 0.1012197212296187, 0.07740331623441431, 0.0654951137368121, 0.0535869112392099, 0.0535869112392099, 0.029770506244005502, 0.0476328099904088, 0.1942027473895862, 0.0971013736947931, 0.2913041210843793, 0.0971013736947931, 0.04855068684739655, 0.0971013736947931, 0.04855068684739655, 0.04855068684739655, 0.04855068684739655, 0.2515168621840957, 0.09146067715785297, 0.16005618502624272, 0.13719101573677947, 0.18292135431570594, 0.045730338578926485, 0.045730338578926485, 0.045730338578926485, 0.022865169289463243, 0.022865169289463243, 0.2406457570969598, 0.1203228785484799, 0.2406457570969598, 0.1203228785484799, 0.2406457570969598, 0.21819262787267898, 0.10227779431531828, 0.1806907699570623, 0.12273335317838192, 0.09886853450480766, 0.09545927469429706, 0.05113889715765914, 0.054548156968169746, 0.03409259810510609, 0.04091111772612731, 0.112066526885935, 0.112066526885935, 0.33619958065780503, 0.112066526885935, 0.112066526885935, 0.112066526885935, 0.112066526885935, 0.13235621880157117, 0.39706865640471345, 0.13235621880157117, 0.26547717558561323, 0.26547717558561323, 0.1320946008335186, 0.17172298108357417, 0.17172298108357417, 0.07925676050011116, 0.09246622058346302, 0.10567568066681488, 0.1320946008335186, 0.05283784033340744, 0.03962838025005558, 0.02641892016670372, 0.21550893841316598, 0.11342575705956104, 0.15879605988338547, 0.05671287852978052, 0.10208318135360495, 0.14745348417742934, 0.06805545423573663, 0.09074060564764884, 0.03402772711786831, 0.02268515141191221, 0.2147443647349061, 0.15338883195350436, 0.12271106556280348, 0.06135553278140174, 0.09203329917210261, 0.18406659834420522, 0.06135553278140174, 0.06135553278140174, 0.03067776639070087, 0.03067776639070087, 0.3222419991010898, 0.1611209995505449, 0.1611209995505449, 0.1611209995505449, 0.1611209995505449, 0.1611209995505449, 0.17474003222240403, 0.18768373831295249, 0.14238076699603291, 0.07766223654329069, 0.1359089139507587, 0.07119038349801646, 0.051774824362193786, 0.09707779567911336, 0.032359265226371114, 0.032359265226371114, 0.2421443568276721, 0.09685774273106884, 0.19371548546213768, 0.09685774273106884, 0.19371548546213768, 0.04842887136553442, 0.04842887136553442, 0.04842887136553442, 0.04842887136553442, 0.18762665013013485, 0.13645574554918896, 0.14924847169442543, 0.13645574554918896, 0.10234180916189173, 0.05543514662935802, 0.0767563568714188, 0.08102059891983096, 0.0383781784357094, 0.042642420484121556, 0.1736461454138606, 0.11576409694257371, 0.1736461454138606, 0.057882048471286855, 0.11576409694257371, 0.057882048471286855, 0.1736461454138606, 0.057882048471286855, 0.057882048471286855, 0.12072860630342479, 0.12072860630342479, 0.12072860630342479, 0.12072860630342479, 0.24145721260684957, 0.12072860630342479, 0.3610080405758966, 0.2419188622838586, 0.20526448921054666, 0.123158693526328, 0.08650432045301609, 0.10409841952820581, 0.06891022137782638, 0.06011317184023152, 0.03958672291917686, 0.04251907276504181, 0.027857323535717047, 0.20372178620263764, 0.13581452413509176, 0.1867449706857512, 0.08488407758443235, 0.08488407758443235, 0.08488407758443235, 0.06790726206754588, 0.03395363103377294, 0.03395363103377294, 0.06790726206754588, 0.22177357054109428, 0.16446129950238453, 0.13954292078990202, 0.11462454207741953, 0.12459189356241253, 0.07973881187994401, 0.032393892326227254, 0.05482043316746151, 0.037377568068723756, 0.032393892326227254, 0.24762183847313796, 0.14857310308388277, 0.12381091923656898, 0.07428655154194139, 0.17333528693119657, 0.04952436769462759, 0.07428655154194139, 0.04952436769462759, 0.024762183847313796, 0.04952436769462759, 0.1981415721197714, 0.06604719070659047, 0.13209438141318094, 0.13209438141318094, 0.2641887628263619, 0.06604719070659047, 0.06604719070659047, 0.06604719070659047, 0.06604719070659047, 0.21358404318332788, 0.1423893621222186, 0.0711946810611093, 0.0711946810611093, 0.2847787242444372, 0.0711946810611093, 0.0711946810611093, 0.0711946810611093, 0.0711946810611093, 0.16852853805251988, 0.11235235870167992, 0.11235235870167992, 0.11235235870167992, 0.2808808967541998, 0.05617617935083996, 0.05617617935083996, 0.05617617935083996, 0.05617617935083996, 0.2795851785778035, 0.2795851785778035, 0.3779860252849699, 0.17280398784656265, 0.11520265856437509, 0.11520265856437509, 0.11520265856437509, 0.08640199392328132, 0.08640199392328132, 0.11520265856437509, 0.057601329282187544, 0.028800664641093772, 0.057601329282187544, 0.15305373124861404, 0.45916119374584213, 0.1520433921358331, 0.45613017640749925, 0.07602169606791655, 0.07602169606791655, 0.07602169606791655, 0.07602169606791655, 0.07602169606791655, 0.07602169606791655, 0.3685954658257172, 0.07371909316514344, 0.14743818633028688, 0.07371909316514344, 0.07371909316514344, 0.14743818633028688, 0.07371909316514344, 0.18818246124090304, 0.3387284302336255, 0.11290947674454183, 0.03763649224818061, 0.07527298449636122, 0.07527298449636122, 0.07527298449636122, 0.03763649224818061, 0.07527298449636122, 0.03763649224818061, 0.17394568796614357, 0.08697284398307178, 0.17394568796614357, 0.1304592659746077, 0.2609185319492154, 0.04348642199153589, 0.04348642199153589, 0.04348642199153589, 0.04348642199153589, 0.04348642199153589, 0.42903207692916845, 0.13407252404036515, 0.10725801923229211, 0.053629009616146056, 0.053629009616146056, 0.053629009616146056, 0.026814504808073028, 0.026814504808073028, 0.053629009616146056, 0.026814504808073028, 0.16748105147405853, 0.16748105147405853, 0.16748105147405853, 0.16748105147405853, 0.16748105147405853, 0.25570306435112833, 0.20247980208799754, 0.1261159910148099, 0.12495896357430705, 0.08562003059721039, 0.046281097620113724, 0.031239740893576763, 0.062479481787153526, 0.032396768334079604, 0.03355379577458245, 0.22679289954221982, 0.17718195276735924, 0.09567825449437398, 0.134658284103193, 0.1169400888264571, 0.056698224885554954, 0.05315458583020777, 0.060241863940902134, 0.03543639055347184, 0.046067307719513395, 0.2314803906877905, 0.2314803906877905, 0.2314803906877905, 0.2314803906877905, 0.2912571314145392, 0.2912571314145392, 0.2912571314145392, 0.22670193605392272, 0.22670193605392272, 0.22670193605392272, 0.22670193605392272, 0.33809040034354537, 0.16492214650904652, 0.08246107325452326, 0.09070718057997558, 0.15667603918359418, 0.04123053662726163, 0.04947664395271395, 0.0329844293018093, 0.01649221465090465, 0.024738321976356975, 0.24893189288882409, 0.09957275715552964, 0.09957275715552964, 0.24893189288882409, 0.04978637857776482, 0.04978637857776482, 0.04978637857776482, 0.09957275715552964, 0.04978637857776482, 0.04978637857776482, 0.10307968461834217, 0.1803894480820988, 0.1803894480820988, 0.07730976346375663, 0.10307968461834217, 0.12884960577292773, 0.10307968461834217, 0.07730976346375663, 0.025769921154585543, 0.025769921154585543, 0.3215260051219502, 0.14569147107088365, 0.10215149121061957, 0.12392148114075162, 0.07368304284044691, 0.06698458440040628, 0.0468892090802844, 0.053587667520325025, 0.0351669068102133, 0.030143062980182826, 0.2385007818331461, 0.14497106346720645, 0.20576538040506723, 0.09820620428423664, 0.11691214795742456, 0.07014728877445474, 0.03741188734637586, 0.042088373264672845, 0.028058915509781894, 0.023382429591484913, 0.22880977195842545, 0.22880977195842545, 0.11440488597921272, 0.11440488597921272, 0.11440488597921272, 0.11440488597921272, 0.1699750687487402, 0.0849875343743701, 0.1699750687487402, 0.0849875343743701, 0.0849875343743701, 0.0849875343743701, 0.1699750687487402, 0.0849875343743701, 0.28277180790468986, 0.11310872316187595, 0.13330670944078238, 0.1696630847428139, 0.07271275060406311, 0.06059395883671926, 0.056554361580937976, 0.06463355609250054, 0.03231677804625027, 0.02019798627890642, 0.15987407678911092, 0.15987407678911092, 0.15987407678911092, 0.31974815357822184, 0.138491945270278, 0.138491945270278, 0.138491945270278, 0.2492855014865004, 0.0830951671621668, 0.0553967781081112, 0.0553967781081112, 0.0830951671621668, 0.0276983890540556, 0.0276983890540556, 0.18155445612270274, 0.18155445612270274, 0.18155445612270274, 0.18155445612270274, 0.18155445612270274, 0.4417997940993402, 0.12370394234781527, 0.08835995881986805, 0.14137593411178886, 0.07068796705589443, 0.035343983527947216, 0.035343983527947216, 0.017671991763973608, 0.017671991763973608, 0.035343983527947216, 0.10253575041964483, 0.10253575041964483, 0.3076072512589345, 0.10253575041964483, 0.10253575041964483, 0.10253575041964483, 0.10253575041964483, 0.26345030766507016, 0.43110050345193296, 0.0718500839086555, 0.0718500839086555, 0.04790005593910366, 0.02395002796955183, 0.02395002796955183, 0.02395002796955183, 0.02395002796955183, 0.4636290001172556, 0.1545430000390852, 0.1545430000390852, 0.20934811081551719, 0.17185292678885739, 0.13123314409330927, 0.10936095341109106, 0.10936095341109106, 0.08123956539109622, 0.04686898003332474, 0.062491973377766324, 0.03437058535777148, 0.04686898003332474, 0.0928850884733834, 0.0928850884733834, 0.1857701769467668, 0.0928850884733834, 0.0928850884733834, 0.1857701769467668, 0.1857701769467668, 0.0928850884733834, 0.13809281066375884, 0.13809281066375884, 0.13809281066375884, 0.27618562132751767, 0.13809281066375884, 0.1948431816950481, 0.17248412805791144, 0.14054262286200192, 0.1149894187052743, 0.07346546195059192, 0.07985376298977381, 0.09582451558772859, 0.05749470935263715, 0.03194150519590953, 0.04152395675468239, 0.21010800207269414, 0.1326997907827542, 0.16034558052916134, 0.10505400103634707, 0.09952484308706565, 0.07187905334065853, 0.09399568513778422, 0.049762421543532824, 0.03317494769568855, 0.038704105644969976, 0.1785860567299432, 0.1339395425474574, 0.0892930283649716, 0.1785860567299432, 0.11905737115329548, 0.11905737115329548, 0.0446465141824858, 0.05952868557664774, 0.0446465141824858, 0.0446465141824858, 0.32196949400029834, 0.12445879600011533, 0.11634191800010782, 0.1136362920001053, 0.08658003200008023, 0.05681814600005265, 0.054112520000050145, 0.05681814600005265, 0.032467512000030084, 0.0378787640000351, 0.1489284918670867, 0.07446424593354335, 0.2978569837341734, 0.07446424593354335, 0.07446424593354335, 0.1489284918670867, 0.07446424593354335, 0.07446424593354335, 0.19240562940853081, 0.1049485251319259, 0.2098970502638518, 0.08745710427660491, 0.06996568342128393, 0.13993136684256785, 0.06996568342128393, 0.05247426256596295, 0.01749142085532098, 0.05247426256596295, 0.3657129702971467, 0.2693460154904476, 0.4040190232356714, 0.1346730077452238, 0.1346730077452238, 0.22670979454274842, 0.14169362158921775, 0.22670979454274842, 0.08265461259371036, 0.07556993151424947, 0.063762129715148, 0.063762129715148, 0.049592767556226214, 0.03070028467766385, 0.037784965757124735, 0.2299315926670966, 0.1699494380582888, 0.11996430921761561, 0.07997620614507707, 0.06997918037694244, 0.10996728344948098, 0.08997323191321172, 0.059982154608807806, 0.049985128840673174, 0.039988103072538535, 0.2743089708968573, 0.2743089708968573, 0.2743089708968573, 0.28884973016929305, 0.1632628909652526, 0.08791078744282832, 0.12558683920404046, 0.06279341960202023, 0.06279341960202023, 0.05023473568161618, 0.05023473568161618, 0.06279341960202023, 0.02511736784080809, 0.3043969691318803, 0.18359292082641615, 0.18359292082641615, 0.18359292082641615, 0.18359292082641615, 0.18359292082641615, 0.3602362824972168, 0.21614176949833006, 0.07204725649944335, 0.07204725649944335, 0.07204725649944335, 0.07204725649944335, 0.07204725649944335, 0.07204725649944335, 0.270866819228964, 0.270866819228964, 0.15152712099153756, 0.15152712099153756, 0.15152712099153756, 0.3030542419830751, 0.15152712099153756, 0.19007286403194718, 0.1425546480239604, 0.09503643201597359, 0.2851092960479208, 0.047518216007986794, 0.047518216007986794, 0.047518216007986794, 0.09503643201597359, 0.047518216007986794, 0.047518216007986794, 0.13065163449487294, 0.2286403603660276, 0.09798872587115469, 0.13065163449487294, 0.09798872587115469, 0.13065163449487294, 0.06532581724743647, 0.032662908623718234, 0.06532581724743647, 0.06532581724743647, 0.32296987960774387, 0.28316180919439393, 0.21237135689579545, 0.07079045229859848, 0.07079045229859848, 0.07079045229859848, 0.07079045229859848, 0.14158090459719697, 0.35345046616203046, 0.11781682205401015, 0.11781682205401015, 0.11781682205401015, 0.11781682205401015, 0.11781682205401015, 0.11781682205401015, 0.3156052060478758, 0.1578026030239379, 0.1578026030239379, 0.1578026030239379, 0.472361672418413, 0.04294197021985573, 0.12882591065956717, 0.04294197021985573, 0.08588394043971145, 0.08588394043971145, 0.04294197021985573, 0.04294197021985573, 0.04294197021985573, 0.3070136286378918, 0.46052044295683764, 0.38069914986352704, 0.09957073805753167, 0.09957073805753167, 0.29871221417259497, 0.09957073805753167, 0.09957073805753167, 0.09957073805753167, 0.09957073805753167, 0.26093190969961044, 0.12611708968814503, 0.13046595484980522, 0.100023898718184, 0.09132616839486364, 0.06958184258656278, 0.06958184258656278, 0.0826284380715433, 0.03913978645494156, 0.03479092129328139, 0.2761290589842388, 0.2761290589842388, 0.31066034817462507, 0.15113206127414194, 0.11754715876877705, 0.0923584818897534, 0.05037735375804731, 0.06716980501072975, 0.05037735375804731, 0.10075470751609462, 0.04198112813170609, 0.025188676879023656, 0.46467690738637435, 0.1548923024621248, 0.08605127914562488, 0.034420511658249955, 0.06884102331649991, 0.017210255829124978, 0.034420511658249955, 0.06884102331649991, 0.034420511658249955, 0.034420511658249955, 0.2459877248863558, 0.16399181659090387, 0.08199590829545193, 0.08199590829545193, 0.08199590829545193, 0.08199590829545193, 0.08199590829545193, 0.08199590829545193, 0.08199590829545193, 0.1514823744642038, 0.08976733301582446, 0.16270329109118184, 0.3085752072418966, 0.07293595807535738, 0.056104583134890286, 0.03366274988093417, 0.04488366650791223, 0.028052291567445143, 0.05049412482140126, 0.2860511558472737, 0.2860511558472737, 0.2528671524143111, 0.08428905080477038, 0.08428905080477038, 0.08428905080477038, 0.2528671524143111, 0.08428905080477038, 0.08428905080477038, 0.4245092900907564, 0.2122546450453782, 0.22000447236495366, 0.4400089447299073, 0.39714673293405006, 0.13238224431135, 0.13238224431135, 0.13238224431135, 0.13238224431135, 0.32675817039616717, 0.08168954259904179, 0.08168954259904179, 0.08168954259904179, 0.08168954259904179, 0.08168954259904179, 0.16337908519808358, 0.08168954259904179, 0.3231061650648123, 0.1491259223376057, 0.11598682848480443, 0.09113250809520348, 0.08284773463200316, 0.0497086407792019, 0.057993414242402216, 0.057993414242402216, 0.06627818770560254, 0.02485432038960095, 0.19532427789669254, 0.1302161852644617, 0.15625942231735404, 0.1302161852644617, 0.09115132968512318, 0.03906485557933851, 0.09115132968512318, 0.10417294821156935, 0.03906485557933851, 0.03906485557933851, 0.30546344691166677, 0.16853155691678168, 0.1158654453802874, 0.08426577845839084, 0.07373255615109198, 0.052666111536494274, 0.052666111536494274, 0.052666111536494274, 0.06319933384379313, 0.02106644461459771, 0.1280102380166615, 0.1280102380166615, 0.32002559504165373, 0.06400511900833075, 0.1280102380166615, 0.06400511900833075, 0.06400511900833075, 0.06400511900833075, 0.06400511900833075, 0.06400511900833075, 0.1845014654032204, 0.16772860491201855, 0.14256931417521576, 0.1257964536840139, 0.11741002343841299, 0.06429596521627377, 0.04472762797653828, 0.0726823954618747, 0.04752310472507192, 0.03354572098240371, 0.1907031011864765, 0.22668481839147206, 0.13313235365848358, 0.11514149505598581, 0.08635612129198936, 0.06476709096899201, 0.057570747527992905, 0.057570747527992905, 0.028785373763996452, 0.04317806064599468, 0.19394840948506062, 0.13964285482924366, 0.2172222186232679, 0.08533730017342667, 0.0620634910352194, 0.10085317293223152, 0.08533730017342667, 0.054305554655816976, 0.038789681897012124, 0.0310317455176097, 0.2918203734229269, 0.1313191680403171, 0.05836407468458538, 0.2626383360806342, 0.05836407468458538, 0.04377305601343904, 0.02918203734229269, 0.05836407468458538, 0.02918203734229269, 0.02918203734229269, 0.3003747360270379, 0.14580524321736854, 0.16572399229077955, 0.11074824484816517, 0.07250424662721605, 0.05816274729436013, 0.046211497850313525, 0.04461799792444065, 0.026292748776902523, 0.030276498591584725, 0.27568166288195967, 0.13784083144097983, 0.13784083144097983, 0.13784083144097983, 0.13784083144097983, 0.13784083144097983, 0.13784083144097983, 0.36368470050331986, 0.47836720463982335, 0.06833817209140335, 0.06833817209140335, 0.06833817209140335, 0.06833817209140335, 0.06833817209140335, 0.06833817209140335, 0.06833817209140335, 0.38818792749483444, 0.29700039917452536, 0.16335021954598894, 0.08910011975235761, 0.11583015567806489, 0.0772201037853766, 0.056430075843159816, 0.06831009181014083, 0.07425009979363134, 0.02376003193396203, 0.035640047900943043, 0.22745157043111566, 0.10803949595477995, 0.17058867782333675, 0.13078465299789152, 0.1250983637371136, 0.09666691743322416, 0.03980402482544524, 0.03980402482544524, 0.028431446303889458, 0.03411773556466735, 0.23164768985974651, 0.16949830965347307, 0.11299887310231538, 0.11582384492987326, 0.11864881675743115, 0.07627423934406288, 0.04519954924092615, 0.08192418299917864, 0.028249718275578845, 0.022599774620463076, 0.4884940806496353, 0.08881710557266097, 0.08881710557266097, 0.044408552786330484, 0.08881710557266097, 0.08881710557266097, 0.044408552786330484, 0.044408552786330484, 0.044408552786330484, 0.4442316076235736, 0.07403860127059561, 0.14807720254119122, 0.07403860127059561, 0.07403860127059561, 0.07403860127059561, 0.07403860127059561, 0.07403860127059561, 0.20345959427965216, 0.16838035388660869, 0.1426555775983768, 0.12628526541495652, 0.10523772117913043, 0.06548124873368115, 0.04443370449785507, 0.07249709681228986, 0.032740624366840575, 0.039756472445449276, 0.34636179634685166, 0.20426464912763045, 0.06808821637587682, 0.10361250318068212, 0.10065214594694834, 0.03256392957107152, 0.029603572337337746, 0.06512785914214304, 0.020722500636136423, 0.029603572337337746, 0.3592608496464956, 0.061941525801119925, 0.0867181361215679, 0.13627135676246382, 0.0867181361215679, 0.012388305160223986, 0.02477661032044797, 0.1486596619226878, 0.061941525801119925, 0.012388305160223986, 0.22386980105683021, 0.22386980105683021, 0.22386980105683021, 0.22386980105683021, 0.40359195490174266, 0.11682925010313602, 0.0955875682662022, 0.06372504551080146, 0.10620840918466912, 0.0955875682662022, 0.05310420459233456, 0.03186252275540073, 0.03186252275540073, 0.010620840918466911, 0.22731804885624665, 0.12034484939448352, 0.11143041610600325, 0.08468711624056248, 0.10697319946176313, 0.08914433288480261, 0.10697319946176313, 0.053486599730881566, 0.04011494979816117, 0.05794381637512169, 0.38768030257919744, 0.19384015128959872, 0.23398290776476086, 0.26600162145888606, 0.09359316310590435, 0.10590805298826018, 0.07142636131766385, 0.06157444941177918, 0.04187062560000984, 0.06896338334119267, 0.027092757741182836, 0.029555735717654005, 0.38311627352166194, 0.27781818825641097, 0.14199596288661007, 0.092606062752137, 0.17286465047065572, 0.092606062752137, 0.0555636376512822, 0.04321616261766393, 0.06173737516809133, 0.0185212125504274, 0.030868687584045665, 0.22329450762361097, 0.22329450762361097, 0.22329450762361097, 0.19388380998565619, 0.14361763702641198, 0.14361763702641198, 0.1220749914724502, 0.10053234591848839, 0.0789897003645266, 0.05744705481056479, 0.05744705481056479, 0.0430852911079236, 0.050266172959244196, 0.2649237162599409, 0.12101453705701003, 0.15699183185774274, 0.0981198949110892, 0.12755586338441596, 0.052330610619247576, 0.05560127378295055, 0.052330610619247576, 0.032706631637029734, 0.03597729480073271, 0.1732465594704097, 0.1347473240325409, 0.1539969417514753, 0.21174579490827852, 0.09624808859467206, 0.05774885315680323, 0.05774885315680323, 0.07699847087573765, 0.019249617718934412, 0.038499235437868824, 0.19905579097167306, 0.2442957434652351, 0.13119586223132995, 0.07690791923905549, 0.0904799049871241, 0.06785992874034308, 0.049763947742918264, 0.04523995249356205, 0.04523995249356205, 0.049763947742918264, 0.3078669020739455, 0.1759239440422546, 0.10262230069131517, 0.07330164335093942, 0.0879619720211273, 0.04398098601056365, 0.04398098601056365, 0.058641314680751525, 0.07330164335093942, 0.029320657340375762, 0.18401727721913225, 0.18401727721913225, 0.18401727721913225, 0.18401727721913225, 0.18401727721913225, 0.18401727721913225, 0.2727603598443867, 0.1470766646219732, 0.12835781639735844, 0.10161660464790875, 0.07754951407340405, 0.07220127172351412, 0.056156544673844316, 0.06150478702373425, 0.04011181762417451, 0.04011181762417451, 0.2551028612464934, 0.13020875209456434, 0.1461526809224702, 0.11692214473797614, 0.08769160855348211, 0.06643303678294099, 0.06643303678294099, 0.05846107236898807, 0.03720250059844696, 0.034545179127129316, 0.4772297213726298, 0.11930743034315745, 0.11930743034315745, 0.11930743034315745, 0.11930743034315745, 0.24662521356707365, 0.24662521356707365, 0.24662521356707365, 0.16489251248792414, 0.41223128121981034, 0.08244625624396207, 0.08244625624396207, 0.08244625624396207, 0.08244625624396207, 0.08244625624396207, 0.11264816111043065, 0.1267291812492345, 0.16897224166564598, 0.15489122152684215, 0.08448612083282299, 0.08448612083282299, 0.11264816111043065, 0.042243060416411495, 0.028162040277607662, 0.08448612083282299, 0.16490380476862118, 0.2514783022721473, 0.1278004486956814, 0.09481968774195718, 0.10718747309960378, 0.070084117026664, 0.06183892678823295, 0.057716331669017414, 0.028858165834508707, 0.032980760953724236, 0.13136403510062483, 0.06568201755031242, 0.19704605265093725, 0.3284100877515621, 0.06568201755031242, 0.06568201755031242, 0.06568201755031242, 0.06568201755031242, 0.06568201755031242, 0.3242391341479698, 0.11727798469181887, 0.13797409963743396, 0.08278445978246038, 0.04829093487310188, 0.062088344836845284, 0.07588575480058868, 0.08968316476433208, 0.027594819927486792, 0.03449352490935849, 0.43294831117415644, 0.09991114873249764, 0.11656300685458058, 0.04995557436624882, 0.0832592906104147, 0.03330371624416588, 0.06660743248833176, 0.04995557436624882, 0.03330371624416588, 0.01665185812208294, 0.5534945846809733, 0.050317689516452116, 0.10063537903290423, 0.050317689516452116, 0.10063537903290423, 0.050317689516452116, 0.050317689516452116, 0.050317689516452116, 0.050317689516452116, 0.22358613671995994, 0.22358613671995994, 0.22358613671995994, 0.20924264934186992, 0.12205821211609079, 0.08718443722577914, 0.13949509956124662, 0.08718443722577914, 0.06974754978062331, 0.06974754978062331, 0.08718443722577914, 0.034873774890311654, 0.06974754978062331, 0.26262611595170315, 0.15802717578755776, 0.18812759022328304, 0.08277613969824454, 0.11061902305129043, 0.062458359954129974, 0.05041819417983986, 0.03085292479661842, 0.02031777974411457, 0.03386296624019095, 0.2560118247226089, 0.144874908563957, 0.1131215039471993, 0.1131215039471993, 0.13693655740976757, 0.0635068092335154, 0.05159928250223126, 0.049614694713683904, 0.027784229039662985, 0.04366093134804183, 0.14940604874144806, 0.10671860624389147, 0.10671860624389147, 0.20276535186339378, 0.12806232749266977, 0.042687442497556585, 0.042687442497556585, 0.10671860624389147, 0.03201558187316744, 0.06403116374633488, 0.20461700614097228, 0.10230850307048614, 0.10230850307048614, 0.10230850307048614, 0.20461700614097228, 0.10230850307048614, 0.06820566871365744, 0.03410283435682872, 0.03410283435682872, 0.03410283435682872, 0.38107461647750096, 0.2835027982478136, 0.2835027982478136, 0.47249859082259027, 0.11812464770564757, 0.11812464770564757, 0.11812464770564757, 0.11812464770564757, 0.11812464770564757, 0.11812464770564757, 0.19002712932182342, 0.140760836534684, 0.1548369201881524, 0.11964671105448141, 0.105570627401013, 0.070380418267342, 0.0985325855742788, 0.0563043346138736, 0.035190209133671, 0.0211141254802026, 0.16434491645139743, 0.12325868733854806, 0.16434491645139743, 0.08217245822569871, 0.12325868733854806, 0.08217245822569871, 0.16434491645139743, 0.08217245822569871, 0.04108622911284936, 0.04108622911284936, 0.29656778863677236, 0.12894251679859667, 0.077365510079158, 0.09025976175901768, 0.10315401343887734, 0.077365510079158, 0.116048265118737, 0.038682755039579, 0.05157700671943867, 0.025788503359719335, 0.16793741680265245, 0.09735502423342171, 0.24825531179522536, 0.09248727302175062, 0.09978889983925726, 0.1022227754450928, 0.07788401938673736, 0.03894200969336868, 0.036508134087533145, 0.03894200969336868, 0.18046998618532456, 0.11444438148337654, 0.2288887629667531, 0.10123926054298694, 0.07923072564233762, 0.07923072564233762, 0.061623897721818144, 0.061623897721818144, 0.04841877678142854, 0.04401706980129867, 0.24336041443629736, 0.16573683396954733, 0.20140172229210815, 0.06713390743070272, 0.05874216900186488, 0.07972151507395947, 0.054546299787445955, 0.05035043057302704, 0.039860757536979735, 0.039860757536979735, 0.1570016622420791, 0.1299324101313758, 0.1678293630863604, 0.10827700844281317, 0.07579390590996922, 0.0920354571763912, 0.1028631580206725, 0.05955235464354724, 0.04331080337712526, 0.05955235464354724, 0.2716250213532888, 0.18459952907505064, 0.10284830905609965, 0.1107597174450304, 0.10284830905609965, 0.05010558646322803, 0.060654130981802355, 0.0553798587225152, 0.02637136129643581, 0.03428276968536655, 0.27411294893047317, 0.10279235584892744, 0.11135838550300474, 0.21415074135193218, 0.08566029654077287, 0.042830148270386434, 0.05139617792446372, 0.06852823723261829, 0.02569808896223186, 0.02569808896223186, 0.25700907299543624, 0.15372505300661607, 0.11289183580173368, 0.13450942138078906, 0.09367620417590666, 0.0792644804565364, 0.055244940924252646, 0.0504410330177959, 0.03602930929842564, 0.02882344743874051, 0.38187640810264484, 0.19093820405132242, 0.19093820405132242, 0.32258393493031934, 0.2395954740391526, 0.18565513171620202, 0.12544265656500136, 0.11164396434285122, 0.1103895377772012, 0.048922636060350536, 0.0551947688886006, 0.0551947688886006, 0.04265050323210046, 0.025088531313000272, 0.21952303909643478, 0.10976151954821739, 0.32928455864465217, 0.10976151954821739, 0.10976151954821739, 0.10976151954821739, 0.10976151954821739, 0.1386859889299415, 0.4160579667898245, 0.1386859889299415, 0.1386859889299415, 0.1386859889299415, 0.1386859889299415, 0.20351931203904491, 0.13715431898283462, 0.14157865185324864, 0.12388132037159255, 0.07963799166745235, 0.0663649930562103, 0.12830565324200657, 0.04866766157455422, 0.03539466296331216, 0.039818995833726174, 0.2934506158865089, 0.12576454966564668, 0.0838430331104311, 0.0838430331104311, 0.12576454966564668, 0.04192151655521555, 0.04192151655521555, 0.12576454966564668, 0.04192151655521555, 0.04192151655521555, 0.30009161329799783, 0.1678478515056598, 0.11952801546615169, 0.10808384377258397, 0.07883762722235536, 0.06739345552878766, 0.04323353750903359, 0.04831983603950813, 0.04069038824379632, 0.025431492652372698, 0.2034027369230535, 0.13161353565609343, 0.1555432694117468, 0.2034027369230535, 0.07178920126696006, 0.05982433438913338, 0.03589460063348003, 0.04785946751130671, 0.03589460063348003, 0.03589460063348003, 0.16951573240005002, 0.13561258592004002, 0.16951573240005002, 0.23732202536007005, 0.06780629296002001, 0.06780629296002001, 0.033903146480010006, 0.06780629296002001, 0.033903146480010006, 0.033903146480010006, 0.28546051889532303, 0.20291771825089228, 0.0928606507249846, 0.08598208400461536, 0.11349635088609228, 0.0687856672036923, 0.04471068368239999, 0.04127140032221538, 0.027514266881476918, 0.03783211696203076, 0.2391198681613009, 0.14347192089678054, 0.09564794726452036, 0.09564794726452036, 0.09564794726452036, 0.04782397363226018, 0.09564794726452036, 0.14347192089678054, 0.04782397363226018, 0.35734677533715886, 0.08933669383428972, 0.11911559177905297, 0.11539322953595756, 0.0744472448619081, 0.06328015813262189, 0.04094598467404945, 0.07816960710500351, 0.03350126018785864, 0.02233417345857243, 0.40966759152149834, 0.07803192219457111, 0.14630985411482084, 0.0877859124688925, 0.07803192219457111, 0.048769951371606945, 0.058523941645928335, 0.06827793192024972, 0.029261970822964167, 0.019507980548642778, 0.25309455158782646, 0.16161459318258797, 0.14331860150154027, 0.11892394592681002, 0.0853812945115559, 0.06708530283050822, 0.04269064725577795, 0.04878931114946052, 0.03964131530893667, 0.03964131530893667, 0.2979875018472368, 0.2234906263854276, 0.0744968754618092, 0.0744968754618092, 0.0744968754618092, 0.0744968754618092, 0.0744968754618092, 0.1489937509236184, 0.20624605788679634, 0.4124921157735927, 0.06874868596226545, 0.06874868596226545, 0.06874868596226545, 0.2658172783638568, 0.10632691134554273, 0.053163455672771365, 0.10632691134554273, 0.10632691134554273, 0.10632691134554273, 0.1594903670183141, 0.053163455672771365, 0.053163455672771365, 0.053163455672771365, 0.17341794198916355, 0.2000976253721118, 0.24011715044653414, 0.09337889184031883, 0.08003905014884471, 0.08003905014884471, 0.05335936676589648, 0.04001952507442236, 0.02667968338294824, 0.02667968338294824, 0.3128195026458697, 0.11078901343981605, 0.1477186845864214, 0.36929671146605353, 0.11078901343981605, 0.03692967114660535, 0.0738593422932107, 0.03692967114660535, 0.03692967114660535, 0.0738593422932107, 0.03692967114660535, 0.240651621761792, 0.169639667799296, 0.15780434213888, 0.11440814805068801, 0.071011953962496, 0.05917662830208, 0.07890217106944, 0.035505976981248, 0.03945108553472, 0.031560868427776, 0.366305065858559, 0.23993014636175394, 0.059982536590438486, 0.2999126829521924, 0.11996507318087697, 0.059982536590438486, 0.059982536590438486, 0.059982536590438486, 0.059982536590438486, 0.21772771556954232, 0.16202992786570591, 0.11899072827637779, 0.1240541635221811, 0.1291175987679844, 0.06076122294963972, 0.06329294057254138, 0.06329294057254138, 0.032912329097721515, 0.027848893851918206, 0.19193433272412733, 0.09596716636206366, 0.19193433272412733, 0.04798358318103183, 0.1439507495430955, 0.04798358318103183, 0.1439507495430955, 0.09596716636206366, 0.04798358318103183, 0.04798358318103183, 0.2702171804007216, 0.2702171804007216, 0.2702171804007216, 0.21031842854783073, 0.3441574285328139, 0.09559928570355943, 0.09559928570355943, 0.07647942856284753, 0.05735957142213565, 0.019119857140711884, 0.05735957142213565, 0.03823971428142377, 0.03823971428142377, 0.20174106245569684, 0.20174106245569684, 0.20174106245569684, 0.040348212491139374, 0.08069642498227875, 0.1613928499645575, 0.040348212491139374, 0.040348212491139374, 0.040348212491139374, 0.040348212491139374, 0.2571192973199287, 0.16038134387282682, 0.11964957400036287, 0.0916464822130439, 0.12728678085144987, 0.05346044795760894, 0.05855191919166693, 0.06109765480869593, 0.03564029863840596, 0.03309456302137696, 0.14234876844078542, 0.14234876844078542, 0.14234876844078542, 0.14234876844078542, 0.14234876844078542, 0.14234876844078542, 0.14234876844078542, 0.4715922141601241, 0.06737031630858915, 0.06737031630858915, 0.06737031630858915, 0.1347406326171783, 0.06737031630858915, 0.20041848767178502, 0.20041848767178502, 0.20041848767178502, 0.12891147136852676, 0.12891147136852676, 0.12891147136852676, 0.12891147136852676, 0.12891147136852676, 0.15084285617800386, 0.10056190411866923, 0.15084285617800386, 0.2514047602966731, 0.10056190411866923, 0.05028095205933462, 0.05028095205933462, 0.05028095205933462, 0.05028095205933462, 0.05028095205933462, 0.32671567467243395, 0.32671567467243395, 0.22612166316482943, 0.22612166316482943, 0.22612166316482943, 0.2632932885055627, 0.1722783245777139, 0.1397729803177679, 0.07801282622387044, 0.11051817048381646, 0.07151175737188124, 0.061760154093897436, 0.04550748196392442, 0.032505344259946015, 0.029254809833951415, 0.16292486203690648, 0.09309992116394657, 0.13964988174591986, 0.1163749014549332, 0.06982494087295993, 0.09309992116394657, 0.16292486203690648, 0.06982494087295993, 0.04654996058197328, 0.06982494087295993, 0.38859740980672747, 0.1413081490206282, 0.07948583382410335, 0.08537367336662953, 0.07359799428157718, 0.07948583382410335, 0.04710271634020939, 0.03827095702642013, 0.04710271634020939, 0.017663518627578523, 0.20200413856340177, 0.1363527935302962, 0.1262525866021261, 0.1818037247070616, 0.1262525866021261, 0.0454509311767654, 0.05050103464085044, 0.06060124156902053, 0.04040082771268035, 0.030300620784510266, 0.23952136954204736, 0.11976068477102368, 0.11976068477102368, 0.11976068477102368, 0.11976068477102368, 0.11976068477102368, 0.23952136954204736, 0.11976068477102368, 0.20545975927970606, 0.26510936681252395, 0.10604374672500957, 0.06627734170313099, 0.09278827838438337, 0.05964960753281789, 0.05964960753281789, 0.05964960753281789, 0.039766405021878594, 0.039766405021878594, 0.27623189230325595, 0.2134519167797887, 0.050223980418773816, 0.12555995104693454, 0.050223980418773816, 0.06277997552346727, 0.06277997552346727, 0.050223980418773816, 0.10044796083754763, 0.025111990209386908, 0.23578199463430718, 0.3536729919514608, 0.11789099731715359, 0.11789099731715359, 0.11789099731715359, 0.11789099731715359, 0.3228316920924662, 0.1518059519192893, 0.25300991986548216, 0.10120396794619287, 0.10120396794619287, 0.10120396794619287, 0.12650495993274108, 0.05060198397309643, 0.025300991986548216, 0.05060198397309643, 0.025300991986548216, 0.28663836132514414, 0.21497877099385812, 0.07165959033128604, 0.07165959033128604, 0.07165959033128604, 0.07165959033128604, 0.07165959033128604, 0.07165959033128604, 0.07165959033128604, 0.38150612591736727, 0.09952333719583495, 0.09952333719583495, 0.09952333719583495, 0.06634889146388996, 0.049761668597917474, 0.03317444573194498, 0.11611056006180744, 0.03317444573194498, 0.03317444573194498, 0.19697799855318843, 0.3338610144969295, 0.12018996521889463, 0.0667722028993859, 0.08346525362423238, 0.06343359275441661, 0.04340193188460084, 0.03338610144969295, 0.016693050724846475, 0.04006332173963154, 0.22485294004972417, 0.18035079566488293, 0.1475597419076315, 0.11242647002486209, 0.09837316127175433, 0.06558210751450289, 0.053871016886913085, 0.05152879876139513, 0.02810661750621552, 0.037475490008287364, 0.3481025564493289, 0.14214187721680932, 0.09717863034210433, 0.08992649374940997, 0.1131333308460319, 0.06091794737863256, 0.050764956148860466, 0.05221538346739934, 0.021756409778083057, 0.0246572644151608, 0.1910574069196252, 0.0955287034598126, 0.0955287034598126, 0.0955287034598126, 0.1910574069196252, 0.0955287034598126, 0.0955287034598126, 0.0955287034598126, 0.167245964773422, 0.41811491193355493, 0.083622982386711, 0.083622982386711, 0.083622982386711, 0.083622982386711, 0.083622982386711, 0.18742274025001318, 0.4310723025750303, 0.09371137012500659, 0.07496909610000527, 0.07496909610000527, 0.03748454805000263, 0.018742274025001317, 0.03748454805000263, 0.018742274025001317, 0.03748454805000263, 0.1142268947713524, 0.1142268947713524, 0.2284537895427048, 0.1142268947713524, 0.0571134473856762, 0.1142268947713524, 0.1142268947713524, 0.0571134473856762, 0.1345636187171309, 0.1345636187171309, 0.1345636187171309, 0.1345636187171309, 0.1345636187171309, 0.1345636187171309, 0.1345636187171309, 0.15320363672942344, 0.15320363672942344, 0.15320363672942344, 0.15320363672942344, 0.15320363672942344, 0.15320363672942344, 0.15320363672942344, 0.15320363672942344, 0.1622110329375847, 0.10814068862505648, 0.1622110329375847, 0.1622110329375847, 0.10814068862505648, 0.10814068862505648, 0.10814068862505648, 0.05407034431252824, 0.10814068862505648, 0.19691493965606174, 0.19691493965606174, 0.19691493965606174, 0.19691493965606174, 0.5529234719623294, 0.1843078239874431, 0.27485430238464625, 0.27485430238464625, 0.25244826259662584, 0.18933619694746937, 0.06311206564915646, 0.25244826259662584, 0.06311206564915646, 0.06311206564915646, 0.12622413129831292, 0.06311206564915646, 0.43345057501871437, 0.08256201428927892, 0.14448352500623812, 0.08256201428927892, 0.08256201428927892, 0.04128100714463946, 0.04128100714463946, 0.06192151071695919, 0.04128100714463946, 0.02064050357231973, 0.38471503178965066, 0.2357589024580864, 0.2357589024580864, 0.2357589024580864, 0.2357589024580864, 0.22108175598517243, 0.47901047130120694, 0.07369391866172414, 0.07369391866172414, 0.07369391866172414, 0.03684695933086207, 0.03684695933086207, 0.03684695933086207, 0.3223366467013386, 0.1611683233506693, 0.08058416167533465, 0.08058416167533465, 0.08058416167533465, 0.08058416167533465, 0.08058416167533465, 0.08058416167533465, 0.08058416167533465, 0.3787350881091071, 0.28874721528835384, 0.20953337746588507, 0.10221140364189515, 0.09965611855084777, 0.06388212727618447, 0.07154798254932661, 0.05110570182094758, 0.04599513163885282, 0.038329276365710686, 0.028108136001521167, 0.29977808111412296, 0.29977808111412296, 0.22844465320592128, 0.22844465320592128, 0.22844465320592128, 0.1954141181666208, 0.0977070590833104, 0.1954141181666208, 0.0977070590833104, 0.0977070590833104, 0.0977070590833104, 0.1954141181666208, 0.0977070590833104, 0.18076245965664606, 0.09038122982832303, 0.09038122982832303, 0.09038122982832303, 0.09038122982832303, 0.09038122982832303, 0.18076245965664606, 0.09038122982832303, 0.18397177313086416, 0.06689882659304151, 0.1505223598343434, 0.05017411994478113, 0.10034823988956226, 0.06689882659304151, 0.21742118642738492, 0.10034823988956226, 0.03344941329652076, 0.05017411994478113, 0.2313558309849111, 0.2313558309849111, 0.2313558309849111, 0.25693003675714804, 0.12846501837857402, 0.12846501837857402, 0.12846501837857402, 0.12846501837857402, 0.12846501837857402, 0.22114551780347153, 0.22114551780347153, 0.22114551780347153, 0.22114551780347153, 0.29835401512249954, 0.3676133431222209, 0.1159784886084709, 0.18041098227984362, 0.24484347595121633, 0.1159784886084709, 0.06443249367137273, 0.051545994937098175, 0.10309198987419635, 0.06443249367137273, 0.038659496202823634, 0.025772997468549087, 0.2956325387854148, 0.14502728317775063, 0.12829336588801019, 0.07251364158887531, 0.08366958644870229, 0.061357696729048346, 0.10598147616835624, 0.05020175186922138, 0.02788986214956743, 0.02788986214956743, 0.2871404139349933, 0.17228424836099596, 0.07178510348374832, 0.2009982897544953, 0.07178510348374832, 0.05742808278699865, 0.04307106209024899, 0.04307106209024899, 0.014357020696749663, 0.028714041393499326, 0.16788419639853883, 0.09157319803556663, 0.22893299508891657, 0.0763109983629722, 0.0763109983629722, 0.0763109983629722, 0.12209759738075551, 0.061048798690377756, 0.030524399345188878, 0.045786599017783314, 0.1631365567689226, 0.1631365567689226, 0.1631365567689226, 0.1631365567689226, 0.1631365567689226, 0.3212145712985336, 0.12410562981988799, 0.10220463632226069, 0.10220463632226069, 0.0803036428246334, 0.05840264932700611, 0.04380198699525458, 0.0803036428246334, 0.03650165582937882, 0.05840264932700611, 0.410136454697239, 0.11042135318771817, 0.08675963464749285, 0.0946468741609013, 0.07887239513408441, 0.05521067659385909, 0.04732343708045065, 0.04732343708045065, 0.039436197567042204, 0.031548958053633766, 0.45907705419581935, 0.06801141543641769, 0.10201712315462652, 0.06801141543641769, 0.10201712315462652, 0.05100856157731326, 0.03400570771820884, 0.06801141543641769, 0.01700285385910442, 0.01700285385910442, 0.094168118350108, 0.141252177525162, 0.188336236700216, 0.141252177525162, 0.047084059175054, 0.141252177525162, 0.094168118350108, 0.047084059175054, 0.094168118350108, 0.16794727945864266, 0.16794727945864266, 0.251920919187964, 0.08397363972932133, 0.08397363972932133, 0.08397363972932133, 0.08397363972932133, 0.25175879918451444, 0.12587939959225722, 0.12587939959225722, 0.12587939959225722, 0.12587939959225722, 0.12587939959225722, 0.12587939959225722, 0.15035387790526855, 0.07517693895263428, 0.15035387790526855, 0.15035387790526855, 0.07517693895263428, 0.15035387790526855, 0.15035387790526855, 0.07517693895263428, 0.07517693895263428, 0.2693104514381173, 0.13465522571905866, 0.13465522571905866, 0.1058005344935461, 0.09137318888078981, 0.057709382451025144, 0.05290026724677305, 0.08656407367653772, 0.03847292163401676, 0.028854691225512572, 0.3044227634929054, 0.1665371588520012, 0.1163969389825815, 0.08237321835690382, 0.08058249621871026, 0.07162888552774245, 0.04476805345483904, 0.06088455269858108, 0.035814442763871225, 0.03402372062567766, 0.12720819755471632, 0.12720819755471632, 0.2120136625911939, 0.08480546503647755, 0.08480546503647755, 0.12720819755471632, 0.12720819755471632, 0.04240273251823878, 0.04240273251823878, 0.04240273251823878, 0.44494138295449304, 0.05234604505346977, 0.07851906758020465, 0.07851906758020465, 0.13086511263367442, 0.07851906758020465, 0.026173022526734886, 0.05234604505346977, 0.026173022526734886, 0.026173022526734886, 0.4307694765647744, 0.1076923691411936, 0.1076923691411936, 0.1076923691411936, 0.1076923691411936, 0.2136935635530555, 0.0712311878510185, 0.2136935635530555, 0.142462375702037, 0.2136935635530555, 0.0712311878510185, 0.0712311878510185, 0.27280704696130575, 0.27280704696130575, 0.2837792967542544, 0.2837792967542544, 0.19838191253242757, 0.09919095626621378, 0.09919095626621378, 0.09919095626621378, 0.09919095626621378, 0.09919095626621378, 0.09919095626621378, 0.09919095626621378, 0.1835988746548957, 0.1835988746548957, 0.3671977493097914, 0.1835988746548957, 0.22368464110813355, 0.22368464110813355, 0.36485817515715985, 0.36485817515715985, 0.2843420585869915, 0.10662827197012183, 0.10662827197012183, 0.10662827197012183, 0.10662827197012183, 0.07108551464674788, 0.03554275732337394, 0.14217102929349576, 0.03554275732337394, 0.03554275732337394, 0.1210396460474564, 0.1210396460474564, 0.1210396460474564, 0.1210396460474564, 0.2420792920949128, 0.1210396460474564, 0.26583811584634787, 0.5316762316926957, 0.199439388560943, 0.23267928665443352, 0.12821103550346336, 0.09022258053947421, 0.08072546679847693, 0.08072546679847693, 0.052234125575485076, 0.06173123931648236, 0.04273701183448779, 0.03798845496398914, 0.41338922318615706, 0.07516167694293764, 0.07516167694293764, 0.11274251541440647, 0.07516167694293764, 0.03758083847146882, 0.11274251541440647, 0.03758083847146882, 0.24737556636477306, 0.24261834393468126, 0.11417333832220294, 0.090387226171744, 0.11893056075229473, 0.0475722243009179, 0.0475722243009179, 0.03330055701064252, 0.028543334580550736, 0.02378611215045895, 0.374769108451832, 0.3091632186777899, 0.1511464624646973, 0.07557323123234864, 0.13740587496790663, 0.06183264373555798, 0.06183264373555798, 0.05496234998716265, 0.03435146874197666, 0.09618411247753464, 0.020610881245185994, 0.3080987683868592, 0.14058875839012022, 0.11067625660498825, 0.09871125589093546, 0.08973750535539587, 0.06879875410580351, 0.053842503213237525, 0.06281625374877711, 0.02991250178513196, 0.03290375196364516, 0.22849044191214718, 0.15818569055456344, 0.16111505519446276, 0.11424522095607359, 0.08495157455708036, 0.07323411599748307, 0.058587292797986454, 0.06151665743788578, 0.029293646398993227, 0.03222301103889255, 0.15391699149798624, 0.15391699149798624, 0.15391699149798624, 0.07695849574899312, 0.07695849574899312, 0.07695849574899312, 0.15391699149798624, 0.07695849574899312, 0.07695849574899312, 0.2922125155349086, 0.12920961571271472, 0.14312449740485322, 0.08945281087803326, 0.09541633160323548, 0.06261696761462328, 0.05963520725202218, 0.06361088773549031, 0.03379328410947923, 0.030811523746878122, 0.29467287258283703, 0.12102635838223665, 0.10524031163672752, 0.09997829605489114, 0.1315503895459094, 0.05788217140020013, 0.05788217140020013, 0.05788217140020013, 0.04209612465469101, 0.031572093491018256, 0.36147955406762605, 0.36702881381838337, 0.17810330386289425, 0.5343099115886827, 0.17810330386289425, 0.21736107349391007, 0.13041664409634604, 0.13041664409634604, 0.13041664409634604, 0.08694442939756403, 0.043472214698782013, 0.08694442939756403, 0.13041664409634604, 0.043472214698782013, 0.2540019336389762, 0.09939206098916462, 0.1288415605415097, 0.14724749776172535, 0.09939206098916462, 0.0662613739927764, 0.05889899910469014, 0.06258018654873328, 0.04417424932851761, 0.0331306869963882, 0.4790128193845869, 0.10886654986013339, 0.06531992991608003, 0.04354661994405335, 0.10886654986013339, 0.0870932398881067, 0.06531992991608003, 0.021773309972026676, 0.021773309972026676, 0.20856867934286386, 0.1564265095071479, 0.12166506295000391, 0.12166506295000391, 0.06952289311428796, 0.052142169835715964, 0.06952289311428796, 0.1564265095071479, 0.03476144655714398, 0.01738072327857199, 0.2852588874091918, 0.2852588874091918, 0.36417882324827633, 0.13006386544581297, 0.11705747890123168, 0.08237378144901489, 0.11705747890123168, 0.05636100835985229, 0.039019159633743895, 0.039019159633743895, 0.026012773089162593, 0.026012773089162593, 0.40220260961689347, 0.13071584812549036, 0.10055065240422337, 0.07038545668295636, 0.09049558716380103, 0.07038545668295636, 0.04022026096168935, 0.050275326202111684, 0.020110130480844673, 0.03016519572126701, 0.23532844612256912, 0.0672366988921626, 0.10085504833824391, 0.0672366988921626, 0.1680917472304065, 0.10085504833824391, 0.10085504833824391, 0.0672366988921626, 0.0336183494460813, 0.0336183494460813, 0.34270317459547883, 0.11920110420712307, 0.08088646356911923, 0.11494392191401152, 0.09791519274156538, 0.06811491668978462, 0.059600552103561535, 0.04470041407767115, 0.04257182293111538, 0.031928867198336534, 0.348597849123993, 0.19366547173555168, 0.05164412579614711, 0.09037722014325744, 0.038733094347110336, 0.05164412579614711, 0.038733094347110336, 0.05164412579614711, 0.116199283041331, 0.012911031449036778, 0.3241126183948437, 0.21607507892989578, 0.054018769732473944, 0.10803753946494789, 0.054018769732473944, 0.054018769732473944, 0.054018769732473944, 0.054018769732473944, 0.10803753946494789, 0.441466320441732, 0.08829326408834641, 0.08829326408834641, 0.08829326408834641, 0.08829326408834641, 0.08829326408834641, 0.18932451584165091, 0.2341645327515156, 0.1096089302241137, 0.09466225792082546, 0.12455560252740193, 0.06975113741534508, 0.06975113741534508, 0.05480446511205685, 0.03487556870767254, 0.024911120505480385, 0.21926186844898088, 0.15946317705380428, 0.2259061674928894, 0.05315439235126809, 0.09302018661471916, 0.09302018661471916, 0.04651009330735958, 0.03986579426345107, 0.026577196175634046, 0.03986579426345107, 0.20055946969995228, 0.2477499331587646, 0.10617854278232768, 0.07668450312056999, 0.11207735071467921, 0.0943809269176246, 0.05308927139116384, 0.04129165552646076, 0.029494039661757687, 0.035392847594109225, 0.09383679905633377, 0.3753471962253351, 0.18767359811266754, 0.09383679905633377, 0.09383679905633377, 0.09383679905633377, 0.09383679905633377, 0.09383679905633377, 0.29679812032777086, 0.1645880485454002, 0.1510972248941379, 0.11871924813110836, 0.0863412713680788, 0.0431706356840394, 0.0431706356840394, 0.0431706356840394, 0.026981647302524624, 0.026981647302524624, 0.17521567419486517, 0.13141175564614888, 0.30662742984101404, 0.08760783709743258, 0.08760783709743258, 0.04380391854871629, 0.08760783709743258, 0.04380391854871629, 0.04380391854871629, 0.04380391854871629, 0.21822454683183504, 0.15033246559526414, 0.16488076871738647, 0.06304264686253012, 0.07759094998465246, 0.09698868748081557, 0.12123585935101946, 0.03879547499232623, 0.02909660624424467, 0.04364490936636701, 0.2909958736419475, 0.2909958736419475, 0.2909958736419475, 0.22456852608916167, 0.15719796826241317, 0.11789847619680988, 0.1740406077191003, 0.08421319728343563, 0.0673705578267485, 0.0673705578267485, 0.03929949206560329, 0.03368527891337425, 0.03368527891337425, 0.2633708315202359, 0.13168541576011794, 0.07681649252673546, 0.09876406182008846, 0.23044947758020637, 0.021947569293352988, 0.043895138586705976, 0.05486892323338247, 0.021947569293352988, 0.05486892323338247, 0.5851413476569799, 0.14282003006812494, 0.14282003006812494, 0.14282003006812494, 0.14282003006812494, 0.14282003006812494, 0.14282003006812494, 0.15902860998577248, 0.47708582995731746, 0.2052728974543975, 0.2052728974543975, 0.2052728974543975, 0.2052728974543975, 0.2052728974543975, 0.2932508493649901, 0.2932508493649901, 0.20745878418609437, 0.41491756837218874, 0.10372939209304718, 0.10372939209304718, 0.10372939209304718, 0.1706037928245122, 0.12795284461838416, 0.0853018964122561, 0.12795284461838416, 0.2559056892367683, 0.04265094820612805, 0.0853018964122561, 0.04265094820612805, 0.04265094820612805, 0.39110140170194807, 0.2897422378721707, 0.2897422378721707, 0.2897422378721707, 0.23498890125204427, 0.23498890125204427, 0.23498890125204427, 0.23498890125204427, 0.14595775774468378, 0.29191551548936756, 0.18244719718085473, 0.036489439436170945, 0.07297887887234189, 0.036489439436170945, 0.07297887887234189, 0.036489439436170945, 0.07297887887234189, 0.036489439436170945, 0.18112480372477147, 0.09056240186238573, 0.18112480372477147, 0.18112480372477147, 0.18112480372477147, 0.09056240186238573, 0.2448754065506876, 0.1224377032753438, 0.15304712909417975, 0.09182827745650785, 0.09182827745650785, 0.07652356454708988, 0.10713299036592583, 0.03826178227354494, 0.03826178227354494, 0.04591413872825392, 0.12934727474565555, 0.12934727474565555, 0.2586945494913111, 0.12934727474565555, 0.12934727474565555, 0.12934727474565555, 0.12934727474565555, 0.37874136046616397, 0.37874136046616397, 0.0898036323030549, 0.0898036323030549, 0.3592145292122196, 0.0898036323030549, 0.0898036323030549, 0.0898036323030549, 0.0898036323030549, 0.0898036323030549, 0.0898036323030549, 0.2548130091026211, 0.10192520364104846, 0.10192520364104846, 0.2548130091026211, 0.05096260182052423, 0.05096260182052423, 0.05096260182052423, 0.10192520364104846, 0.05096260182052423, 0.05096260182052423, 0.25297923882963197, 0.17467614109665067, 0.12046630420458666, 0.10239635857389866, 0.12046630420458666, 0.06324480970740799, 0.05119817928694933, 0.05119817928694933, 0.04216320647160533, 0.021081603235802666, 0.19512246600709238, 0.12195154125443274, 0.09756123300354619, 0.21951277425797894, 0.07317092475265964, 0.024390308250886547, 0.048780616501773094, 0.12195154125443274, 0.048780616501773094, 0.024390308250886547, 0.2996232575725524, 0.134662138234855, 0.13129558477898365, 0.10099660367614126, 0.07743072948504164, 0.08416383639678439, 0.05049830183807063, 0.05723140874981338, 0.030298981102842376, 0.03366553455871375, 0.2124372273898292, 0.26554653423728647, 0.08345748218886147, 0.09104452602421251, 0.08345748218886147, 0.0531093068474573, 0.06828339451815939, 0.06069635068280834, 0.03034817534140417, 0.06069635068280834, 0.1938138774157465, 0.1938138774157465, 0.1938138774157465, 0.1938138774157465, 0.2072114622097434, 0.13186183958801853, 0.1448125559761275, 0.1283298260276252, 0.1012510553979428, 0.08359098759597604, 0.07064027120786708, 0.04944818984550695, 0.03649747345739799, 0.04473883843164914, 0.15708804622930178, 0.15708804622930178, 0.15708804622930178, 0.15708804622930178, 0.15708804622930178, 0.15708804622930178, 0.15708804622930178, 0.43499197868485234, 0.14499732622828412, 0.08699839573697048, 0.05799893049131365, 0.1159978609826273, 0.028999465245656823, 0.05799893049131365, 0.028999465245656823, 0.028999465245656823, 0.18079662204710337, 0.18079662204710337, 0.18079662204710337, 0.36159324409420673, 0.16323324979733944, 0.3264664995946789, 0.16323324979733944, 0.20917867922868585, 0.4183573584573717, 0.36878555805965185, 0.36878555805965185, 0.23138116843166834, 0.11569058421583417, 0.1446132302697927, 0.11569058421583417, 0.20245852237770978, 0.057845292107917085, 0.057845292107917085, 0.057845292107917085, 0.028922646053958542, 0.028922646053958542, 0.22060298429948638, 0.1654522382246148, 0.22060298429948638, 0.09191791012478599, 0.09191791012478599, 0.055150746074871596, 0.055150746074871596, 0.027575373037435798, 0.0367671640499144, 0.0367671640499144, 0.23672125915607686, 0.1993442182366963, 0.14327865685762547, 0.09967210911834815, 0.059180314789019216, 0.06385244490394179, 0.07008195172383855, 0.05139343126414827, 0.04049179432932894, 0.03581966421440637, 0.25473402666147743, 0.11662521702573667, 0.13197064031859676, 0.21790501075861324, 0.06445077783001237, 0.052174439195724294, 0.03989810056143623, 0.07365803180572841, 0.021483592610004123, 0.027621761927148155, 0.29829234732094745, 0.11750910652037323, 0.10846994448034453, 0.22597905100071775, 0.05423497224017226, 0.05423497224017226, 0.03615664816011484, 0.05423497224017226, 0.02711748612008613, 0.02711748612008613, 0.16522224030558583, 0.16522224030558583, 0.33044448061117165, 0.16522224030558583, 0.16522224030558583, 0.37886412353570315, 0.334177790026239, 0.1670888950131195, 0.1670888950131195, 0.1670888950131195, 0.2237301110331519, 0.14915340735543461, 0.14915340735543461, 0.07457670367771731, 0.14915340735543461, 0.14915340735543461, 0.07457670367771731, 0.07457670367771731, 0.24995881039942694, 0.15081313700077156, 0.15081313700077156, 0.10612776307461702, 0.09076716578750141, 0.06423522501884715, 0.06144238914846249, 0.06144238914846249, 0.03351403044461591, 0.029324776639038916, 0.18590063629403092, 0.18590063629403092, 0.11897640722817979, 0.13384845813170226, 0.08923230542113483, 0.11154038177641855, 0.04461615271056742, 0.052052178162328655, 0.04461615271056742, 0.04461615271056742, 0.13748876398800344, 0.11457396999000287, 0.13748876398800344, 0.13748876398800344, 0.06874438199400172, 0.13748876398800344, 0.0916591759920023, 0.04582958799600115, 0.022914793998000574, 0.0916591759920023, 0.2895955520711165, 0.2895955520711165, 0.24109226958209704, 0.24109226958209704, 0.24109226958209704, 0.24109226958209704, 0.23524175494388588, 0.11762087747194294, 0.11762087747194294, 0.11762087747194294, 0.23524175494388588, 0.16700041586401354, 0.11928601133143825, 0.1908576181303012, 0.11133361057600903, 0.08747640830972138, 0.10338120982057981, 0.07952400755429216, 0.055666805288004516, 0.03976200377714608, 0.03976200377714608, 0.24046935208715625, 0.1163561381066885, 0.24046935208715625, 0.08532783461157156, 0.07757075873779233, 0.0698136828640131, 0.046542455242675404, 0.038785379368896165, 0.054299531116454636, 0.023271227621337702, 0.28407928748620476, 0.1142665290447304, 0.14759426668277675, 0.1142665290447304, 0.08093879140668403, 0.06348140502485022, 0.06982954552733524, 0.05395919427112269, 0.04126291326615265, 0.030153667386803855, 0.2469312408249545, 0.1479015244524467, 0.14404322381455678, 0.10674631764828761, 0.09388531552198791, 0.07716601275779827, 0.04629960765467896, 0.057874509568348706, 0.04629960765467896, 0.03343860552837925, 0.2950220270097435, 0.2950220270097435, 0.3337664365824732, 0.1335065746329893, 0.06675328731649464, 0.1335065746329893, 0.06675328731649464, 0.06675328731649464, 0.1335065746329893, 0.06675328731649464, 0.28334952420813203, 0.1652872224547437, 0.09444984140271068, 0.1259331218702809, 0.09182623469707983, 0.04984852740698619, 0.04722492070135534, 0.07346098775766387, 0.036730493878831934, 0.03148328046757023, 0.16954874065589276, 0.1525938665903035, 0.10172924439353566, 0.1356389925247142, 0.1525938665903035, 0.05086462219676783, 0.05086462219676783, 0.08477437032794638, 0.016954874065589275, 0.0678194962623571, 0.32555897742654366, 0.10851965914218122, 0.07234643942812082, 0.07234643942812082, 0.14469287885624163, 0.14469287885624163, 0.03617321971406041, 0.03617321971406041, 0.07234643942812082, 0.03617321971406041, 0.2375523121015276, 0.2375523121015276, 0.2375523121015276, 0.2375523121015276, 0.23910881647185778, 0.1258467455115041, 0.13843142006265452, 0.08809272185805286, 0.1258467455115041, 0.1258467455115041, 0.05033869820460164, 0.05033869820460164, 0.02516934910230082, 0.03775402365345123, 0.15634661177094697, 0.15634661177094697, 0.31269322354189394, 0.15634661177094697, 0.15634661177094697, 0.15634661177094697, 0.13043241114673457, 0.13043241114673457, 0.26086482229346913, 0.13043241114673457, 0.13043241114673457, 0.13043241114673457, 0.13043241114673457, 0.16244283257228356, 0.1082952217148557, 0.16244283257228356, 0.1082952217148557, 0.1082952217148557, 0.1082952217148557, 0.1082952217148557, 0.05414761085742785, 0.05414761085742785, 0.1082952217148557, 0.13533318213533999, 0.15788871249123, 0.13533318213533999, 0.24811083391479, 0.06766659106766999, 0.04511106071178, 0.06766659106766999, 0.06766659106766999, 0.02255553035589, 0.02255553035589, 0.22170004351403053, 0.22170004351403053, 0.22170004351403053, 0.22170004351403053, 0.20558602100642015, 0.1755002618347489, 0.11532874349140643, 0.13037162307724204, 0.08524298431973519, 0.08022869112445664, 0.05515722514806394, 0.06017151834334248, 0.0501429319527854, 0.03510005236694978, 0.23488801942341356, 0.14261058322135822, 0.21811030375031257, 0.08808300728378007, 0.05872200485585339, 0.06711086269240386, 0.07130529161067911, 0.04613871810102766, 0.03774986026447718, 0.029361002427926695, 0.13458245151377693, 0.09252543541572164, 0.2018736772706654, 0.15981666117261012, 0.07570262897649953, 0.06729122575688846, 0.1009368386353327, 0.06729122575688846, 0.05046841931766635, 0.042057016098055296, 0.1095590586014454, 0.1095590586014454, 0.24346457466987867, 0.13390551606843326, 0.0730393724009636, 0.0730393724009636, 0.08521260113445753, 0.06086614366746967, 0.048692914933975734, 0.048692914933975734, 0.25989327245139143, 0.25989327245139143, 0.308746166667326, 0.10291538888910867, 0.07351099206364906, 0.10291538888910867, 0.17642638095275773, 0.05880879365091924, 0.08821319047637886, 0.04410659523818943, 0.02940439682545962, 0.02940439682545962, 0.38365314197342193, 0.45759274036405717, 0.22879637018202859, 0.22879637018202859, 0.17072150720767, 0.17072150720767, 0.1280411304057525, 0.23474207241054623, 0.06402056520287625, 0.06402056520287625, 0.0426803768019175, 0.0426803768019175, 0.02134018840095875, 0.0426803768019175, 0.5649400279607204, 0.05649400279607204, 0.05649400279607204, 0.05649400279607204, 0.11298800559214409, 0.05649400279607204, 0.05649400279607204, 0.4233242600563393, 0.14110808668544642, 0.14110808668544642, 0.14110808668544642, 0.3667428904251009, 0.32547302440049397, 0.07723088714587993, 0.143428790413777, 0.10481334684083704, 0.08274737908487136, 0.05516491938991423, 0.04413193551193139, 0.09929685490184562, 0.038615443572939966, 0.027582459694957116, 0.2726844367867978, 0.1817896245245319, 0.09089481226226595, 0.09089481226226595, 0.09089481226226595, 0.09089481226226595, 0.1817896245245319, 0.1719501199348118, 0.1719501199348118, 0.1719501199348118, 0.1719501199348118, 0.1719501199348118, 0.1719501199348118, 0.1719501199348118, 0.35130929618195517, 0.3601548230309565, 0.15359543923379027, 0.10063149467041431, 0.0953351002140767, 0.05296394456337595, 0.06355673347605113, 0.04766755010703835, 0.058260339019713545, 0.05296394456337595, 0.026481972281687974, 0.297440023477596, 0.20449001614084727, 0.074360005869399, 0.074360005869399, 0.0371800029346995, 0.05577000440204925, 0.05577000440204925, 0.0371800029346995, 0.13013001027144827, 0.01859000146734975, 0.3552513429050864, 0.3552513429050864, 0.37297299810355367, 0.09907095262125644, 0.08741554643052038, 0.11072635881199248, 0.06993243714441631, 0.034966218572208156, 0.04079392166757618, 0.10489865571662446, 0.05244932785831223, 0.023310812381472104, 0.2890451472686035, 0.14452257363430174, 0.14452257363430174, 0.14452257363430174, 0.14452257363430174, 0.14452257363430174, 0.42469655321560706, 0.0993970656462059, 0.10843316252313372, 0.0903609687692781, 0.07228877501542248, 0.01807219375385562, 0.02710829063078343, 0.10843316252313372, 0.03614438750771124, 0.01807219375385562], \"Term\": [\"acceptability\", \"accrue\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"adjusting\", \"adjusting\", \"adjusting\", \"adjusting\", \"adjusting\", \"adjusting\", \"adjusting\", \"adjusting\", \"adjusting\", \"adp\", \"adp\", \"adp\", \"adp\", \"adp\", \"adp\", \"adp\", \"adp\", \"adp\", \"adp\", \"adversaries\", \"adversaries\", \"adversaries\", \"ajd\", \"ajd\", \"ajd\", \"ajd\", \"ajd\", \"ajd\", \"ajd\", \"ajd\", \"ajd\", \"aki\", \"aki\", \"aki\", \"aki\", \"alarm\", \"alarm\", \"alarm\", \"alarm\", \"alarm\", \"alarm\", \"alarm\", \"alarms\", \"alarms\", \"alarms\", \"alarms\", \"alarms\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"alpha\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"andreas\", \"andreas\", \"andreas\", \"andreas\", \"andreas\", \"andreas\", \"anequal\", \"annotator\", \"annotator\", \"annotator\", \"annotator\", \"annotator\", \"annotator\", \"annotator\", \"annotator\", \"annotator\", \"annotator\", \"annotators\", \"annotators\", \"annotators\", \"annotators\", \"annotators\", \"annotators\", \"annotators\", \"annotators\", \"annotators\", \"annotators\", \"anticipation\", \"anticipation\", \"anticipation\", \"anticipation\", \"anticipation\", \"anticipation\", \"anticipatory\", \"anticipatory\", \"anticipatory\", \"anticipatory\", \"anticipatory\", \"anticipatory\", \"anticipatory\", \"anticipatory\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"argmax\", \"argmax\", \"argmax\", \"argmax\", \"argmax\", \"argmax\", \"argmax\", \"argmax\", \"argmax\", \"argmax\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"array\", \"array\", \"array\", \"array\", \"array\", \"array\", \"array\", \"array\", \"array\", \"array\", \"assortment\", \"assortment\", \"assortment\", \"assortment\", \"assortment\", \"atoms\", \"atoms\", \"atoms\", \"atoms\", \"atoms\", \"atoms\", \"atoms\", \"atoms\", \"atoms\", \"atoms\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"attentional\", \"aucj\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"basket\", \"basket\", \"basket\", \"bauer\", \"bauer\", \"bauer\", \"bauer\", \"bauer\", \"bauer\", \"bb\", \"bb\", \"bb\", \"bb\", \"bb\", \"bb\", \"belgium\", \"beliefs\", \"beliefs\", \"beliefs\", \"beliefs\", \"beliefs\", \"beliefs\", \"beliefs\", \"beliefs\", \"beliefs\", \"beliefs\", \"bernier\", \"bernier\", \"bernier\", \"bernier\", \"bernier\", \"bernier\", \"bernoulli\", \"bernoulli\", \"bernoulli\", \"bernoulli\", \"bernoulli\", \"bernoulli\", \"bernoulli\", \"bernoulli\", \"bernoulli\", \"bernoulli\", \"bertsimas\", \"bertsimas\", \"bg\", \"bg\", \"bg\", \"bg\", \"bg\", \"bg\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"blanchard\", \"blanchard\", \"bluebird\", \"bluebird\", \"bluebird\", \"bluebird\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"bodenhausen\", \"bodenhausen\", \"bodenhausen\", \"boils\", \"boils\", \"bonuses\", \"bot\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branch\", \"branching\", \"branching\", \"branching\", \"branching\", \"branching\", \"branching\", \"branching\", \"branching\", \"branching\", \"branching\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"calcium\", \"capacitance\", \"capacitance\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"casasent\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"cbmpi\", \"ce\", \"ce\", \"ce\", \"ce\", \"ce\", \"ce\", \"ce\", \"ce\", \"ce\", \"ce\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cg\", \"cg\", \"cg\", \"cg\", \"cgm\", \"cgm\", \"cgm\", \"cgm\", \"cgm\", \"cgm\", \"cgm\", \"cgm\", \"cgm\", \"cgml\", \"cgml\", \"cgml\", \"cgml\", \"cgml\", \"cgms\", \"cgms\", \"cgms\", \"cgms\", \"cgms\", \"cgms\", \"cgms\", \"cgms\", \"channels\", \"channels\", \"channels\", \"channels\", \"channels\", \"channels\", \"channels\", \"channels\", \"channels\", \"channels\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"cij\", \"cij\", \"cij\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"cmu\", \"cmu\", \"cmu\", \"cmu\", \"cmu\", \"cmu\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnns\", \"cnns\", \"cnns\", \"cnns\", \"cnns\", \"cnns\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"coin\", \"coin\", \"coin\", \"coin\", \"coin\", \"collobert\", \"collobert\", \"collobert\", \"collobert\", \"collobert\", \"collobert\", \"collobert\", \"collobert\", \"complements\", \"complements\", \"complements\", \"complements\", \"complements\", \"complements\", \"completion\", \"completion\", \"completion\", \"completion\", \"completion\", \"completion\", \"completion\", \"completion\", \"completion\", \"completion\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conductances\", \"conductances\", \"conductances\", \"conductances\", \"conductances\", \"conductances\", \"conductances\", \"conductances\", \"conductances\", \"conductances\", \"connor\", \"connor\", \"consumers\", \"contraction\", \"contraction\", \"contraction\", \"contraction\", \"contraction\", \"contraction\", \"contraction\", \"contraction\", \"contraction\", \"contraction\", \"contrastive\", \"contrastive\", \"contrastive\", \"contrastive\", \"contrastive\", \"contrastive\", \"contrastive\", \"contrastive\", \"contrastive\", \"contrastive\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"converter\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"correlations\", \"correlations\", \"correlations\", \"correlations\", \"correlations\", \"correlations\", \"correlations\", \"correlations\", \"correlations\", \"correlations\", \"correlogram\", \"correlogram\", \"correlogram\", \"correlogram\", \"correlogram\", \"correlogram\", \"correlogram\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cp\", \"cpu\", \"cpu\", \"cpu\", \"cpu\", \"cpu\", \"cpu\", \"cpu\", \"cpu\", \"cpu\", \"cpu\", \"crowdsourced\", \"crowdsourced\", \"crowdsourced\", \"crowdsourced\", \"crowdsourcing\", \"crowdsourcing\", \"crowdsourcing\", \"crowdsourcing\", \"crowdsourcing\", \"ctt\", \"ctt\", \"ctt\", \"ctt\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"db_html\", \"db_html\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"defect\", \"defect\", \"defect\", \"defect\", \"defect\", \"defect\", \"defect\", \"defective\", \"defective\", \"defective\", \"defective\", \"defective\", \"defective\", \"defective\", \"defective\", \"delay\", \"delay\", \"delay\", \"delay\", \"delay\", \"delay\", \"delay\", \"delay\", \"delay\", \"delay\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"detector\", \"detector\", \"detector\", \"detector\", \"detector\", \"detector\", \"detector\", \"detector\", \"devoted\", \"devoted\", \"diaper\", \"diaper\", \"diaper\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital\", \"digital\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimensionnality\", \"dimensionnality\", \"dimensionnality\", \"discontinuous\", \"discontinuous\", \"discontinuous\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"downsampled\", \"downsampled\", \"downsampled\", \"dpps\", \"dpps\", \"dpps\", \"dpps\", \"dpps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"driven\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"durbin\", \"durbin\", \"dx\", \"dx\", \"dx\", \"dx\", \"dx\", \"dx\", \"dx\", \"dx\", \"dx\", \"dx\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"efficacy\", \"elkan\", \"elkan\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"epiphany\", \"epiphany\", \"epiphany\", \"epiphany\", \"epiphany\", \"epiphany\", \"epiphany\", \"epiphany\", \"epiphany\", \"epiphany\", \"erfinv\", \"erfinv\", \"erfinv\", \"erfinv\", \"erfinv\", \"erfinv\", \"erfinv\", \"erfinv\", \"ernst\", \"ernst\", \"ernst\", \"ernst\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"etal\", \"etal\", \"etal\", \"ev\", \"ev\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"examples\", \"exc\", \"exc\", \"exc\", \"exc\", \"exc\", \"exc\", \"exc\", \"exc\", \"exc\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"executions\", \"executions\", \"executions\", \"executions\", \"executions\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"experiments\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"extrapolated\", \"extrapolated\", \"extrapolated\", \"extrapolating\", \"extrapolating\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"faces\", \"facing\", \"facing\", \"facing\", \"facing\", \"facing\", \"facing\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"failing\", \"failing\", \"failing\", \"failing\", \"failing\", \"failing\", \"failing\", \"failing\", \"failing\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"feed\", \"feed\", \"feed\", \"feed\", \"feed\", \"feed\", \"feed\", \"feed\", \"feed\", \"feraud\", \"feraud\", \"feraud\", \"feraud\", \"feraud\", \"feraud\", \"ffl\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fj\", \"fj\", \"fj\", \"fj\", \"fj\", \"fj\", \"fj\", \"fj\", \"fj\", \"fj\", \"fldc\", \"fldc\", \"fldc\", \"fldc\", \"fldc\", \"fldc\", \"fldc\", \"fldc\", \"fldc\", \"flic\", \"flic\", \"flic\", \"flic\", \"flic\", \"flic\", \"flic\", \"flic\", \"flic\", \"flid\", \"flid\", \"flid\", \"flid\", \"flid\", \"flid\", \"flid\", \"flid\", \"flid\", \"flipping\", \"flipping\", \"florin\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"fluctuating\", \"fluctuating\", \"fluctuation\", \"fluctuation\", \"fluctuation\", \"fluctuation\", \"fluctuation\", \"fluctuation\", \"fluctuation\", \"fluctuation\", \"folos\", \"folos\", \"folos\", \"folos\", \"folos\", \"folos\", \"folos\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fom\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fragment\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fs\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"funlx\", \"funlx\", \"funlx\", \"funlx\", \"funx\", \"funx\", \"funx\", \"furniture\", \"furniture\", \"furniture\", \"furniture\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"ganglion\", \"ganglion\", \"ganglion\", \"ganglion\", \"ganglion\", \"ganglion\", \"ganglion\", \"ganglion\", \"ganglion\", \"ganglion\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gpu\", \"gpu\", \"gpu\", \"gpu\", \"gpu\", \"gpu\", \"gpu\", \"gpu\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"grafting\", \"grafting\", \"grafting\", \"grafting\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"gutmann\", \"gutmann\", \"gutmann\", \"gutmann\", \"gutmann\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hinge\", \"hourly\", \"hourly\", \"hourly\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hyperacuity\", \"hyperacuity\", \"hyperacuity\", \"hyperacuity\", \"hyperacuity\", \"hyperacuity\", \"hyperacuity\", \"hyperacuity\", \"hyvarinen\", \"hyvarinen\", \"hyvarinen\", \"hyvarinen\", \"hyvarinen\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"independence\", \"independence\", \"independence\", \"independence\", \"independence\", \"independence\", \"independence\", \"independence\", \"independence\", \"independence\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"inh\", \"inh\", \"inh\", \"inh\", \"inh\", \"inh\", \"inh\", \"inh\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"inhibitory\", \"ink\", \"inlier\", \"inlier\", \"inlier\", \"inlier\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"inputs\", \"inputs\", \"inputs\", \"inputs\", \"inputs\", \"inputs\", \"inputs\", \"inputs\", \"inputs\", \"inputs\", \"inspect\", \"inspect\", \"inspect\", \"instances\", \"instances\", \"instances\", \"instances\", \"instances\", \"instances\", \"instances\", \"instances\", \"instances\", \"instances\", \"instants\", \"ionosphere\", \"ionosphere\", \"ionosphere\", \"ionosphere\", \"ionosphere\", \"ipeirotis\", \"ipeirotis\", \"ipeirotis\", \"ipeirotis\", \"ipeirotis\", \"ipeirotis\", \"ipeirotis\", \"ipeirotis\", \"irh\", \"irh\", \"irhl\", \"irhl\", \"irhl\", \"irhl\", \"irhl\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iti\", \"iv\", \"iv\", \"iv\", \"iv\", \"iv\", \"iv\", \"iv\", \"iv\", \"iv\", \"iv\", \"jagabathula\", \"jat\", \"jat\", \"jat\", \"jat\", \"jat\", \"jat\", \"jat\", \"jc\", \"jc\", \"jc\", \"jc\", \"jc\", \"jc\", \"jc\", \"jck\", \"jck\", \"jck\", \"jck\", \"jd\", \"jd\", \"jd\", \"jd\", \"jd\", \"jd\", \"jd\", \"jd\", \"jd\", \"jpu\", \"jpu\", \"kaj\", \"kca\", \"kca\", \"kca\", \"kca\", \"kca\", \"kca\", \"kca\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernelization\", \"kernelization\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kfd\", \"kfd\", \"kfd\", \"kfd\", \"kfd\", \"kfd\", \"kfd\", \"kfd\", \"kfd\", \"kfd\", \"kj\", \"kj\", \"kj\", \"kj\", \"kj\", \"kj\", \"kj\", \"kj\", \"kj\", \"kl\", \"kl\", \"kl\", \"kl\", \"kl\", \"kl\", \"kl\", \"kl\", \"kl\", \"kl\", \"kosecka\", \"kosecka\", \"krause\", \"krause\", \"krause\", \"krause\", \"krause\", \"krause\", \"krause\", \"kxzt\", \"kxzt\", \"kyoto\", \"kyoto\", \"kyt\", \"kyt\", \"kyt\", \"kyt\", \"kyt\", \"kzt\", \"kzt\", \"kzt\", \"kzt\", \"kzt\", \"kzt\", \"kzt\", \"kzt\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"lag\", \"lag\", \"lag\", \"lag\", \"lag\", \"lag\", \"lag\", \"lag\", \"lag\", \"lag\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"leaf\", \"leaf\", \"leaf\", \"leaf\", \"leaf\", \"leaf\", \"leaf\", \"leaf\", \"leaf\", \"leaf\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"leighton\", \"leighton\", \"leighton\", \"leighton\", \"leighton\", \"leighton\", \"leighton\", \"lel\", \"lem\", \"lem\", \"lem\", \"lem\", \"lem\", \"lem\", \"lem\", \"lem\", \"lesion\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"lle\", \"lle\", \"lle\", \"lle\", \"lle\", \"lle\", \"lle\", \"lle\", \"lle\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load\", \"load\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"lt\", \"lt\", \"lt\", \"lt\", \"lt\", \"lt\", \"lt\", \"lt\", \"lt\", \"lt\", \"lumping\", \"lumping\", \"lumping\", \"lumping\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"marketing\", \"marketing\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matroids\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"maxia\", \"maxia\", \"maxia\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mcts\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"median\", \"median\", \"median\", \"median\", \"median\", \"median\", \"median\", \"median\", \"median\", \"median\", \"memorization\", \"memorization\", \"memorization\", \"memorization\", \"memorization\", \"memorization\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"miiller\", \"miiller\", \"miiller\", \"miiller\", \"miiller\", \"mink\", \"mink\", \"mink\", \"mise\", \"mise\", \"mise\", \"mise\", \"mise\", \"mise\", \"mise\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mkc\", \"mkc\", \"mkc\", \"mkc\", \"mkc\", \"mkc\", \"mkc\", \"mkc\", \"mkc\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mkl\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mobility\", \"mobility\", \"mobility\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"modes\", \"modes\", \"modes\", \"modes\", \"modes\", \"modes\", \"modes\", \"modes\", \"modes\", \"modes\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"monetary\", \"mrr\", \"mrr\", \"msc\", \"msc\", \"msc\", \"msc\", \"msc\", \"msc\", \"msc\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"multi\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"nonstochastic\", \"nonstochastic\", \"nonstochastic\", \"nq\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"nvm\", \"nvm\", \"nvm\", \"nvm\", \"nvm\", \"nvm\", \"nvm\", \"obeys\", \"obeys\", \"obeys\", \"obeys\", \"obeys\", \"obeys\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"offered\", \"offered\", \"offered\", \"offered\", \"offered\", \"offered\", \"offered\", \"offered\", \"offered\", \"offered\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"operators\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimisation\", \"optimisation\", \"optimisation\", \"optimisation\", \"optimisation\", \"optimisation\", \"optimisation\", \"optimisation\", \"optimisation\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"ordinal\", \"ordinal\", \"ordinal\", \"ordinal\", \"ordinal\", \"ordinal\", \"ordinal\", \"ordinal\", \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"organizing\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"ortner\", \"oscillator\", \"oscillator\", \"oscillator\", \"oscillator\", \"oscillator\", \"oscillator\", \"oscillator\", \"oscillator\", \"oscillator\", \"oscillator\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"pai\", \"pain\", \"pain\", \"pain\", \"pain\", \"pain\", \"pain\", \"pain\", \"pain\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"patch\", \"pcmac\", \"pcmac\", \"pcmac\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"perceived\", \"perceived\", \"perceived\", \"perceived\", \"perceived\", \"perceived\", \"perceived\", \"perceived\", \"perceived\", \"perceived\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"periphery\", \"permutations\", \"permutations\", \"permutations\", \"permutations\", \"permutations\", \"permutations\", \"phoneme\", \"phoneme\", \"phoneme\", \"pix\", \"pix\", \"pix\", \"pix\", \"pix\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"pkq\", \"plessis\", \"plessis\", \"pmg\", \"pmg\", \"pmg\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"pr\", \"pr\", \"pr\", \"pr\", \"pr\", \"pr\", \"pr\", \"pr\", \"pr\", \"pr\", \"practically\", \"practically\", \"practically\", \"practically\", \"practically\", \"practically\", \"prefers\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"prevalence\", \"prevalence\", \"prevalence\", \"prevalence\", \"prevalence\", \"prevalence\", \"prevalence\", \"prevalence\", \"prevalence\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"psms\", \"psms\", \"psms\", \"psms\", \"psms\", \"psms\", \"psms\", \"psms\", \"psth\", \"psth\", \"psth\", \"psth\", \"psth\", \"psth\", \"psth\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pu\", \"pull\", \"pull\", \"pull\", \"pull\", \"pull\", \"pull\", \"pull\", \"pull\", \"pulled\", \"pulled\", \"pulled\", \"pulled\", \"pulled\", \"pulled\", \"pulled\", \"pulling\", \"pulling\", \"pulling\", \"pulling\", \"pulling\", \"pulling\", \"pulling\", \"pulling\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pun\", \"pun\", \"pun\", \"pun\", \"purchase\", \"purchase\", \"qai\", \"qai\", \"qsq\", \"qsq\", \"qsq\", \"qsq\", \"qsq\", \"qsq\", \"qsq\", \"qsq\", \"queries\", \"queries\", \"queries\", \"queries\", \"queries\", \"queries\", \"queries\", \"queries\", \"queries\", \"queries\", \"radiologist\", \"rahim\", \"rahim\", \"rahim\", \"rahim\", \"ramp\", \"ramp\", \"ramp\", \"ramp\", \"ramp\", \"ramp\", \"ramp\", \"ramp\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"raphson\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"raviv\", \"raviv\", \"raykar\", \"raykar\", \"raykar\", \"rcl\", \"rcl\", \"rcl\", \"rcl\", \"rcl\", \"rcl\", \"rcl\", \"rcl\", \"rcls\", \"rcls\", \"rcls\", \"rcls\", \"rcls\", \"rcls\", \"rcls\", \"rcls\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnn\", \"rcnns\", \"rcnns\", \"rcnns\", \"rcpn\", \"rcpn\", \"rcpn\", \"rcpn\", \"rcpn\", \"rcpn\", \"rcv\", \"rcv\", \"rcv\", \"rcv\", \"reads\", \"rearranging\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"registries\", \"registries\", \"registries\", \"registries\", \"registries\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"rescaled\", \"rescaled\", \"rescaled\", \"rescaled\", \"rescaled\", \"rescaled\", \"rescaled\", \"reserve\", \"reserve\", \"reserve\", \"reserve\", \"reserve\", \"reserve\", \"reserve\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"retina\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenues\", \"revenues\", \"revenues\", \"revenues\", \"revenues\", \"revisions\", \"revisions\", \"revisions\", \"revisions\", \"revisions\", \"revisions\", \"revisions\", \"ringach\", \"ringach\", \"rishabh\", \"rishabh\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rodent\", \"rodent\", \"rodent\", \"rodent\", \"rodents\", \"rodents\", \"ross\", \"ross\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rowley\", \"rowley\", \"rowley\", \"rowley\", \"rowley\", \"rowley\", \"rudemo\", \"rudemo\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"sage\", \"sage\", \"sage\", \"sage\", \"sage\", \"sage\", \"sage\", \"sage\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"scandinavian\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"shields\", \"shimazaki\", \"shinomoto\", \"shinomoto\", \"shinomoto\", \"shogun\", \"shogun\", \"shogun\", \"shogun\", \"shogun\", \"shogun\", \"shogun\", \"shogun\", \"shogun\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"smce\", \"smce\", \"smce\", \"smce\", \"smce\", \"smce\", \"smce\", \"smce\", \"smce\", \"smo\", \"smo\", \"smo\", \"smo\", \"smo\", \"smo\", \"smo\", \"smo\", \"smo\", \"smo\", \"socher\", \"socher\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"som\", \"som\", \"som\", \"som\", \"som\", \"som\", \"som\", \"som\", \"som\", \"som\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spammer\", \"spammer\", \"spammer\", \"spammer\", \"spammer\", \"spammer\", \"spammer\", \"spammer\", \"spammer\", \"spammer\", \"spammers\", \"spammers\", \"spammers\", \"spammers\", \"spammers\", \"spammers\", \"spammers\", \"spammers\", \"spammers\", \"sparsest\", \"sparsest\", \"sparsest\", \"sparsest\", \"sparsest\", \"sparsest\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spotting\", \"spotting\", \"spotting\", \"spotting\", \"spotting\", \"spotting\", \"spotting\", \"spotting\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stitch\", \"stitch\", \"stitch\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"submodular\", \"subsquares\", \"substitutes\", \"substitutes\", \"substitutes\", \"substitutes\", \"substitutes\", \"substitutes\", \"sugiyama\", \"sugiyama\", \"summable\", \"summable\", \"summable\", \"summable\", \"summable\", \"sung\", \"sung\", \"superfluous\", \"superfluous\", \"superfluous\", \"superfluous\", \"superfluous\", \"supermodular\", \"supermodular\", \"supermodular\", \"supermodular\", \"supermodular\", \"supermodular\", \"supermodular\", \"supermodular\", \"supermodular\", \"superpixel\", \"sussex\", \"sussex\", \"sussex\", \"swn\", \"swn\", \"swn\", \"swn\", \"talkers\", \"talkers\", \"talkers\", \"talkers\", \"talkers\", \"talkers\", \"talkers\", \"talkers\", \"talkers\", \"talkers\", \"tarantula\", \"tarantula\", \"tarantula\", \"tarantula\", \"tarantula\", \"tarantula\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"temp\", \"temp\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"tetris\", \"tetris\", \"tetris\", \"tetris\", \"tetris\", \"tetris\", \"tetris\", \"tetris\", \"tetris\", \"tetris\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"tighe\", \"tighe\", \"tighe\", \"tighe\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timepoints\", \"timepoints\", \"timepoints\", \"timepoints\", \"timepoints\", \"timepoints\", \"timepoints\", \"timescales\", \"timescales\", \"timescales\", \"timescales\", \"timescales\", \"timescales\", \"timescales\", \"timescales\", \"timescales\", \"tlh\", \"tlh\", \"tlh\", \"tlh\", \"tokyo\", \"tokyo\", \"tokyo\", \"tokyoacjp\", \"tokyoacjp\", \"toys\", \"toys\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"tracking\", \"tracking\", \"tracking\", \"tracking\", \"tracking\", \"tracking\", \"tracking\", \"tracking\", \"tracking\", \"tracking\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trill\", \"trill\", \"trill\", \"trill\", \"trill\", \"tsm\", \"turk\", \"turk\", \"turk\", \"turk\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"unbalanced\", \"unbalanced\", \"unfolded\", \"unfolded\", \"unfolded\", \"unfolded\", \"unigram\", \"unigram\", \"unigram\", \"unigram\", \"unigram\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"valence\", \"valence\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"vlmms\", \"vlmms\", \"vlmms\", \"vlmms\", \"vlmms\", \"vlmms\", \"vlmms\", \"vlmms\", \"vlmms\", \"vlmms\", \"vlx\", \"vlx\", \"vlx\", \"vlx\", \"vm\", \"vm\", \"vm\", \"vm\", \"vm\", \"vm\", \"vm\", \"vm\", \"vm\", \"vm\", \"vmax\", \"vmax\", \"vmax\", \"vmax\", \"vmax\", \"vmax\", \"waibel\", \"waibel\", \"waibel\", \"waibel\", \"waibel\", \"waibel\", \"waibel\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"wee\", \"wee\", \"wee\", \"wee\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"windows\", \"windows\", \"windows\", \"windows\", \"windows\", \"windows\", \"windows\", \"windows\", \"windows\", \"windows\", \"withdrawn\", \"withdrawn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wosi\", \"wsd\", \"wsd\", \"wsd\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xa\", \"xjd\", \"xjd\", \"xjd\", \"xjd\", \"xjd\", \"xjd\", \"xjd\", \"xjik\", \"xjik\", \"xjik\", \"xjik\", \"xsi\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"ya\", \"ya\", \"ya\", \"ya\", \"ya\", \"ya\", \"ya\", \"ycp\", \"ycp\", \"ycp\", \"ycp\", \"ycp\", \"ycp\", \"ycp\", \"ye\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yi\", \"yij\", \"yij\", \"yij\", \"yij\", \"yij\", \"yij\", \"yij\", \"yij\", \"yij\", \"yij\", \"ykpred\", \"ykpred\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"zijk\", \"zijk\", \"zijk\", \"zijk\", \"zijk\", \"zijk\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\", \"zt\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [9, 2, 8, 6, 7, 4, 3, 10, 5, 1]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el22081377346738212321297887883\", ldavis_el22081377346738212321297887883_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el22081377346738212321297887883\", ldavis_el22081377346738212321297887883_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el22081377346738212321297887883\", ldavis_el22081377346738212321297887883_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "!pip install pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "import os\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "# Create the 'results' directory if it doesn't exist\n",
        "os.makedirs('./results', exist_ok=True)  # This line is added to create the directory\n",
        "\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqwwhIZmvSkk"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
        "\n",
        "Often, we treat topic models as black-box algorithms, but hopefully, this article addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
        "\n",
        "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
        "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
        "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
        "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}